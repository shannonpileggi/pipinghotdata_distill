---
title: "Getting started with unit testing in R"
description: |
  Automating the dev work you are probably already doing
base_url: https://www.pipinghotdata.com/
twitter:
  site: "@PipingHotData"
  creator: "@PipingHotData"
author:
  - name: Shannon Pileggi
date: 11-01-2021
preview: img/thumbnail.PNG
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
    self_contained: false
draft: false
creative_commons: CC BY
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE, dpi=1)
```


```{r thumbnail, eval=TRUE, echo=FALSE}
#| fig.cap: > 
#|   Background slide adapted from the 2019 Building Tidy Tools workshop (2-testing.pdf, slides 17 & 18); 
#|   presenter images added.
#| fig.alt: >
#|   Header: Testthat gives a new workflow.
#|   Body: Circle diagram with two blocks: 1. Modify code, 2. Run automated tests.
#|   On side: Images of Shannon and Gordon from the workshop.
knitr::include_graphics("img/thumbnail.PNG")
```

# TL;DR

This blog post accompanies the R-Ladies Philly workshop on Nov 11, 2021 
([recording on YouTube](https://youtu.be/4bPekjzIYiU){target=_blank_}), where we live coded use of the
`usethis`, `devtools`, `testthat`, and `covr` packages to automate R package testing.
This post complements the recording through highlighting key points, rather than 
listing all actions.

# Abstract

In this workshop, Shannon Pileggi and Gordon Shotwell discuss how to get started 
with unit testing in R, which is formal automated testing of functions within 
packages. We demonstrate handy functions in `usethis` and `devtools`, strategies
for writing tests, debugging techniques, and broad concepts in function writing 
that facilitate a smoother testing process.

This workshop picks up exactly where we left our little `ralph` 
(aka **R**-**L**adies **Ph**illy) package one year ago with 
[â€œYour first R package in 1 hour: Tools that make R package development easyâ€](https://www.pipinghotdata.com/posts/2020-10-25-your-first-r-package-in-1-hour/){target="_blank"}. Participants will get the most out of this workshop if they review those materials
in advance, or if they are already familiar with building R packages with `devtools` 
and `usethis`.


# Packages

This material was developed using:

| Software / package  | Version               |
|---------------------|-----------------------|
| R                   | 4.1.1                 | 
| RStudio             | 351 "Ghost Orchid"    |
| `usethis`           | 2.1.2                 |
| `devtools`          | 2.4.2                 |
| `testthat`          | 3.1.0                 |
| `covr`              | 3.5.1                 |
| `broom`             | 0.7.9                 |
| `glue`              | 1.4.2                 |
| `magrittr`          | 2.0.1                 |
| `purrr`             | 0.3.4                 |
| `rlang`             | 0.4.12                |


# Tool kit

This table contains the general process functions used in this workshop. Single usage functions 
only need to be used one time in the development process; multiple usage functions
are executed as needed. 


| Usage    | Function                                       | Purpose                           |
|----------|------------------------------------------------|-----------------------------------|
| Single   | `usethis::use_testthat()`                      | initialize testing infrastructure | 
| Multiple | `usethis::use_test()`                          | create a new test file            |
|          | `devtools::test()`                             | execute and evaluate all tests in package |
|          | `covr::report()`                               | reports test coverage |
|          | `browser()`                                    | debugging: interrupt and inspect function execution |
|          | `devtools::check()`                            | build package locally and check   |
|          | `devtools::load_all()`                         | load functions in `r emo::ji("folder")` `R/` into memory    |
|          | `usethis::use_r("function")`                   | create R script for function      |
|          | `usethis::use_package("package")     `         | add package dependency            |
|          | `devtools::document()`                         | build and add documentation       |

Other resources:

* R package development [cheat sheet](https://rklopotek.blog.uksw.edu.pl/files/2017/09/package-development.pdf){target="_blank"}

* [Ch. 12 Testing](https://r-pkgs.org/tests.html){target="_blank"} in R packages by Hadley Wickham and Jenny Bryan

* [Ch. 8.2 Signalling conditions](https://adv-r.hadley.nz/conditions.html#signalling-conditions){target="_blank"} in Advanced R by  Hadley Wickham

* [testthat](https://testthat.r-lib.org/){target="_blank"} package documentation

* {usethis} [user interface functions](https://usethis.r-lib.org/reference/ui.html){target="_blank"}

* [covr](https://covr.r-lib.org/){target="_blank"} package documentation

* [Building tidy tools workshop](https://github.com/hadley/tidy-tools){target="_blank"} at `rstudio::conf(2019)` by Hadley Wickham and Charlotte Wickham

* Debugging
   
   + RStudio blog post by Jonathan McPherson [Debugging with the RStudio IDE](https://support.rstudio.com/hc/en-us/articles/205612627-Debugging-with-the-RStudio-IDE){target="_blank"}
   
   + [Ch 22 Debugging](https://adv-r.hadley.nz/debugging.html){target="_blank"} in Advanced R by  Hadley Wickham

* Indrajeet Patil's curated list of [awesome tools to assist R package development](https://indrajeetpatil.github.io/awesome-r-pkgtools/){target="_blank"}.

# Why unit test

If you write R functions, then you already test your code. You write a function, 
see if it works, and iterate on the function until you
achieve your desired result. In the package development work flow, it looks 
something like this:

```{r original-process, eval=TRUE, echo=FALSE}
#| fig.cap: > 
#|   Workflow cycle of function development without automated testing, from
#|   2019 Building Tidy Tools workshop.
#| fig.alt: >
#|   Circle with three blocks: Modify code, reload code (`load_all()`), and
#|   explore in console.  
knitr::include_graphics("img/tidy-tools-workflow-no-unit-testing.PNG")
```

This process is where most of us likely start, and possibly even stay for a while.
But it can be tedious, time consuming, and error-prone to manually check
all possible combinations of function inputs and arguments.

Instead, we can _automate_ tests with `testthat` in a new workflow.

```{r test-process-1, eval=TRUE, echo=FALSE}
#| fig.cap: > 
#|   Workflow cycle of function development when getting started with
#|   automated testing, from 2019 Building Tidy Tools workshop.
#| fig.alt: >
#|   Circle with three blocks: Modify code, reload code (`load_all()`), and
#|   run automated tests (`test()`).  
knitr::include_graphics("img/tidy-tools-workflow-testing-1.PNG")
```

And once you trust and get comfortable with the tests you have set up, you can
speed up the development process even more by removing the reload code step.

```{r test-process-2, eval=TRUE, echo=FALSE}
#| fig.cap: > 
#|   Workflow cycle of function development when comfortable with
#|   automated testing, from 2019 Building Tidy Tools workshop.
#| fig.alt: >
#|   Circle with two blocks: Modify code and
#|   run automated tests (`test()`).  
knitr::include_graphics("img/tidy-tools-workflow-testing-2.PNG")
```

Like anything in programming, there is an up-front time investment in learning
this framework and process, but with potentially significant downstream time savings.

# Getting started

This post picks up exactly where we left the `ralph` package in [Your first R package in 1 hour](https://www.pipinghotdata.com/posts/2020-10-25-your-first-r-package-in-1-hour/#tl-dr){target=_blank_} in November 2020. In order to keep that as a stand-alone resource, I
created a second repo called `ralphGetsTested` for this workshop, which was a copy of `ralph` as we left it.

1. If you want to follow along with the unit testing steps and practice yourself, fork and clone the [`ralph`](https://github.com/shannonpileggi/ralph){target=_blank_} repo.

`usethis::create_from_github("shannonpileggi/ralph")`

2. If you want to see the repository as it stood at the conclusion of the unit testing workshop, fork and clone [`ralphGetsTested`](https://github.com/shannonpileggi/ralphGetsTested){target=_blank_} repo.

`usethis::create_from_github("shannonpileggi/ralphGetsTested")`

# Keyboard shortcuts


* `Ctrl + S` for save file

* `Ctrl + Shift + L` for `devtools::load_all()`

* `Ctrl + Shift + F10` to restart R

* `Ctrl + Shift + T` for `devtools::test()`

# `(9:00)` Status of `ralph`

The little `ralph` package has a single, untested function
that computes a correlation and returns tidy results.

```{r, eval=TRUE, echo=FALSE}
library(tidyverse)
```


```{r, eval=TRUE}
compute_corr <- function(data, var1, var2){

  # compute correlation ----
  stats::cor.test(
    x = data %>% dplyr::pull({{var1}}),
    y = data %>% dplyr::pull({{var2}})
  ) %>%
  # tidy up results ----
  broom::tidy() %>%
  # retain and rename relevant bits ----
  dplyr::select(
    correlation = .data$estimate,
    pval = .data$p.value
  )

}
```

Here is an example execution:

```{r, eval=TRUE}
compute_corr(data = faithful, var1 = eruptions, var2 = waiting)
```


# `(16:30)` Test set up

To get your package ready for automated testing, submit

```{r}
usethis::use_testthat()
```

From [R Packages Ch 12 Testing](https://r-pkgs.org/tests.html){target="_blank"}, this does three things:

1. Create a `tests/testthat` directory.

2. Add `testthat` to the `Suggests` field in the `DESCRIPTION`.

3. Create a file `tests/testthat.R` that runs all your tests when you execute `devtools::check()`.

```{r use-testthat, eval=TRUE, echo=FALSE}
#| fig.cap: > 
#|   `(17:54)`: `DESCRIPTION` and console output after submitting `usethis::use_testthat()`.
#| fig.alt: >
#|   DESCRIPTION adds `testthat >= 3.0.0 to `Suggests`, console states
#|   actions listed above.
knitr::include_graphics("img/use_testthat.PNG")
```

Note that the edition of `testthat` is specified to 3. It is my understanding that
this version/edition departs significantly from previous versions, both in 
function scopes and execution. 

<aside>
Dan Sjoberg shared that when he updated `{gtsummary}`
from `testthat` 2 to 3 with the help of a [friendly PR](https://github.com/ddsjoberg/gtsummary/pull/756){target="_blank"}, 
the `R CMD check` dropped from 40 to 14 minutes. ðŸ™Œ
</aside>

# `(25:10)` First test

To create our first test file, submit

```{r}
usethis::use_test("compute_corr")
```

. Here, we name this file the same as our function name. This creates a new 
file under `tests` -> `testthat` named `test-compute_corr.R`, and the file 
pre-populates with an example test that we can replace.

For our first test, we create an object that contains the expected
results of an example function execution, and then we assess the correctness
of the output using the `testthat::expect_` functions.

We name the overall test chunk `assess_compute_corr` - you can name this
whatever would be useful for you to read in a testing log. In this test,
we evaluate if the function returns the correct `class` of object, in this
case, a `data.frame`.

```{r}
test_that("assess_compute_corr", {
  expected <- compute_corr(data = faithful, var1 = eruptions, var2 = waiting)
  expect_s3_class(expected, "data.frame")
})
```

Now there are two ways to execute the test.

1. The `Run Tests` button ( ![Run tests button in RStudio](img/run-test-gui.PNG){width=12%} )on the top right hand side of the testing script executes
the tests in this script only (not all tests in the package), and excutes this in
a fresh R environment. 

2. Submitting `devtools::test()` (or `Ctrl + Shift + T`) executes all tests in the 
package in your global environment.

Here, we submit our first test with the `Run Tests` button, and it passes! ðŸŽ‰

```{r test-expect-class, eval=TRUE, echo=FALSE}
#| fig.cap: > 
#|   `(29:23)` Passing result after submitting the first test with the `Run Tests` button.
#| fig.alt: >
#|   `test-compute_corr.R` shows 1 expectation in `test_that(...)`; `Build` pane on top right hand
#|   side shows [FAIL 0 | WARN 0 | SKIP 0 | Pass 1].
knitr::include_graphics("img/test-expect-class.png")
```

Submitting `devtools::check()` will also execute all tests in the package, and you
will see an error in the log if the test fails.

# `(36:08)` Test coverage

Now we have added a total of three tests, and we examine our test coverage with

```{r}
covr::report()
```

This function requires the package to not be loaded, so restart R 
(`Ctrl + Shift + F10`) prior to execution.

```{r covr-report, eval=TRUE, echo=FALSE}
#| fig.cap: > 
#|   `(37:05)` `covr::report()` creates output in viewer showing the percent of code covered by the tests.
#| fig.alt: >
#|   `test-compute_corr.R` shows 3 expectations in `test_that(...)`; `Build` pane on top right hand
#|   side shows [FAIL 0 | WARN 0 | SKIP 0 | Pass 3]; `Viewer` panel on bottom right hand side shows 100% test coverage.
knitr::include_graphics("img/covr-report.png")
```


The percent coverage evaluates the percent of code that was ever executed during through
the `test_that()` functions. In the viewer panel, click on `Files` and then `R/compute_corr.R`. 
The **1***x* on the left hand side counts how many times that line of code has been executed when you ran your test suite; 
in our case, each line of code was executed one time.

```{r covr-report-1x, eval=TRUE, echo=FALSE}
#| fig.cap: > 
#|   `(37:19)` `covr::report()` shows which lines of code were executed, and how many times, 
#|   as a result of your tests.
#| fig.alt: >
#|   In Viewer panel, executed lines of code are highlighted in pale green and marked 
#|   on left hand side with 1x.
knitr::include_graphics("img/covr-report-1x.png")
```


# `(41:27)` Debugging

At this point, our tests consist of the following:

```{r}
test_that("assess_compute_corr", {
  expected <- compute_corr(data = faithful, var1 = eruptions, var2 = waiting)
  expect_s3_class(expected, "data.frame")
  expect_equal(dim(expected), c(1,2))
  expect_named(expected, c("correlation", "pval"))
  expect_equal(expected[["correlation"]], 0.901)
})
```

and we executed our tests with `Ctrl + Shift + T`.


```{r debugging-1, eval=TRUE, echo=FALSE}
#| fig.cap: > 
#|   `(42:18)` Our first test failure.
#| fig.alt: >
#|   In Viewer panel, we see 
#|   `names(actual)` is a character vector (`cor`)
#|   `names(expected)` is absent.
knitr::include_graphics("img/debugging-1.png")
```

We first demonstrate some debugging in the console, where we reveal that the
`correlation` column of the `expected` tibble has a `"names"` attribute of 
`"corr"`.

```{r, eval=TRUE, echo=FALSE}
expected <- compute_corr(data = faithful, var1 = eruptions, var2 = waiting)
```


```{r, eval=TRUE, echo=TRUE}
str(expected)
```


Next, we demonstrate an alternative way of arriving at this through use of the
`browser()` function. To do so,

1. Insert `browser()` into the source of your function.

2. Load your functions with `devtools::load_all()` (Ctrl + Shift + L).

3. Execute the function.

```{r browser-1, eval=TRUE, echo=FALSE}
#| fig.cap: > 
#|   `(46:31)` Now we have entered browser mode.
#| fig.alt: >
#|   In compute_corr.R source script, we see `browser()` is highlighted in yellow.
#|   In console, instead of `>`, we see `Browse[1]>`.
#|   We also see a new environment window.
knitr::include_graphics("img/browser-1.png")
```

Here, this allows you to step into your function with arguments exactly as called. 
The function is run in a fresh environment, and we can execute the 
function line be line to see what is happening. In addition, we can see objects
as they are evaluated in the environment.

To resolve the issue:

1. Stop browser mode with the red `Stop` square on the console,

2. Modify the source of the function as shown.


```{r}
compute_corr <- function(data, var1, var2){

  # compute correlation ----
  results <- stats::cor.test(
    x = data %>% dplyr::pull({{var1}}),
    y = data %>% dplyr::pull({{var2}})
  ) %>%
  # tidy up results ----
  broom::tidy() %>%
  # retain and rename relevant bits ----
  dplyr::select(
    correlation = .data$estimate,
    pval = .data$p.value
  )

  attr(results$correlation, "names") <- NULL

  return(results)

}
```

3. `devtools::load_all()` (`Ctrl + Shift + L`)

4. If needed, step into `browser()` mode again to confirm or further 
troubleshoot.

Now, after updating the `compute_corr()` function remove the `names` attribute,
we still have failed test! ðŸ˜±

```{r debugging-2, eval=TRUE, echo=FALSE}
#| fig.cap: > 
#|   `(58:02)` Now we have a new failure message. Note the `debug at` lines in the 
#|   `Build` pane indicate that I forgot to remove the `browser()` line from 
#|   the source function after our workshop break.
#| fig.alt: >
#|   In the `Build` pane we see a failing test with the note `actual: 0.9008` 
#|   and `expected: 0.9010`.
knitr::include_graphics("img/debugging-2.png")
```

We can correct this by adding the `tolerance` argument to the `expect_equal()` function.

```{r debugging-3, eval=TRUE, echo=FALSE}
#| fig.cap: > 
#|   (1:00:14) Four passing tests!
#| fig.alt: >
#|   In the `Build` pane we see [FAIL 0 | WARN 0 | SKIP 0 | Pass 4]
knitr::include_graphics("img/debugging-3.png")
```

# `(1:00:20)` Testing function inputs

The `{testthat}` package explicitly evaluates the _outputs_ of your function. To
determine if a user correctly specifies _inputs_ to your function,

1. Assess the function inputs programmatically in the source of your function.

2. Return a signal, such as an error, if the input does not conform to expectations.

3. Formalize this catch in a test with functions such as `testthat::expect_error()`.

Read To check if the supplied variables actually exist in the data set, we add the following
to `compute_corr.R`:

```{r, eval=TRUE}
compute_corr <- function(data, var1, var2){

  var1_chr <- rlang::as_label(rlang::ensym(var1))
  var2_chr <- rlang::as_label(rlang::ensym(var2))

  # alert user if variable is not in data set ----
  if (!(var1_chr %in% names(data))){
    #usethis::ui_stop("{var1_chr} is not in the data set.")
    stop(glue::glue("{var1_chr} is not in the data set."))
  }

  # alert user if variable is not in data set ----
  if (!(var2_chr %in% names(data))){
    stop(glue::glue("{var2_chr} is not in the data set."))
  }

  # compute correlation ----
  results <- stats::cor.test(
    x = data %>% dplyr::pull({{var1}}),
    y = data %>% dplyr::pull({{var2}})
  ) %>%
  # tidy up results ----
  broom::tidy() %>%
  # retain and rename relevant bits ----
  dplyr::select(
    correlation = .data$estimate,
    pval = .data$p.value
  )

  attr(results$correlation, "names") <- NULL

  return(results)

}
```


You can enforce an error with 

1. the `stop()` function from base R

2. the `stopifnot()` function in base R (discussed later at `(1:32:40)`)

3. `usethis::ui_stop()`, which provides some additional functionality to the user
including show traceback and rerun with debug, but also adds another dependency to 
your package.

Read [Ch. 8.2 Signalling Conditions](https://adv-r.hadley.nz/conditions.html#signalling-conditions){target="_blank"} 
of Advanced R to learn more about messages, warnings, and errors.

When we execute the function with faulty inputs, we see our error with our handy note:

```{r, eval=TRUE, error=TRUE}
compute_corr(data = faithful, var1 = erruptions, var2 = waiting)
```

Now we add an additional test to catch the error.

```{r}
test_that("assess_compute_corr", {
  expected <- compute_corr(data = faithful, var1 = eruptions, var2 = waiting)
  expect_s3_class(expected, "data.frame")
  expect_equal(dim(expected), c(1,2))
  expect_named(expected, c("correlation", "pval"))
  expect_equal(expected[["correlation"]], 0.901, tolerance = 0.001)
  # catching errors
  expect_error(compute_corr(data = faithful, var1 = erruptions, var2 = waiting))
})
```


```{r expect-error, eval=TRUE, echo=FALSE}
#| fig.cap: > 
#|   `(1:09:05)` `devtools::test()` shows in the console that we have FIVE passing tests! ðŸ¥³
#| fig.alt: >
#|   In the console we see [FAIL 0 | WARN 0 | SKIP 0 | Pass 5]
knitr::include_graphics("img/expect-error.png")
```

_Note_: this does introduce an additional dependency to the package through `glue`
or `usethis` if using `ui_stop`. Don't forget to declare these dependencies with 
`usethis::use_package("package-name")`.

# `(1:14:04)` More

For the remainder of the time, we explore the `awesome_rladies()` function:

```{r, eval=TRUE}
awesome_rladies <- function(v) {
  
  sapply(v, function(x) {
    if (x == 1) {
      verb <- "is"
      noun <- "RLady"
    }
    
    if (x > 1) {
      verb <- "are"
      noun <- "RLadies"
    }
    
    as.character(glue::glue("There {verb} {x} awesome {noun}!"))
  })
}
```


Here are example executions:

```{r, eval=TRUE}
awesome_rladies(1)
awesome_rladies(1:2)
```


We discuss the following:

* Can we break this up to make it easier to test?

_Note:_ Eventually, testing will likely end up as an exercise in refactoring
your code - breaking it down such that the simplest elements each belong
to a function that can be individually and independently tested.

* What type of object should the function output?

* What type of object does this function expect, can we put up guardrails so the 
user doesn't send the wrong thing? How do we test those guardrails?

Rather than write out these details in this already long post, you may watch
in the recording!


# Question & Answer

This discussion is paraphrased from the workshop recording. It was commented throughout that
there is not a right answer to most of this. ðŸ¤·

1. `(7:20)` What is your take on getting started with unit testing?

There is something about unit testing that sounds really scary, like it is something that
real programmers do and regular people don't do, but often times it is the
opposite. When you get familiar with testing your own code, it is an easy way to 
combat others criticizing your work because you can look at your code and see
its test coverage. This is standard across different languages and types of programming. 
What it means for your code to be correct is that it passes tests, 
so this can be a fairly objective way to defend your work.

2. `(19:50)` When do you write tests? 

If the function writing is a process of discovery, and you are not sure what the
function will do, write the test after the function is in a stable state. 
In other cases, when you know precisely how you want the function to behave, writing 
the test before you write the function could be a useful approach (test driven 
development).

3. `(22:19)` When I start writing tests, I get sucked into a rabbit hole of tests. 
How can I have a better process for writing tests?

Have solid tests for things that you really care about as a developer. 
Most of the time, it is good to write more tests than less. Get a couple of tests 
down, and as you discover bugs, make sure there is a test for every bug that you fix.

4. `(23:53)` Is it fair to say that you should consider problems that tripped you 
up when building the function as good test candidates?

Yeah, for sure! If you make the mistake as you are writing the function, you
are likely to make the mistake again in six months, so a test will keep you honest
on those problems.

5. `(32:50)` Can we write test that statements with more descriptive errors?

Think of `test_that()` as a paragraph break to keep related expectations together (Gordon),
and give the test a brief but evocative name - you do you (from the `test_that()` help file).
There are also `expect_error()` and `expect_warning()` functions.

6. `(37:00)` How is the percent coverage from `covr::report()` calculated?

This evaluates which lines of code have been executed for that test. It is not necessarily
evaluating if the function is tested well, but rather has this line of code
ever been run by your test suite.

7. `(38:10)` You mentioned earlier that when you fix a bug, that is a good time
to write a test. Do you have an example of doing this?

Yes! I develop and maintain packages for internal R users, and a common application 
of this for me is when a user calls the function in a reasonable way and gets a 
really unfriendly error message. I resolve this through writing a better
error message for the function and then include that as a test with `expect_error()`.

8. `(39:27)` How is running tests different than trying out different data 
types and sets on your function and debugging?

It is not different at all! What I have learned from being around other  
talented programmers is that they don't have amazing brains that they can hold
all these different variables in. They are able to write good software by 
exporting the normal checks that everyone does when writing functions into a
formal set of expectations that you can get a computer to run. Testing is about
getting that stuff out of your brain so that a computer can execute it.

9. `(53:50)` Do you have advice on how to choose the data to feed into the 
expected object?

You can choose data from base R to  minimize package dependencies, write small
in-line data, or use your own specific data that that you add to your package.
Also consider data that get to the extreme corners of your function (e.g., missing data,
weird values). "Throw lizards at your function!"

10. `(55:57)` Do you commonly see `testthat` used against a script instead of a function?

You can use `expect_equal()` in scripts, but there are packages like `{assertr}` 
which might be more appropriate for R scripts.

11. `(1:09:38)` Regarding dependencies, what do you consider when you are developing a package?

This depends on the developer and user. For internal things, feel free to add 
as many dependencies as you are comfortable with. Base R can cover a lot of the
functionality introduced with your dependencies. It depends on how much work
you want to do to support the dependencies versus what you are getting out of it. You can
also consider pulling in a single function from a package rather than declaring an
entire package as a dependency.

12. `(1:34:55)` Does the `testthat()` framework work for shiny apps?

I recommend to put much as logic as you can into functions that live outside of 
the app, and then you can use `testthat()` on those functions. If you are doing tests
that involve reactivity in shiny apps, then you need to use `{shinytest}`. 


# Personal reflection

When curating for `@WeAreRLadies` on Twitter in February 2021, I asked if there were
[any volunteers for workshops unit testing](https://twitter.com/WeAreRLadies/status/1362739057789394957){target="_blank"},
and Gordon Shotwell replied affirmatively! ðŸ™‹ At that point, we were complete
strangers who had never personally interacted. 

Despite having no experience with unit testing, after a bit of conversation
and much encouragement from both R-Ladies Philly and Gordon, I agreed to develop
a workshop with Gordon's support. (Why not? Teaching is the best way 
for me to learn. ðŸ˜Š)

In small and infrequent time chunks reading and tinkering, three 30 minute meetings with Gordon, and
a few chat exchanges, I learned so much about unit testing between February and November! And
I was so glad to be able to give back and share that knowledge (and confidence!ðŸ’ª) with
R-Ladies Philly. 

I also really liked the mentor-mentee relationship modeled in this workshop - I think
it made the material approachable for beginners and elevated for those more 
experienced. It is a workshop format I highly recommend trying.

# Acknowledgements 

Thank you very much to R-Ladies Philly for hosting this workshop. In particular, [Alice Walsh](https://twitter.com/sciencealice){target="_blank"} 
provided feedback on workshop materials and this blog post. In addition, 
many thanks to [Gordon Shotwell](https://twitter.com/gshotwell){target="_blank"}  for volunteering his time and expertise to our learning. ðŸ’œ


```{r, echo=FALSE, eval=TRUE}
knitr::knit_exit()
```



# Notes

* `devtools::check()` confirm error free

* `devtools::load_all()` command shift L

* ?compute_corr

* `compute_corr(data = faithful, var1 = eruptions, var2 = waiting)`

* review function

* `usethis::use_testthat()` to create overall testing infrastructure - ONE TIME

* `usethis::use_test("compute_corr")` to create a new test file

   + WHAT TO NAME THIS FILE? - same as function name
   
   + WHAT TO NAME TESTS IN THIS FILE?
   
    + It can be whatever also the name can have spaces. It's just something that tells you what failed when the test fails
    
    + So there's no right name, it's just something that reminds future you where the failing test is
  
   
* tests define correctness for your code - what do i care that is right?

* higher risk would do more tests

* to run tests `devtools::test()`

* run tests button runs test in fresh environment; devtools test in global environment

* browser() in function for troubleshooting

* `covr::report` percent of code covered

   + restart R, then run

   + THIS ISNT WORKING
   
   + `covr::report`

* put `browser()` in function for debugging - NOT IN A PIPE

* to test variables in input data set, create a message/warning & test for that message/warning

* Restart R -> cntrl + shift + F10

# Testing code

To enter one at a time.

```{r, eval = FALSE}
test_that("length", {
  expected <- compute_corr(data = faithful, var1 = eruptions, var2 = waiting)
  expect_s3_class(expected, "data.frame")
  expect_equal(dim(expected), c(1,2))
  expect_named(expected, c("correlation", "pval"))
  # can run without tolerance
  expect_equal(expected[["correlation"]], 0.901, tolerance = 0.001)
})
```

Correction to add to `compute_corr` for `expect_equal` to pass:

```{r, eval = FALSE}
# remove attribute name from results
attr(results$correlation, "names") <- NULL
```


# Distinguishing message/warning/error:

https://adv-r.hadley.nz/conditions.html#signalling-conditions

Error to add to `compute_corr` to evaluate inputs:

Errors are the most severe; they indicate that there is no way for a function to continue and execution must stop.

Warnings fall somewhat in between errors and message, and typically indicate that something has gone wrong but the function has been able to at least partially recover.

Messages are the mildest; they are way of informing users that some action has been performed on their behalf.


```{r}
stop("This is what an error looks like")
#> Error in eval(expr, envir, enclos): This is what an error looks like

warning("This is what a warning looks like")
#> Warning: This is what a warning looks like

message("This is what a message looks like")
#> This is what a message looks like
```


# checking variable names

Add to function body:

```{r, eval = FALSE}
  # convert variable names to character strings
  var1_chr <- rlang::as_label(rlang::ensym(var1))
  var2_chr <- rlang::as_label(rlang::ensym(var2))

  # alert user if variable not in data set ----
  if (!(var1_chr %in% names(data))){
    #stop(glue::glue("{var1_chr} not in data set."))
    usethis::ui_stop("{var1_chr} is not in the data set.")
  }

  # alert user if variable not in data set ----
  if (!(var2_chr %in% names(data))){
    #stop(glue::glue("{var2_chr} not in data set."))
    usethis::ui_stop("{var2_chr} is not in the data set.")
  }
```



Add to test

```{r, eval = FALSE}
test_that("length", {
  # evaluate correctness of output produced ----
  expected <- compute_corr(data = faithful, var1 = eruptions, var2 = waiting)
  expect_s3_class(expected, "data.frame")
  expect_equal(dim(expected), c(1,2))
  expect_named(expected, c("correlation", "pval"))
  expect_equal(expected[["correlation"]], 0.901, tolerance = 0.001)
  # catching errors ----
  expect_error(compute_corr(data = faithful, var1 = erruptions, var2 = waiting))
})

```


* `devtools::test()` passes

* `devtools::check()` has a warning

* `usethis::use_package("usethis")`

* `devtools::check()`




# Challenge function

* Can we break this up to make it easier to test?

* What type of object should the function output?

* What type of object does this function expect, can we put up guardrails so the user doesn't send the wrong thing? How do we test those guardrails?

## original

`usethis::use_r("awesome_rladies")`

```{r}
awesome_rladies <- function(v) {
  
  sapply(v, function(x) {
    if (x == 1) {
      verb <- "is"
      noun <- "RLady"
    }
    
    if (x > 1) {
      verb <- "are"
      noun <- "RLadies"
    }
    
    as.character(glue::glue("There {verb} {x} awesome {noun}!"))
  })
}
```


## broken up

```{r, eval = FALSE}
write_rladies <- function(x){
  if (x == 1) {
    verb <- "is"
    noun <- "RLady"
  } 
  
  if(x > 1) {
    verb <- "are"
    noun <- "RLadies"
  }
  
  as.character(glue::glue("There {verb} {x} awesome {noun}!"))
}




compose_rladies <- function(x){
  #sapply(x, write_rladies)
  purrr::map_chr(x, write_rladies)
}
```


 `usethis::use_test()` to create a second test file, right?
 
## tests

```{r}
usethis::use_test("awesome_rladies")
```


what to name test?

```{r}
test_that("assess-awesome_rladies", {
  # evaluate output produced ----
  expected <- write_rladies(1)
  expect_type(expected, "character")
})

```



add to function

```{r, eval = FALSE}
write_rladies <- function(x){
  
 if ( !(is.numeric(x) & (x >= 1)) ){
    usethis::ui_stop("Input must be numeric and greater than or equal to 1.")
  }

  
  if (x == 1) {
    verb <- "is"
    noun <- "RLady"
  } 
  
  if(x > 1) {
    verb <- "are"
    noun <- "RLadies"
  }
  
  glue::glue("There {verb} {x} awesome {noun}!")
}
```

add to test, need to work through this more



```{r}
test_that("correct", {
  # evaluate output produced ----
  expected_1 <- write_rladies(1)
  expected_2 <- write_rladies(2)
  expect_type(expected_1, "character")
  expect_equal(expected_1, "There is 1 awesome RLady!")
  expect_equal(expected_2, "There are 2 awesome RLadies!")
  # ----
  expected_multiple <- compose_rladies(1:2)
  expect_length(expected_multiple, 2)
  # catching errors ----
  expect_error(compose_rladies("a"))
  expect_error(compose_rladies(0))
})

```

# testthat 3e

* [https://testthat.r-lib.org/articles/third-edition.html](https://testthat.r-lib.org/articles/third-edition.html)

* the checking is a bit different from regular testthat (uses waldo package), and the tests run in parallel (so much faster)

* use first edition if nothing is specified; add this to your DESCRIPTION file

```
Roxygen: list(markdown = TRUE)
RoxygenNote: 7.1.2
Config/testthat/edition: 3
Config/testthat/parallel: true
```

* gtsummary check went from 40 min to 14 min
