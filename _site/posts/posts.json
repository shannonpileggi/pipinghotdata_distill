[
  {
    "path": "posts/2022-08-30-my-4-biggest-internship-takeaways/",
    "title": "My 4 Biggest Internship Takeaways",
    "description": "What I've learned after interning at the PCCTC.",
    "author": [
      {
        "name": "Kirina Sirohi",
        "url": {}
      }
    ],
    "date": "2022-08-30",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nAbout Me\r\nIntroduction\r\nWhat I’ve\r\nLearned\r\nFinal\r\nThoughts\r\n\r\n\r\n\r\n\r\nFigure 1: My last Data Science team meeting at the PCCTC with\r\nVictoria Catherine, Meghan Harris, Shannon Pileggi, and Travis Gerke.\r\n\r\n\r\n\r\nAbout Me\r\nHi everyone! My name is Kirina Sirohi and I am going to be a third\r\nyear Statistics major/Data Science minor at Cal Poly, SLO. I had the\r\nwonderful opportunity to work at the Prostate Cancer Clinical Trials\r\nConsortium (PCCTC) as a data science intern this summer for about 8\r\nweeks. It was a fully remote internship so I got to work out of the\r\ncomfort of my home in Massachusetts, though I did get to meet some of my\r\nco-workers after the RStudio Conference in Washington D.C. halfway\r\nthrough the internship.  Throughout the summer I got experience\r\nwith medical monitoring on clinical trials, automating processes for\r\nmanual review that reduced the number of checks data management had to\r\nexamine, and provided figures for a regulatory committee to review\r\nclinical data for ongoing trials. I also picked up some technical skills\r\ninvolving R such as data analysis, data cleaning, wrangling with the\r\ntidyverse, unit testing in a new R package, and customizing ggplots.\r\nAdditionally, I learned more about using GitHub regarding pull requests,\r\ncode review processes, and deploying quarto slides via GitHub Pages.\r\nIntroduction\r\nPrior to starting at the PCCTC, I was doubting if I had the skills to\r\ndo well and pondered for hours on end why I was chosen for this role out\r\nof all possible candidates. Throughout the internship, I started to get\r\nmore confident in my abilities through the help and mentorship of my\r\nfellow colleagues. I was slightly nudged to write a blog post about\r\nanything related to this internship so I decided to inscribe my biggest\r\ntakeaways as an intern so that I can refer back to this whenever work\r\ngets difficult to remind myself that if I got through this once, I can\r\ndo it a heck of a lot more times.\r\nWhat I’ve Learned\r\n1. Don’t be afraid\r\nto reach out and ask for help\r\nWhen I was in high school, I used to be uncomfortable asking other\r\npeople questions because I feared it would make people think less of me.\r\nHowever, when I was really struggling in one of my classes, my parents\r\nforced me to ask for help and after weeks of going at least once a week,\r\nI drastically improved. I then took that mindset with me to college and\r\nmy internship this summer. The number of questions I asked everyone was\r\nabsurd, but I came to realize that if I hadn’t asked any of them, I\r\nwould have completed close to nothing.  There’s no worse\r\nfeeling than spending hours on a problem and getting a simple solution\r\nafter asking for help. As a simple solution to that problem, I decided\r\nto deploy the “1 hour rule” where I would spend no longer than an hour\r\non a single problem before asking for help. The one hour was not a\r\nstrict deadline because some tasks would need shorter and longer time,\r\nbut on average, one hour was long enough for me to spend time testing,\r\nthinking, and fixing. If I wasn’t able to figure it out in that time,\r\nI’d go and ask for help which was great because it showed I wanted to\r\nlearn and also was very relieving when it was fixed. After all, an\r\ninternship is a learning experience and the best way to learn is by\r\nasking questions!\r\n2. It is okay to make mistakes\r\nMistakes suck, but they are inevitable. You can calculate everything\r\ndown to the last number, go through every line of code, reread every\r\nsentence, but you will still make a mistake at least once in the\r\nprocess. I’ve always heard about these companies having a large disaster\r\nand saying the intern did it – I did not want\r\nto be the intern. For example, an unknown amount of HBO Max subscribers\r\nreceived an empty test email and the company later came out to say that\r\nit was, in fact, an intern who sent it out by mistake. \r\n\r\nWe mistakenly sent out an empty test email to a portion of our HBO Max mailing list this evening. We apologize for the inconvenience, and as the jokes pile in, yes, it was the intern. No, really. And we’re helping them through it. ❤️— HBOMaxHelp (@HBOMaxHelp) June 18, 2021\r\n\r\n\r\nAfter reading that, I made sure to double and triple check everything\r\nI did so that my work would not result in a catastrophe such as that.\r\nThrough the guidance of my co-workers, I was able to not only fix my\r\nmistakes, but also learn from them and use that knowledge for future\r\nassignments. Everyone makes mistakes and that is okay, but what is not\r\nokay is not learning from those mistakes and continuing to make them.\r\nGetting caught up and upset about a mistake is not going to make you a\r\nbetter person/worker if you don’t do anything about it afterwards. The\r\npurpose of an internship is to grow and learn so that you can make fewer\r\nmistakes when you have a full-time job. So really, mistakes are crucial\r\nto the learning process and are the reason successful people are\r\nsuccessful.\r\n3. How to work in a\r\nprofessional environment\r\nWhat I have learned about working in a professional environment is\r\nthat being a working professional does not mean you have to be\r\nprofessional all the time. What really put me at ease were the amount of\r\n“lol”s and “haha”s I received in Teams messages and emails. It reminded\r\nme that people aren’t just their jobs and that they have personalities.\r\nThis could just be the PCCTC, but I never felt pressured to write or act\r\na certain way, which was really nice to see because I had always thought\r\nusing slang was considered unprofessional. I feel like as the younger\r\ngenerations start getting into the professional world, rules that used\r\nto be strongly upheld will start to relax. For example, in my opinion,\r\ndressing up very nicely for meetings should not be so strongly supported\r\nas long as you are getting your work done. If I get 500 lines of code\r\ndone in 4 hours wearing sweatpants, I’m still going to get those 500\r\nlines of code done in 4 hours wearing a pantsuit - I’ll just be more\r\nuncomfortable. But again, these are my opinions and people can freely\r\ndisagree.  Back to the topic at hand: working in a professional\r\nenvironment. For the rest of your life, most of what you get and don’t\r\nget is a direct result of connections so establishing these connections\r\nwhenever you can is super important when you’re working with\r\nprofessionals. In sum, working in a professional environment is a lot\r\nless scary than I was anticipating once I realized that working\r\nprofessionals still have personalities and aren’t these corporate robots\r\nthat criticize my every move.\r\n4. Any amount of work\r\nyou do is appreciated\r\nGoing into my internship, I was concerned that I wouldn’t be able to\r\ndo anything right or that I would spend all summer getting close to\r\nnothing done. My first week was a bit intimidating because I was the\r\nonly intern and I did not have as much experience as the rest of my\r\nteam. However, as the summer went on, I picked up more small tips that\r\nwere very valuable and helped me excel. It wasn’t until my second to\r\nlast week that I realized that even if I had only completed one task, I\r\nstill contributed more than I would have had I not been there. \r\nOne thing I realized about working at this company (as with most) is\r\nthat the tasks are never ending. Even if you finish one challenging,\r\nlong task, there are still so many to be done so it’s very helpful\r\nhaving another body to help out. Once I came to learn that there was so\r\nmuch work that had to be done, I realized that my work was actually\r\nbeneficial and not unimportant busy work that can often be assigned to\r\ninterns. Even when I wasn’t physically working and just shadowing\r\nothers, I got so much insight into how other people work and picked up\r\nlittle skills that I got to incorporate that either made tasks more\r\nefficient.\r\nFinal Thoughts\r\nOverall, I’m so grateful I got to work alongside a great group of\r\npeople this summer and I have definitely grown both as a data scientist\r\nand a person, and that is all thanks to them. I have learned so much\r\nthrough their mentorship and kindness and honestly had such a blast this\r\nsummer. Work is work but if you’re surrounded by the right people, there\r\nwill never be a dull moment. The knowledge I am taking away and the\r\nskills I have gained are immeasurable. I could not have asked for a\r\nbetter internship experience.\r\nAcknowledgements\r\nI’d like to thank everyone at the PCCTC for giving me this wonderful\r\ninternship opportunity, but especially everyone on the data science\r\ndream team: Travis Gerke, Shannon Pileggi, Victoria Catherine, and\r\nMeghan Harris. I’d also like to specifically thank Shannon for being my\r\nbiggest mentor and teaching me more than I could ever imagine, both\r\nprofessionally and socially.\r\nIf you want to keep in touch with me, check out my new {postcards}\r\npersonal website with my contact information. https://ksirohi7.github.io/Postcard\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-30-my-4-biggest-internship-takeaways/images/team_photo.png",
    "last_modified": "2022-08-30T16:00:36-04:00",
    "input_file": "my-4-biggest-internship-takeaways.knit.md",
    "preview_width": 1920,
    "preview_height": 1017
  },
  {
    "path": "posts/2022-06-02-locating-r-and-r-adjacent-software-and-configuration-files/",
    "title": "Locating R and R Adjacent Software and Configuration Files",
    "description": "My personal R administration on Windows 10",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2022-06-02",
    "categories": [],
    "contents": "\n\nContents\nTL;DR\nBackground\nResources\nPackages\nBooks\n\nWindows\nRecommended settings\nAdministrator rights\n\nHomes\nHome drive\nR Home\nHome\n\nSoftware\nR\nR packages\nRtools\nGit\nQuarto\n\nConfigurations\n.Renviron\n.Rprofile\n.gitconfig\n.shrtcts.R\n\nSummary\nAcknowledgements\n\n\n\n\nFigure 1: File tree visualizing the hierarchy of R and R adjacent\nsoftware and configuration files on my Windows 10 operating system.\n\n\n\n\n\n\nTL;DR\nThere is a lot to keep track of to install and maintain R and R\nadjacent software and configuration files. Here is what my Windows 10\nset up looks like, with associated code if available. Your set up might\ndiffer from mine, but you can use the associated code to figure it out.\n🤗\n\n\nItem\n      Code\n      Location\n    Home\n\nHome drive\nSys.getenv(\"HOMEDRIVE\")\nC:/User home directory\nSys.getenv(\"USERPROFILE\"), Sys.getenv(\"HOMEPATH\"), fs::path_home()\nC:/Users/pileggisR home directory\nSys.getenv(\"HOME\"), Sys.getenv(\"R_User\"), fs::path_home_r()\nC:/Users/pileggis/OneDrive - Memorial Sloan Kettering Cancer Center/DocumentsSoftware\n\nR\nR.home(), Sys.getenv(\"R_HOME\")\nC:/Program Files/R/R-4.2.0R packages\n.libPaths()\nC:/Program Files/R/R-4.2.0/libraryRStudio\n\nC:/Program Files/RStudioRtools\ndevtools::find_rtools(), devtools::has_devel()\nC:/rtools42Git\n\nC:/Program Files/GitQuarto\n\nC:/Program Files/QuartoConfiguration\n\n.Renviron\nusethis::edit_r_environ()\nC:/Users/pileggis/OneDrive - Memorial Sloan Kettering Cancer Center/Documents/.Renviron.Rprofile\nusethis::edit_r_profile()\nC:/Users/pileggis/OneDrive - Memorial Sloan Kettering Cancer Center/Documents/.Rprofile.gitconfig\nusethis::edit_git_config()\nC:/Users/pileggis/.gitconfig.shrtcts.R\nshrtcts::locate_shortcuts_source()\nC:/Users/pileggis/OneDrive - Memorial Sloan Kettering Cancer Center/Documents/.shrtcts.R\n\nBackground\nWhen Travis Gerke tweeted about the breaker of chains keyboard\nshortcut, I was eager to try it out!\n\nCheat code for quickly viewing data frames as you build them in #rstats: assign a keyboard shortcut to @MilesMcBain's {breakerofchains} (quick how-to in thread) pic.twitter.com/P9P6hLa7Xx— King Ranch (@travisgerke) March 14, 2022\n\n\nHowever, it was not immediately clear to me where to save the the\nshrtcts.R configuration file. This took a bit of trial and\nerror to figure out, and led me to document where everything R and R\nadjacent is located.\nResources\nIn this post I refer to two packages and two books.\nPackages\nusethis for helpers that automate\nrepetitive tasks during project set up and development.\nfs for cross-platform file system\noperations.\nBooks\nHappy Git\nwith R, a manual for integrating Git with R; abbreviated to\nHappy Git for the remainder of this post.\nWhat they Forgot to\nTeach You About R, is in progress but still extremely helpful\ndocumentation on best practices when working in R; abbreviated to\nWTF for the remainder of this post.\nWindows\nI am working on a computer supplied by employer with a Windows 10\noperating system.\nRecommended settings\nWhen getting started with a Windows computer, I recommend changing\ndefault settings such that:\nDisplay the full path in the title bar\nShow hidden files, folders, and drives\n Hide extension for known file types\n\nSome Windows 🪟 defaults I always change on a new computer💻🧐Do you have other defaults you change on Windows?gotta love the switcheroos between hide/display & check/uncheck pic.twitter.com/X1XZ24ltz1— Shannon Pileggi (@PipingHotData) October 19, 2021\n\n\nAdministrator rights\nMuch of how you work in Windows depends on your level of rights.\nFortunately, I was granted administrator privileges on my work laptop,\nwhich allows me to install software and save configuration files where I\nwant to.\nIf you do not have administrator privileges, knowing this can still\nhelp you work with IT to complete your set up or find alternate viable\nlocations.\nHomes\nOn Mac OS, the “user home directory” and the “R home directory” are\nthe same, but on Windows OS they differ. On Windows, the R home\ndirectory tends to be associated with the user’s documents directory,\nwhereas the user home directory is a system level directory.\nHome drive\nYou can locate your home drive through environment variables. See the\nhelp file for Sys.getenv and environment variables for more information.\n\n\nSys.getenv(\"HOMEDRIVE\")\n\n\n\nC:/\nR Home\nThese three commands all point to the “R home directory”.\n\n\nSys.getenv(\"HOME\")\nSys.getenv(\"R_User\")\nfs::path_home_r()\n\n\n\nC:/Users/pileggis/OneDrive - Memorial Sloan Kettering Cancer Center/Documents\nHome\nHere, we find the “user home directory”.\nWrapping the function in fs::path_real() allows you to\nsee a consistent full path, rather than the short\nfile path or any other path specification returned by the\nenvironment variables.\n\n\nfs::path_real(Sys.getenv(\"USERPROFILE\"))\nfs::path_real(Sys.getenv(\"HOMEPATH\"))\nfs::path_home()\n\n\n\nC:/Users/pileggis\nSoftware\nR\nFrom the R.home() help file:\n\nThe R home directory is the top-level directory of the R installation\nbeing run. The R home directory is often referred to as R_HOME, and is\nthe value of an environment variable of that name in an R session.\n\n\n\nfs::path_real(R.home())\nfs::path_real(Sys.getenv(\"R_HOME\"))\n\n\n\nC:/Program Files/R/R-4.2.0\nR packages\n.libPaths() shows where R is looking for packages in the\ncurrent session. If more than one .libPaths() is present, R\nattaches packages from the libraries in the order shown (searches first\nlibrary first).\nFor Windows users:\nUnder R < 4.2.0\nMy packages were initially being installed to a default location\nin my R home directory (given by fs::path_home_r(), the\n“Documents” directory).\nYou may have more than one path present, which likely correspond\nto a directory specific to the user and a directory available to all\nusers.\n\nWith R >= 4.2.0 (released 2022-04-22), the default personal\nlibrary for Windows is “now a subdirectory of local application data\ndirectory (usually a hidden directory\nC:/Users/username/AppData/Local)” as discussed in the\nJumping Rivers New features in R 4.2.0 post.\nThese locations may work well for some users, and others may want to\nchange the default location. I modified my default location to be under\nmy R installation using the .Renviron configuration file\n(demonstrated below).\n\n\n.libPaths()\n\n\n\n\"C:/Program Files/R/R-4.2.0/library\"\nLastly, if you are using the renv package for\nreproducible environments, all utilized packages are installed in global package cache which is shared across\nall projects. My cache is located at\nC:/Users/pileggis/AppData/Local/renv.\nRtools\nRtools is a collection of tools required to build source packages on Windows. Basically, if you\nwant to do anything with packages beyond CRAN (install from github,\nbuild locally) on Windows, you need R tools.\nIn order to work, R tools must be installed at\n\"C:/rtools42\"\nwhich is where mine is, too. (rtools 42 is required for\nR 4.2.0.)\nYou can confirm if your installation worked properly if\n\n\ndevtools::find_rtools()\n\n\n\nreturns TRUE. You can further confirm readiness with\n\n\ndevtools::has_devel()\n\n\n\nwhich returns\nYour system is ready to build packages!\nGit\nAs recommended in Happy Git Ch 6.2 Install Git on Windows, git is\ninstalled at\nC:/Program Files/Git.\nQuarto\nQuarto is an open\nsource publishing system backed by RStudio that began to receive public endorsement in April 2022.\nThe Quarto installation allowed me to choose between a user specific\nversus an all-user installation, which can be very helpful depending on\nyour administrative privileges. I chose the latter, and for me Quarto is\ninstalled at\nC:/Program Files/Quarto.\nConfigurations\n.Renviron\nThe .Renviron file contains environment variables that\nare set in R Sessions (this does not contain R code).\nTo edit this file, submit:\n\n\nusethis::edit_r_environ()\n\n\n\nThe user-level .Renviron file lives in the base of the\nuser’s home directory. For me, that means\nC:/Users/pileggis/OneDrive - Memorial Sloan Kettering Cancer Center/Documents/.Renviron\nMy .Renviron file has has been modified to establish\ndefault library locations for R package installations.\nR_LIBS_USER = \"C:/Program Files/R/library/%v\"\nAs explained in WTF\nCh 8.4 How to transfer your library when updating R, the\n%v wildcard automatically adjusts the installation folder\nwhen you update to a new R version.\n.Rprofile\nAs stated in WTF Ch 7.2 .Rprofile\n\nThe .Rprofile file contains R code to be run when R\nstarts up. It is run after the .Renviron file is\nsourced.\n\nAgain, you can edit this file with\n\n\nusethis::edit_r_profile()\n\n\n\nand mine lives at\nC:/Users/pileggis/OneDrive - Memorial Sloan Kettering Cancer Center/Documents/.Rprofile.\nMy .Rprofile has been modified two ways:\nEstablish a default location for R projects created via\nusethis::create_from_github() or\nusethis::use_course().\nTo enable RStudio shortcuts with the shrtcts package.\n\n\n# ------------------------------------------------------------------------------\n#  1/ Establish a default location for R projects created via \n# `usethis::create_from_github()` or `usethis::use_course()`\noptions(usethis.destdir = \"C:/Users/pileggis/Documents/gh-personal\")\n\n# ------------------------------------------------------------------------------\n# 2/ enable create RStudio shortcuts with the shrtcts package\nif (interactive() && requireNamespace(\"shrtcts\", quietly = TRUE)) {\n  shrtcts::add_rstudio_shortcuts(set_keyboard_shortcuts = TRUE)\n}\n\n\n\n.gitconfig\nYou can open your .gitconfig file for editing with\n\n\nusethis::edit_git_config()\n\n\n\nMy .gitconfig file lives at:\n\"C:/Users/pileggis/.gitconfig\"\nand has been modified to allow me to switch between work and personal\ngithub identities when working on R projects, which is described in more\ndetail in GitHub ssh and config with multiple accounts by\nTravis Gerke.\n.shrtcts.R\nThe .shrtcts.R file contains R code that defines the\nenabled shortcuts. Mine is located in my R home directory at\n\n\nshrtcts::locate_shortcuts_source()\n\n\n\n\"C:/Users/pileggis/OneDrive - Memorial Sloan Kettering Cancer Center/Documents/.shrtcts.R\"\nI currently have two shortcuts enabled:\nbreaker of chains (mentioned in Background), and\nautomatically end comments with dashes to 80 characters - see\nthis tweet for details.\nSummary\nThis topic probably isn’t exciting for many, but having this\nunderstanding can save you time during installation and troubleshooting.\nAnd maybe next I get set up on a new OS it will go quicker. 😊\nAcknowledgements\nThanks to Travis\nGerke for inspiring me to try new things in R! Thanks to Omar Chua,\nwho provided hours of IT support as I was waiting for administrative\nprivileges. And thank you to Jenny Bryan and David Aja\nfor their feedback on this post.\n\n\n\n\n\n\n",
    "preview": "posts/2022-06-02-locating-r-and-r-adjacent-software-and-configuration-files/R-file-tree.png",
    "last_modified": "2022-08-11T11:39:30-04:00",
    "input_file": {},
    "preview_width": 1280,
    "preview_height": 768
  },
  {
    "path": "posts/2022-05-12-code-line-highlighting-in-quarto-revealjs-presentations/",
    "title": "Code line highlighting in Quarto revealjs presentations",
    "description": "Three methods make your code lines stand out",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2022-05-12",
    "categories": [],
    "contents": "\n\nContents\nTL; DR\nTerminology\ncode-line-numbers\nMethod 1:\nMethod 2:\nMethod 3:\nSummary\nAcknowledgments\n\n\n\n\nFigure 1: Left hand side shows .qmd source code;\nright hand side shows rendered slides with line highlighting.\n\n\n\nTL; DR\nI started playing with Quarto revealjs presentations, it is fun! Here\nare three methods I learned to highlight code lines.\nTerminology\nSome terminology introduced in the Quarto documentation includes:\nexecutable / compute vs non-executable / non-compute\nThis differentiates code that computes or executes versus code that\ndoes not (display only code).\n\ncode chunk vs code block\nGenerally, it seems code chunk is more consistent with executable /\ncompute code, whereas as code block more typically refers to display\nonly code.\n\ncode cells / cell option\nI think this Quarto specific lingo for code chunks and options\nwithin code chunks. In Quarto, you can use some of the existing chunk\noptions from RMarkdown, as well as new Quarto specific options.\nFrom the documentation, “Cell options affect the execution and\noutput of executable code blocks.”\n\nFor more information on displaying code, see:\nQuarto -> Guide -> Tools -> Visual Editor -> Technical Writing -> Displaying code\nhttps://quarto.org/docs/visual-editor/technical.html#displaying-code\nFor a complete overview of all code block options supported in\nQuarto, see\nQuarto -> Reference -> Code Cells -> Knitr.\nhttps://quarto.org/docs/reference/cells/cells-knitr.html#code-output\ncode-line-numbers\nTo highlight code lines in Quarto revealjs presentations, use the\noption code-line-numbers. This is a Quarto specific cell\noption for Quarto presentation formats of revealjs. This\nmeans that code-line-numbers will not render code\nhighlighting when used in Quarto documents (.qmd) of format\nhtml, nor within an R markdown document\n(.Rmd). Note that Quarto cell options have a dash their\nname.\nI first found this option in presentation revealjs documentation:\nQuarto -> Guide -> Formats -> Presentations -> Revealjs -> Code Blocks.\nhttps://quarto.org/docs/presentations/revealjs/#code-blocks\nMethod 1:\n\n```{r}\n#| echo: TRUE\n#| eval: FALSE\n#| code-line-numbers: \"1\"\nx <- 1:10\nx\nLETTERS[x]\n```\n\nHere, we are executing code cell options via special comments\nindicated by #|. Comment-based syntax is recommended for\nQuarto to make documents more portable and consistent across execution\nengines. See\nQuarto -> Guide -> Computations -> Using R -> Chunk Options.\nhttps://quarto.org/docs/computations/r.html#chunk-options\nMethod 2:\n\n```{r, echo=TRUE, eval=FALSE, `code-line-numbers`=\"2\"}\nx <- 1:10\nx\nLETTERS[x]\n```\n\nQuarto code cell options have dash in their name; R only supports\ndashes in variable names when wrapped in the back tick.\nMethod 3:\n\n```{.r code-line-numbers=\"3\"}\nx <- 1:10\nx\nLETTERS[x]\n```\n\n.r signals a non-compute code block (which is what we\nused above with eval=FALSE). These are verbatim code blocks\nwith a class set on them for language highlighting. In this case, the\nlanguage is R.\nSummary\nThere is a lot of wonderful Quarto documentation, but navigating it\nall takes practice and time. I am new to Quarto, so suggestions and\ncorrections to this post are most welcome.\nFor a quick starter on this topic, you can download the full Quarto\nrevealjs presentation demonstration\nquarto-code-hl.qmd.\nAnd here are the rendered presentation slides, which you can click\nthrough.\n\n\n\n\n\nAcknowledgments\nThank you very much to Chris Dervieux for kindly explaining all of\nthis to me on RStudio Community.\n\n\n\n",
    "preview": "posts/2022-05-12-code-line-highlighting-in-quarto-revealjs-presentations/quarto-code-hl-layout.PNG",
    "last_modified": "2022-08-11T11:39:30-04:00",
    "input_file": {},
    "preview_width": 1280,
    "preview_height": 720
  },
  {
    "path": "posts/2022-01-24-report-ready-pdf-tables-with-rmarkdown-knitr-kableextra-and-latex/",
    "title": "Report Ready PDF tables with rmarkdown, knitr, kableExtra, and LaTeX",
    "description": "Customized raw data tables and gtsummary tables in PDF output",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2022-01-24",
    "categories": [],
    "contents": "\n\nContents\nTL; DR\nPackages\nBackground\nDocument Set-up\nYAML\nLaTeX\nknitr options\n\nRaw Data Tables\nDefault Column Names\nCustom Column Names\n\nSummary Data Tables\nDefault column names\nCustom column names\n\nSummary\nAcknowledgements\n\n\n\n\nFigure 1: Artwork by @allison_horst.\n\n\n\nTL; DR\nThis post was updated on 2022-07-05 as updates in from\ngtsummary 1.5.1 to 1.6.1 greatly streamlined exporting\nsummary tables to pdf.\nCustomizing tables in PDF output is possible with {knitr}\n, {kableExtra} and a bit of LaTeX.\nUpdates available in {gtsummary} 1.5.1 also allow users to\nmore easily take advantage of these features in summary tables.\n\n\n\nFigure 2: Scrolling PDF of penguins report with custom tables.\n\n\n\nCheck out the source file for the Penguins Report and rendered\nresults:\npenguins_report.Rmd\npenguins_report.pdf\nPackages\nThis material was developed using:\nSoftware / package\nVersion\nR\n4.2.0\nRStudio\n351 “Ghost Orchid”\nrmarkdown\n2.11\nknitr\n1.37\nkableExtra\n1.3.4\ntinytex\n0.34\ngtsummary\n1.6.1\npandoc\n2.14.0.3\npalmerpenguins\n0.1.0\n\n\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(gtsummary)\nlibrary(palmerpenguins)\n\n\n\nBackground\n\n\nCan anyone point me to a good R package that can create tables that\nare easily outputted in PDF. So far every package I have found seems to\nrequire numerous external packages and plug-ins in order to output the\ntable as a PDF document. Any advice welcome.\n\n— Charlie Harper (@charlieharperuk)\nJanuary\n20, 2022\n\nYou and me both, Charlie! This is tricky. I tried to avoid the LaTeX\nroute through {pagedown} , but ultimately because I\nhad many tables that varied in size and length, this was not a quick\napproach.\nHere is a solution I have landed upon; I hope it helps you and others\nas well. If anyone has additional tips or approaches, please share in\nthe comments!\nFor a comprehensive overview of the many reporting options available\nvia RMarkdown, and how to customize them, check out the excellent 2021\nRStudio webinar Business Reports with R Markdown by Christophe\nDervieux.\nDocument Set-up\nHere is the initial set up of my .Rmd document,\nincluding the YAML, some knitr options, and some LaTeX\noptions.\n\n---\ntitle: \"Penguins Report\"\nauthor: \"Shannon Pileggi\"\ndate: \"`r Sys.Date()`\"\noutput:\n  pdf_document: \n    toc: true\n    toc_depth: 2\n    number_sections: true\n    keep_tex: yes\nlatex_engine: pdflatex\nclassoption: landscape\nheader-includes:\n  \\usepackage{helvet}\n  \\renewcommand\\familydefault{\\sfdefault}\ninclude-before:\n- '`\\newpage{}`{=latex}'\n---\n\n\\let\\oldsection\\section\n\\renewcommand\\section{\\clearpage\\oldsection}\n\noptions(knitr.kable.NA = '') \n\nYAML\nkeep_tex: yes This can be useful for reviewing the\ntex output to troubleshoot errors. For more ideas on how to leverage\nthis, check out the blog post Modifying R Markdown’s LaTeX styles by Travis\nGerke.\nlatex_engine: pdflatex The LaTeX engine can be\nchanged to take advantage of other LaTeX features; see R Markdown: The Definitive Guide Ch 3.3.7 Advanced\nCustomization for details.\nclassoption: landscape Changes orientation from\nportrait to landscape for wide tables.\nheader-includes: \\usepackage{helvet} \\renewcommand\\familydefault{\\sfdefault}\nChanges the default font from serif to sans serif.\ninclude-before: - '{=latex}' Creates a\npage break in between title page and table of contents.\nLaTeX\n\\let\\oldsection\\section \\renewcommand\\section{\\clearpage\\oldsection}\ncreates a page break for each new numbered top level section.\nknitr options\noptions(knitr.kable.NA = '') displays blank instead of\nNA for missing values.\nRaw Data Tables\nDefault Column Names\nHere are options I used to create a basic table with default column\nnames.\n\n\npenguins %>% \n  knitr::kable(\n    format = \"latex\",\n    align = \"l\",\n    booktabs = TRUE,\n    longtable = TRUE,\n    linesep = \"\",\n    ) %>%\n  kableExtra::kable_styling(\n      position = \"left\",\n      latex_options = c(\"striped\", \"repeat_header\"),\n      stripe_color = \"gray!15\"\n    )\n\n\n\nalign is vectorized and can be used to control alignment of\nall columns; i.e., align = rep(\"l\", 8) can also be used to\nequivalently left justify all columns.\n\n\n\nFigure 3: Raw data table PDF output with default column\nnames.\n\n\n\nMany of knitr::kable() arugments are passed as\n... Other arguments, and are described in more\ndetail in the help file of kableExtra::kbl().\nformat = \"latex\" specifies the output\nformat.\nalign = \"l\" specifies column alignment.\nbooktabs = TRUE is generally recommended for\nformatting LaTeX tables.\nlongtable = TRUE handles tables that span multiple\npages.\nlinesep = \"\" prevents default behavior of extra\nspace every five rows.\nAdditional styling options are specified with\nkableExtra::kable_styling().\nposition = \"left\" places table on left hand side of\npage.\nlatex_options = c(\"striped\", \"repeat_header\")\nimplements table striping with repeated headers for tables that span\nmultiple pages.\nstripe_color = \"gray!15\" species the stripe color\nusing LaTeX color specification from the xcolor package - this specifies a mix of 15% gray\nand 85% white.\nCustom Column Names\nI was also interested in implementing column names with specific line\nbreaks, which is a bit more complicated. To achieve this, use both\ncol.names and escape = FALSE. Be cautious with\nescape = FALSE as this may cause rendering errors if your\ntable contains special LaTeX characters like \\ or\n%.\n\n\n# original column names\nnames(penguins)\n\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\n\n#Create column names with line breaks for demonstration.\ncolumn_names <- penguins %>% \n  names() %>% \n  str_replace_all( \"_\", \"\\n\")\n\ncolumn_names\n\n\n[1] \"species\"             \"island\"              \"bill\\nlength\\nmm\"   \n[4] \"bill\\ndepth\\nmm\"     \"flipper\\nlength\\nmm\" \"body\\nmass\\ng\"      \n[7] \"sex\"                 \"year\"               \n\n\n\npenguins %>% \n  knitr::kable(\n    format = \"latex\",\n    booktabs = TRUE,\n    longtable = TRUE,\n    linesep = \"\",\n    align = \"l\",\n    col.names = linebreak(column_names, align = \"l\"),\n    escape = FALSE\n    ) %>%\n  kableExtra::kable_styling(\n      position = \"left\",\n      latex_options = c(\"striped\", \"repeat_header\"),\n      stripe_color = \"gray!15\"\n    )\n\n\n\n\n\n\nFigure 4: Raw data table PDF output with custom column\nnames with line breaks.\n\n\n\nSummary Data Tables\nWith the release of gtsummary 1.5.1, these print to pdf\nfeatures are now also available for summary tables through updates to gtsummary::as_kable_extra().\nDefault column names\nApply styling as desired with gtsummary; for\nexample, bold labels.\nPass the same options to gtsummary::as_kable_extra()\nthat can be passed to knitr::kable() /\nkableExtra::kbl().\nFinish with additional kableExtra::kable_styling()\nspecifications.\n\n\npenguins %>% \n  gtsummary::tbl_summary(\n    by = species\n  ) %>% \n  gtsummary::bold_labels() %>% \n  gtsummary::as_kable_extra(\n    format = \"latex\",\n    booktabs = TRUE,\n    longtable = TRUE,\n    linesep = \"\"\n    ) %>%\n  kableExtra::kable_styling(\n      position = \"left\",\n      latex_options = c(\"striped\", \"repeat_header\"),\n      stripe_color = \"gray!15\"\n    )\n\n\n\n\n\n\nFigure 5: Summary tables PDF output with default\ncolumn names.\n\n\n\nCustom column names\nWhen this post was originally published on 2022-01-24, this was\nharder. Thanks to updates in gtsummary 1.6.1, custom column\nnames can now be implemented directly in modify_header()\nand seamlessly rendered to pdf via kableExtra.\n\n\npenguins %>% \n  gtsummary::tbl_summary(\n    by = species,\n    statistic = list(all_categorical() ~ \"{n} ({p}%)\")\n  ) %>%\n  gtsummary::bold_labels() %>%   \n  gtsummary::modify_header(\n    label = \"**Characteristic**\", \n    all_stat_cols() ~ \"**{level}**\\nN = {n}\"\n  ) %>%\n  gtsummary::as_kable_extra(\n    format = \"latex\",\n    booktabs = TRUE,\n    longtable = TRUE,\n    linesep = \"\"\n    ) %>%\n  kableExtra::kable_styling(\n      position = \"left\",\n      latex_options = c(\"striped\", \"repeat_header\"),\n      stripe_color = \"gray!15\"\n    )\n\n\n\n\n\n\nFigure 6: Summary tables PDF output with custom\ncolumn names, including line breaks and bolding.\n\n\n\nSummary\nWith a little bit of LaTeX and fairy dust 🧙, report ready PDF tables\nare possible. 🥂\nAcknowledgements\nThank you Daniel Sjoberg for updating {gtsummary} to make\nprinting to pdf more streamlined for summary tables! 🎉 And for kindly\nproviding feedback on this post. Also, thanks to Travis Gerke\nfor tips on leveraging LaTeX via rmarkdown.\n\n\n\n",
    "preview": "posts/2022-01-24-report-ready-pdf-tables-with-rmarkdown-knitr-kableextra-and-latex/img/pdf_scroll.gif",
    "last_modified": "2022-08-11T11:39:30-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-13-modifying-the-github-repo-containing-my-distill-website/",
    "title": "Modifying the GitHub repo containing my Distill website",
    "description": "Adventures with giscus (easier) and GitHub Actions (harder)",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2021-12-13",
    "categories": [],
    "contents": "\n\nContents\nTL; DR\nGiscus commenting\nGitHub Actions README\nAfterwards\nAcknowledgements\n\n\n\n\nFigure 1: Artwork by @allison_horst. Except in my case, I just needed 12 days of intermittent attempts.\n\n\n\nTL; DR\nI made two modifications to my distill website and the corresponding GitHub repository:\nAdded giscus commenting to my blog - this was fairly straightforward and completed within 1 to 2 hours.\nAdded GitHub actions to automatically update the repository’s README with blog stats - this took several attempts over the course of a few days.\nGiscus commenting\nStraightforward and completed within 1 to 2 hours.\nUpdating the commenting mechanism on my website from discus was something I had been meaning to do for a while, but held off on because you never know when a seemingly small change will take you down a rabbit hole.\nI went for it, and thanks to Joel’s awesome post Enable giscus in Distill it was a pretty quick and easy process. 🙌\n\n\ntonight i intended to update my #rstats blog from disqus to utterances, but then i found this great post by @joel_nitta about ✨giscus✨! thanks, Joel! 👏i hope the new giscus gets used - go ahead, pipe up!😉https://t.co/Wx1j80TKVD\n\n— Shannon Pileggi (@PipingHotData) December 1, 2021\n\nGitHub Actions README\nSeveral attempts over the course of a few days.\nAfter the amazingly quick success of the giscus implementation, I was ready to tackle more! I thought a mini-project on my blog would be a nice entry point to learn GitHub Actions, and I decided to replicate Matt Dray’s post Up-to-date blog stats in your README. Between intricacies with both the README and the GitHub Action workflow yaml, this happened:\n\n\nhow is figuring out #rstats github actions going, you ask? preeetttty well 👍 pic.twitter.com/Boj7KvXCAi\n\n— Shannon Pileggi (@PipingHotData) December 3, 2021\n\nI mean, sometimes you can skip reading documentation fully, make a few tweaks to your code, cross your fingers, and hope that everything works. GitHub Actions is probably not one of those things. 😬 Below are a few of the errors I suffered through to get this working; hopefully I can save you some of the pain!\nGitHub Actions\nWorkflow #1 (error) I needed to add {distill} as an installed package in my workflow. 🤷\nWorkflow #2 (error) I was lazy with my README and workflow and loaded tidvyerse instead of the specific packages I needed, which didn’t go well. I tried to resolve this via the pak route (see next error), which also didn’t go well, and ended up trimming back the dependencies to the essential ones.\n\nCheck out Alison Hill working through some unexpected GitHub Actions problems with bookdown & Netlify!\nWorkflow #10 (error) I tried a few different things to install packages in the workflow with pak instead of install.packages. I attempted to use r-lib/actions/setup-r-dependencies, and I really should have read the documentation thoroughly. My blog repo is not a package and does not have DESCRIPTION file. I tried to add a DESCRIPTION file to coerce this, but really, this workflow setup is made for packages. I also attempted to review the source of the workflow and make some tweaks to achieve something similar outside of the package framework, but to no avail. My workflow currently uses install.packages(). I could have tried r-lib/actions/setup-renv instead, but I was tired. There are many errors in my workflow history that correspond to this experimentation.\nREADME\nWorkflow #30 (no error) I thought I would get fancy and make the ggplot graphic interactive through {ggiraph} with hover over and click effects. This didn’t error out, but it also didn’t work, which I should have anticipated, as the rendered file is a .md and not a .Rmd.\nWorkflow #26 (no error) I had many workflows that didn’t error out, but also did not result in an updated README. I actually had a lot of trouble getting the README to work. Some things I learned include:\nusethis::use_readme_rmd() is designed for use with R packages, and again, didn’t work well on my blog repo.\ndevtools::build_rmd() is designed for use with R packages, and again, didn’t work well on my blog repo.\nAfter I ditched these and my README still wasn’t rendering correctly, I really, truly thought I was going crazy. I finally found a solution on RStudio Community by updating rmarkdown::render(\"README.Rmd\") to rmarkdown::render(\"README.Rmd\", \"github_document\") in the YAML of the GitHub Action workflow. I wonder if {distill} has some YAML stuff going on that required the \"github_document\" argument.\n\nAfter much perseverance and persistence, I prevailed with my first GitHub Action. 💪\n\n\n\nFigure 2: Updated README file for the pipinghotdata_distill GitHub repository, with the stats section automatically updated daily with GitHub Actions.\n\n\n\nPerhaps reading the documentation more closely or slowing down to think through things might have worked better for this situation. If you want to have a laugh or or want to feel better about your own struggling efforts, browse through my 34 commits or check out the corresponding GitHub Action history Dec 1 - 12, 2021. Despite the frustration, I remain grateful for these experiences as I learned a lot.\nAfterwards\nSo what happened after these changes?\nGiscus\nOne unexpected side effect of using giscus is all of the automatic notifications I receive about the discussion. I get notifications for:\non my blog: all comments\non blogs by others:\nreplies to my comments\nall comments subsequent to mine, by anyone, as we are now all engaged in the same GitHub discussion.\n\nThe last one was a bit surprising to me, but I like it for now!\nGitHub Actions\nNow that I am using GitHub Actions on the README of my repository, I am no longer the only contributor to my repository! 😯 This means that when I open up my R project, I now need to remember to pull before I start working on my website.\nIn addition, despite my GitHub Actions working, I ran out of free time as shown in the workflow #39 error. 🤦 I guess I either need to figure out {pak} out with GitHub Actions to hopefully speed things up, or experiment less to get my actions right.😆\nAcknowledgements\nThanks to:\nDan Sjoberg and Travis Gerke for talking me through some GitHub Action concepts!\nAllison Horst for the amazing illustrations.\nJoel Nitta for the giscus post.\nMatt Dray for the readme github action post.\nChristophe Dervieux for being a hero on RStudio Community.\n\n\n\n",
    "preview": "posts/2021-12-13-modifying-the-github-repo-containing-my-distill-website/woof_today.png",
    "last_modified": "2022-08-11T11:39:29-04:00",
    "input_file": {},
    "preview_width": 1627,
    "preview_height": 927
  },
  {
    "path": "posts/2021-11-23-getting-started-with-unit-testing-in-r/",
    "title": "Getting started with unit testing in R",
    "description": "Automating the dev work you are probably already doing",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2021-11-23",
    "categories": [],
    "contents": "\n\nContents\nTL;DR\nWorkshop recording\nAbstract\nPackages\nTool kit\nWhy unit test\nGetting started\nKeyboard shortcuts\n(9:00) Status of ralph\n(16:30) Test set up\n(25:10) First test\n(36:08) Test coverage\n(41:27) Debugging\n(1:00:20) Testing function inputs\n(1:14:04) More\nQuestion & Answer\nPersonal reflection\nAcknowledgements\n\n\n\n\nFigure 1: Question and answer, paraphrased, from the workshop.\n\n\n\nTL;DR\nThis blog post accompanies the R-Ladies Philly workshop on Nov 11, 2021 (recording on YouTube), where we live coded use of the usethis, devtools, testthat, and covr packages to automate R package testing. This post complements the recording through highlighting key points, rather than listing all actions.\nWorkshop recording\n\n\nAbstract\nIn this workshop, Shannon Pileggi and Gordon Shotwell discuss how to get started with unit testing in R, which is formal automated testing of functions within packages. We demonstrate handy functions in usethis and devtools, strategies for writing tests, debugging techniques, and broad concepts in function writing that facilitate a smoother testing process.\nThis workshop picks up exactly where we left our little ralph (aka R-Ladies Philly) package one year ago with “Your first R package in 1 hour: Tools that make R package development easy”. Participants will get the most out of this workshop if they review those materials in advance, or if they are already familiar with building R packages with devtools and usethis.\nPackages\nThis material was developed using:\nSoftware / package\nVersion\nR\n4.1.1\nRStudio\n351 “Ghost Orchid”\nusethis\n2.1.2\ndevtools\n2.4.2\ntestthat\n3.1.0\ncovr\n3.5.1\nbroom\n0.7.9\nglue\n1.4.2\nmagrittr\n2.0.1\npurrr\n0.3.4\nrlang\n0.4.12\nTool kit\nThis table contains the general process functions used in this workshop. Single usage functions only need to be used one time in the development process; multiple usage functions are executed as needed.\nUsage\nFunction\nPurpose\nSingle\nusethis::use_testthat()\ninitialize testing infrastructure\nMultiple\nusethis::use_test()\ncreate a new test file\n\ndevtools::test()\nexecute and evaluate all tests in package\n\ncovr::report()\nreports test coverage\n\nbrowser()\ndebugging: interrupt and inspect function execution\n\ndevtools::check()\nbuild package locally and check\n\ndevtools::load_all()\nload functions in 📂 R/ into memory\n\nusethis::use_r(\"function\")\ncreate R script for function\n\nusethis::use_package(\"package\")\nadd package dependency\n\ndevtools::document()\nbuild and add documentation\nOther resources:\nR package development cheat sheet\nCh. 12 Testing in R packages by Hadley Wickham and Jenny Bryan\nCh. 8.2 Signalling conditions in Advanced R by Hadley Wickham\ntestthat package documentation\n{usethis} user interface functions\ncovr package documentation\nBuilding tidy tools workshop at rstudio::conf(2019) by Hadley Wickham and Charlotte Wickham\nDebugging\nRStudio blog post by Jonathan McPherson Debugging with the RStudio IDE\nCh 22 Debugging in Advanced R by Hadley Wickham\n\nIndrajeet Patil’s curated list of awesome tools to assist R package development.\nWhy unit test\nIf you write R functions, then you already test your code. You write a function, see if it works, and iterate on the function until you achieve your desired result. In the package development work flow, it looks something like this:\n\n\n\nFigure 2: Workflow cycle of function development without automated testing, from 2019 Building Tidy Tools workshop.\n\n\n\nThis process is where most of us likely start, and possibly even stay for a while. But it can be tedious, time consuming, and error-prone to manually check all possible combinations of function inputs and arguments.\nInstead, we can automate tests with testthat in a new workflow.\n\n\n\nFigure 3: Workflow cycle of function development when getting started with automated testing, from 2019 Building Tidy Tools workshop.\n\n\n\nAnd once you trust and get comfortable with the tests you have set up, you can speed up the development process even more by removing the reload code step.\n\n\n\nFigure 4: Workflow cycle of function development when comfortable with automated testing, from 2019 Building Tidy Tools workshop.\n\n\n\nLike anything in programming, there is an up-front time investment in learning this framework and process, but with potentially significant downstream time savings.\nGetting started\nThis post picks up exactly where we left the ralph package in Your first R package in 1 hour in November 2020. In order to keep that as a stand-alone resource, I created a second repo called ralphGetsTested for this workshop, which was a copy of ralph as we left it.\nIf you want to follow along with the unit testing steps and practice yourself, fork and clone the ralph repo.\nusethis::create_from_github(\"shannonpileggi/ralph\")\nIf you want to see the repository as it stood at the conclusion of the unit testing workshop, fork and clone the ralphGetsTested repo.\nusethis::create_from_github(\"shannonpileggi/ralphGetsTested\")\nKeyboard shortcuts\nCtrl + S for save file\nCtrl + Shift + L for devtools::load_all()\nCtrl + Shift + F10 to restart R\nCtrl + Shift + T for devtools::test()\n(9:00) Status of ralph\nThe little ralph package has a single, untested function that computes a correlation and returns tidy results.\n\n\n\n\n\ncompute_corr <- function(data, var1, var2){\n\n  # compute correlation ----\n  stats::cor.test(\n    x = data %>% dplyr::pull({{var1}}),\n    y = data %>% dplyr::pull({{var2}})\n  ) %>%\n  # tidy up results ----\n  broom::tidy() %>%\n  # retain and rename relevant bits ----\n  dplyr::select(\n    correlation = .data$estimate,\n    pval = .data$p.value\n  )\n\n}\n\n\n\nHere is an example execution:\n\n\ncompute_corr(data = faithful, var1 = eruptions, var2 = waiting)\n\n\n# A tibble: 1 x 2\n  correlation      pval\n        <dbl>     <dbl>\n1       0.901 8.13e-100\n\n(16:30) Test set up\nTo get your package ready for automated testing, submit\n\n\nusethis::use_testthat()\n\n\n\nFrom R Packages Ch 12 Testing, this does three things:\nCreates a tests/testthat directory.\nAdds testthat to the Suggests field in the DESCRIPTION.\nCreates a file tests/testthat.R that runs all your tests when you execute devtools::check().\n\n\n\nFigure 5: (17:54): DESCRIPTION and console output after submitting usethis::use_testthat().\n\n\n\nNote that the edition of testthat is specified to 3, which departs a bit from previous versions both in function scopes and execution.\n\nDan Sjoberg shared that when he updated {gtsummary} to testthat 3 with the help of a friendly PR, the R CMD check dropped from 40 to 14 minutes. 🙌\n(25:10) First test\nTo create our first test file, submit\n\n\nusethis::use_test(\"compute_corr\")\n\n\n\n. Here, we name this file the same as our function name. This creates a new file under tests -> testthat named test-compute_corr.R, and the file pre-populates with an example test that we can replace.\nFor our first test, we create an object that contains the expected results of an example function execution, and then we assess the correctness of the output using the testthat::expect_ functions.\nWe name the overall test chunk assess_compute_corr - you can name this whatever would be useful for you to read in a testing log. In this test, we evaluate if the function returns the correct class of object, in this case, a data.frame.\n\n\ntest_that(\"assess_compute_corr\", {\n  expected <- compute_corr(data = faithful, var1 = eruptions, var2 = waiting)\n  expect_s3_class(expected, \"data.frame\")\n})\n\n\n\nNow there are two ways to execute the test.\nThe Run Tests button (  )on the top right hand side of the testing script executes the tests in this script only (not all tests in the package), and excutes this in a fresh R environment.\nSubmitting devtools::test() (or Ctrl + Shift + T) executes all tests in the package in your global environment.\nHere, we submit our first test with the Run Tests button, and it passes! 🎉\n\n\n\nFigure 6: (29:23) Passing result after submitting the first test with the Run Tests button.\n\n\n\nSubmitting devtools::check() will also execute all tests in the package, and you will see an error in the log if the test fails.\n(36:08) Test coverage\nNow we have added a total of three tests, and we examine our test coverage with\n\n\ncovr::report()\n\n\n\nThis function requires the package to not be loaded, so restart R (Ctrl + Shift + F10) prior to execution.\n\n\n\nFigure 7: (37:05) covr::report() creates output in viewer showing the percent of code covered by the tests.\n\n\n\nThe percent coverage evaluates the percent of code that was ever executed during through the test_that() functions. In the viewer panel, click on Files and then R/compute_corr.R. The 1x on the left hand side counts how many times that line of code has been executed when you ran your test suite; in our case, each line of code was executed one time.\n\n\n\nFigure 8: (37:19) covr::report() shows which lines of code were executed, and how many times, as a result of your tests.\n\n\n\n(41:27) Debugging\nAt this point, our tests consist of the following:\n\n\ntest_that(\"assess_compute_corr\", {\n  expected <- compute_corr(data = faithful, var1 = eruptions, var2 = waiting)\n  expect_s3_class(expected, \"data.frame\")\n  expect_equal(dim(expected), c(1,2))\n  expect_named(expected, c(\"correlation\", \"pval\"))\n  expect_equal(expected[[\"correlation\"]], 0.901)\n})\n\n\n\nand we executed our tests with Ctrl + Shift + T.\n\n\n\nFigure 9: (42:18) Our first test failure.\n\n\n\nNow, we see our first failed test that we need to debug, which was triggered by expect_equal().\nWe first demonstrate some debugging in the console, where we reveal that the correlation column of the expected tibble has a \"names\" attribute of \"cor\".\n\n\n\n\n\nstr(expected)\n\n\ntibble [1 x 2] (S3: tbl_df/tbl/data.frame)\n $ correlation: Named num 0.901\n  ..- attr(*, \"names\")= chr \"cor\"\n $ pval       : num 8.13e-100\n\nNext, we demonstrate an alternative way of arriving at this through use of the browser() function. To do so,\nInsert browser() into the source of your function.\nLoad your functions with devtools::load_all() (Ctrl + Shift + L).\nExecute the function.\n\n\n\nFigure 10: (46:31) Now we have entered browser mode.\n\n\n\nHere, this allows you to step into your function with arguments exactly as called. The function is run in a fresh environment, and we can execute the function line by line to see what is happening. In addition, we can see objects as they are evaluated in the environment.\nTo resolve the issue:\nStop browser mode with the red Stop square on the console,\nModify the source of the function as shown.\n\n\ncompute_corr <- function(data, var1, var2){\n\n  # compute correlation ----\n  results <- stats::cor.test(\n    x = data %>% dplyr::pull({{var1}}),\n    y = data %>% dplyr::pull({{var2}})\n  ) %>%\n  # tidy up results ----\n  broom::tidy() %>%\n  # retain and rename relevant bits ----\n  dplyr::select(\n    correlation = .data$estimate,\n    pval = .data$p.value\n  )\n\n  attr(results$correlation, \"names\") <- NULL\n\n  return(results)\n\n}\n\n\n\ndevtools::load_all() (Ctrl + Shift + L)\nIf needed, step into browser() mode again to confirm or further troubleshoot.\nNow, after updating the compute_corr() function to remove the names attribute, we still have failed test! 😱\n\n\n\nFigure 11: (58:02) Now we have a new failure message. Note the debug at lines in the Build pane indicates that I forgot to remove the browser() line from the function source after our workshop break.\n\n\n\nWe can correct this by adding the tolerance argument to the expect_equal() function.\n\n\n\nFigure 12: (1:00:14) Four passing tests!\n\n\n\n(1:00:20) Testing function inputs\nThe {testthat} package explicitly evaluates the outputs of your function. To determine if a user correctly specifies inputs to your function,\nAssess the function inputs programmatically in the source of your function.\nReturn a signal, such as an error, if the input does not conform to expectations.\nFormalize this catch in a test with functions such as testthat::expect_error().\nTo check if the supplied variables actually exist in the data set, we add the following to compute_corr.R:\n\n\ncompute_corr <- function(data, var1, var2){\n\n  var1_chr <- rlang::as_label(rlang::ensym(var1))\n  var2_chr <- rlang::as_label(rlang::ensym(var2))\n\n  # alert user if variable is not in data set ----\n  if (!(var1_chr %in% names(data))){\n    #usethis::ui_stop(\"{var1_chr} is not in the data set.\")\n    stop(glue::glue(\"{var1_chr} is not in the data set.\"))\n  }\n\n  # alert user if variable is not in data set ----\n  if (!(var2_chr %in% names(data))){\n    stop(glue::glue(\"{var2_chr} is not in the data set.\"))\n  }\n\n  # compute correlation ----\n  results <- stats::cor.test(\n    x = data %>% dplyr::pull({{var1}}),\n    y = data %>% dplyr::pull({{var2}})\n  ) %>%\n  # tidy up results ----\n  broom::tidy() %>%\n  # retain and rename relevant bits ----\n  dplyr::select(\n    correlation = .data$estimate,\n    pval = .data$p.value\n  )\n\n  attr(results$correlation, \"names\") <- NULL\n\n  return(results)\n\n}\n\n\n\nYou can enforce an error with\nthe stop() function from base R,\nthe stopifnot() function in base R, discussed later at (1:32:40), or\nusethis::ui_stop(), which provides some additional functionality to the user including show traceback and rerun with debug, but also adds another dependency to your package.\nRead Ch. 8.2 Signalling Conditions of Advanced R to learn more about messages, warnings, and errors.\nWhen we execute the function with faulty inputs, we see our error with our handy note:\n\n\ncompute_corr(data = faithful, var1 = erruptions, var2 = waiting)\n\n\nError in compute_corr(data = faithful, var1 = erruptions, var2 = waiting): erruptions is not in the data set.\n\nNow we add an additional test to catch the error and resubmit devtools::test().\n\n\ntest_that(\"assess_compute_corr\", {\n  expected <- compute_corr(data = faithful, var1 = eruptions, var2 = waiting)\n  expect_s3_class(expected, \"data.frame\")\n  expect_equal(dim(expected), c(1,2))\n  expect_named(expected, c(\"correlation\", \"pval\"))\n  expect_equal(expected[[\"correlation\"]], 0.901, tolerance = 0.001)\n  # catching errors\n  expect_error(compute_corr(data = faithful, var1 = erruptions, var2 = waiting))\n})\n\n\n\n\n\n\nFigure 13: (1:09:05) devtools::test() shows in the console that we have FIVE passing tests! 🥳\n\n\n\nNote: This introduces an additional dependency to the package through including glue or usethis in compute_corr(). Don’t forget to declare these dependencies with usethis::use_package(\"package-name\").\n(1:14:04) More\nFor the remainder of the time, we explore the awesome_rladies() function:\n\n\nawesome_rladies <- function(v) {\n  \n  sapply(v, function(x) {\n    if (x == 1) {\n      verb <- \"is\"\n      noun <- \"RLady\"\n    }\n    \n    if (x > 1) {\n      verb <- \"are\"\n      noun <- \"RLadies\"\n    }\n    \n    as.character(glue::glue(\"There {verb} {x} awesome {noun}!\"))\n  })\n}\n\n\n\nHere are example executions:\n\n\nawesome_rladies(1)\n\n\n[1] \"There is 1 awesome RLady!\"\n\nawesome_rladies(1:2)\n\n\n[1] \"There is 1 awesome RLady!\"    \"There are 2 awesome RLadies!\"\n\nWe discuss the following:\nCan we break this up to make it easier to test?\nNote: Eventually, testing will likely end up as an exercise in refactoring your code - breaking it down such that the simplest elements each belong to a function that can be individually and independently tested.\nWhat type of object should the function output?\nWhat type of object does this function expect, can we put up guardrails so the user doesn’t send the wrong thing? How do we test those guardrails?\nRather than write out these details in this already long post, you may watch in the recording!\nQuestion & Answer\nThis discussion is paraphrased from the workshop recording. It was commented throughout that there is not a right answer to most of this. 🤷\n(7:20) What is your take on getting started with unit testing?\nThere is something about unit testing that sounds really scary, like it is something that real programmers do and regular people don’t do, but often times it is the opposite. When you get familiar with testing your own code, it is an easy way to combat others criticizing your work because you can look at your code and see its test coverage. This is standard across different languages and types of programming. What it means for your code to be correct is that it passes tests, so this can be a fairly objective way to defend your work.\n(19:50) When do you write tests?\nIf the function writing is a process of discovery, and you are not sure what the function will do, write the test after the function is in a stable state. In other cases, when you know precisely how you want the function to behave, writing the test before you write the function could be a useful approach (test driven development).\n(22:19) When I start writing tests, I get sucked into a rabbit hole of tests. How can I have a better process for writing tests?\nHave solid tests for things that you really care about as a developer. Most of the time, it is good to write more tests than less. Get a couple of tests down, and as you discover bugs, make sure there is a test for every bug that you fix.\n(23:53) Is it fair to say that you should consider problems that tripped you up when building the function as good test candidates?\nYeah, for sure! If you make the mistake as you are writing the function, you are likely to make the mistake again in six months, so a test will keep you honest on those problems.\n(32:50) Can we write test_that() statements with more descriptive errors?\nThink of test_that() as a paragraph break to keep related expectations together (Gordon), and give the test a brief but evocative name - you do you (from the test_that() help file). There are also expect_error() and expect_warning() functions.\n(37:00) How is the percent coverage from covr::report() calculated?\nThis evaluates which lines of code have been executed for that test. It is not necessarily evaluating if the function is tested well, but rather has this line of code ever been run by your test suite.\n(38:10) You mentioned earlier that when you fix a bug, that is a good time to write a test. Do you have an example of doing this?\nYes! I develop and maintain packages for internal R users, and a common application of this for me is when a user calls the function in a reasonable way and gets a really unfriendly error message. I resolve this through writing a better error message for the function and then include that as a test with expect_error().\n(39:27) How is running tests different than trying out different data types and sets on your function and debugging?\nIt is not different at all! What I have learned from being around other\ntalented programmers is that they don’t have amazing brains that they can hold all these different variables in. They are able to write good software by exporting the normal checks that everyone does when writing functions into a formal set of expectations that you can get a computer to run. Testing is about getting that stuff out of your brain so that a computer can execute it.\n(53:50) Do you have advice on how to choose the data to feed into the expected object?\nYou can choose data from base R to minimize package dependencies, write small in-line data, or use your own specific data that you add to your package. Also consider data that get to the extreme corners of your function (e.g., missing data, weird values). “Throw lizards at your function!”\n(55:57) Do you commonly see test_that() used against a script instead of a function?\nYou can use expect_equal() in scripts, but there are packages like {assertr} which might be more appropriate for R scripts.\n(1:09:38) Regarding dependencies, what do you consider when you are developing a package?\nThis depends on the developer and user. For internal things, feel free to add as many dependencies as you are comfortable with. Base R can cover a lot of the functionality introduced with your dependencies. It depends on how much work you want to do to support the dependencies versus what you are getting out of it. You can also consider pulling in a single function from a package rather than declaring an entire package as a dependency.\n(1:34:55) Does the {testthat} framework work for shiny apps?\nI recommend putting as much as logic as you can into functions that live outside of the app, and then you can use test_that() on those functions. If you are doing tests that involve reactivity in shiny apps, then you need to use {shinytest}.\nPersonal reflection\nWhen curating for @WeAreRLadies on Twitter in February 2021, I asked if there were any volunteers for a workshop on unit testing, and Gordon Shotwell replied affirmatively! 🙋 At that point, we were complete strangers who had never personally interacted.\nDespite having no experience with unit testing, after a bit of conversation and much encouragement from both R-Ladies Philly and Gordon, I agreed to develop a workshop with Gordon’s support. (Why not? Teaching is the best way for me to learn. 😊)\nIn small and infrequent time chunks reading and tinkering, three 30 minute meetings with Gordon, and a few chat exchanges, I learned so much about unit testing between February and November! And I was so glad to be able to give back and share that knowledge (and confidence!💪) with R-Ladies Philly.\nI also really liked the mentor-mentee relationship modeled in this workshop - I think it made the material approachable for beginners and elevated for those more experienced. It also put me at ease during workshop preparation knowing that Gordon could respond to questions that I likely wouldn’t have had experience with. It is a workshop format I highly recommend trying.\nAcknowledgements\nThank you very much to R-Ladies Philly for hosting this workshop. In particular, Alice Walsh provided feedback on workshop materials and this blog post. In addition, many thanks to Gordon Shotwell for volunteering his time and expertise to our learning. 💜\n\n\n\n\n\n\n",
    "preview": "posts/2021-11-23-getting-started-with-unit-testing-in-r/img/unit-testing-thumbnail.png",
    "last_modified": "2022-08-11T11:39:28-04:00",
    "input_file": {},
    "preview_width": 1280,
    "preview_height": 720
  },
  {
    "path": "posts/2021-10-11-estimating-correlations-adjusted-for-group-membership/",
    "title": "Estimating correlations adjusted for group membership",
    "description": "A linear mixed model approach with applied examples in a Palmer penguins Simpson's paradox and TidyTuesday Spotify songs",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2021-10-11",
    "categories": [],
    "contents": "\n\nContents\nTL;DR\nOverview\nPackages\nggplot theme\nIntroduction to correlations\nSimpson’s paradox: a Palmer penguins example\nMultiple features: spotify songs example\nData\nVisual exploration\nCorrelation computations\nResults\n\nDiscussion\nAcknowledgements\n\n\n\n\nFigure 1: A Palmer penguins Simpson’s paradox yields an unadjusted correlation estimate between body mass and bill depth of -0.47 across all three species; an adjusted estimate computed from a linear mixed model is 0.62.\n\n\n\nTL;DR\nCorrelations are a common analytic technique to quantify associations between numeric variables; however, estimates can be misleading when underlying groups or related observations are present. Adjusted correlation estimates can be obtained through a linear mixed model that yield sensible estimates of overall correlations across subgroups.\nOverview\nIn various settings, correlations can be mass estimated to identify signals as to which of many independent variables have the strongest association with a dependent variable. Moreover, the correlations are commonly estimated in aggregate, or total, across may subgroups. Example applications include:\nmarket research, when evaluating associations between product ratings and product sales across many products, in order to identify attributes with the strongest relationship with sales across all products.\nbiological research, when evaluating associations between gene mRNA expressions across many cancer types.\nPackages\nThis material was developed using:\nSoftware / package\nVersion\nR\n4.0.5\nRStudio\n1.4.1103\ntidyverse\n1.3.0\nbroom\n0.7.9\nperformance\n0.7.2\nlme4\n1.1-23\ngt\n0.3.1\ngtExtras\n0.2.16\npalmerpenguins\n0.1.0\n\n\nlibrary(tidyverse)      # general use ----\nlibrary(broom)          # tidying of stats results ----\nlibrary(lme4)           # linear mixed models ----   \nlibrary(performance)    # obtain r-squared ----\nlibrary(gt)             # create table ----\nlibrary(gtExtras)       # table formatting ----\nlibrary(palmerpenguins) # simpsons paradox example ----\n\n\n\nggplot theme\nSome personal ggplot preferences for use later.\n\n\ntheme_shannon <- function(){\n  # theme minimal creates white background and removes ticks on axes ----\n  theme_minimal() +\n  theme(\n    # removes grid lines from plot ----\n    panel.grid = element_blank(),\n    # moves legend to top instead of side ----\n    legend.position = \"top\",\n    # removes title from legend, often superfluous ----\n    legend.title = element_blank(),\n    # creates the light gray box around the plot ----\n    panel.background = element_rect(color = \"#F2F2F2\"),\n    # creates the gray background boxes for faceting labels ----\n    strip.background = element_rect(\n      color = \"#F2F2F2\",\n      fill = \"#F2F2F2\"\n    ),\n    # if using facet grid, this rotates the y text to be more readable ----\n    strip.text.y = element_text(angle = 0),\n    # this produces a fully left justified title ----\n    plot.title.position = \"plot\"\n  )\n}\n\n\n\nIntroduction to correlations\n\n\n\n\n\n\nCorrelations (r) take on values between -1 and 1, and measure the strength of the linear association between two numeric variables. Here are the three extreme forms of correlation:\n\n\n\nA value of 0 indicates no linear association, whereas values of -1 or 1 indicate a perfect linear association. Two conditions that need to be satisfied for good correlation estimates are:\nThe relationship is linear.\nThe observations are independent.\nThe linearity of the relationship can be evaluated by examining a scatter plot. Independence of observations is evaluated by thinking about the origin and nature of the data. A classic way of violating the independence assumption is when observations arise from repeated measures; a less obvious way the independence observation can be violated is from what Isabella Ghement refers to as a random grouping factor.\nLet’s check this out by examining correlations from a Simpson’s paradox example. 🧐\nSimpson’s paradox: a Palmer penguins example\nThe Palmer penguins data set comes from the palmerpenguins package. Andrew Heiss recently tweeted a quick demonstration of a Simpson’s paradox for this data, where the relationship between bill_depth_mm and body_mass_g can appear differently when analyzed across all species of penguins versus within species.\nFor initial data preparation, I am retaining the relevant variables, removing missing observations, and creating a duplicate of the species variable for point colors and faceting later.\n\n\ndat_penguins <- penguins %>% \n  dplyr::select(species, bill_depth_mm, body_mass_g) %>% \n  # duplicate species variable for coloring & grouping ---\n  mutate(species_category = species) %>% \n  drop_na()\n\n\n\n\n\n# species colors used in the palmerpenguins readme ----\ncolors_penguins <- c(\n  \"Adelie\" = \"darkorange\",\n  \"Chinstrap\" = \"purple\",\n  \"Gentoo\" = \"cyan4\"\n  )\n\n\n\nLet’s examine the correlations between bill_depth_mm and body_mass_g for the three penguin species individually, as well as across all species.\n\n\ndat_penguins %>% \n  # add stack for all species to be analyzed together ----\n  bind_rows(dat_penguins %>% mutate(species_category = \"All\")) %>% \n  # now examine by 3 species plus all ----\n  group_by(species_category) %>% \n  nest() %>% \n  # within each group, compute base n and correlation ----\n  mutate(\n    base_n = map_int(data, nrow),\n    corr = map(data, ~ cor.test(x = .x$bill_depth_mm, y = .x$body_mass_g) %>% broom::tidy())\n    ) %>% \n  ungroup() %>% \n  # bring results back to raw data ----\n  unnest(c(data, corr)) %>% \n  mutate(\n    # create ordered facet label for plotting ----\n    species_category = fct_relevel(species_category, \"Gentoo\", \"Chinstrap\", \"Adelie\", \"All\"),\n    corr_label =  glue::glue(\"{species_category}\\nn = {base_n}\\n r = {scales::number(estimate, 0.01)}\"),\n    corr_label = fct_reorder(as.character(corr_label), as.numeric(species_category))\n   ) %>% \n  # create scatter plots ----\n  ggplot(aes(x = bill_depth_mm, y = body_mass_g)) +\n    geom_point(aes(color = species), alpha = 0.5, show.legend = FALSE) +\n    geom_smooth(method = \"lm\", color = \"darkgray\", se = FALSE) +\n    facet_wrap(. ~ corr_label, ncol = 4) +\n    scale_color_manual(values = colors_penguins) + \n    theme_shannon()\n\n\n\n\nHere we see that within species the correlations between bill_depth_mm and body_mass_g are 0.72, 0.60, and 0.58. Yet when we look at the correlation across all species, we have a nonsensical estimate of -0.47 😱, which is a classical presentation of Simpson’s paradox.\nGetting a reasonable estimate of the correlation between bill_depth_mm and body_mass_g across all three species requires a bit more work. To estimate a correlation that adjusts for species, we can implement a linear mixed model, also frequently referred to as a multilevel model, which “allow us to create models for situations where the observations are not independent of one another” as stated in Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R by Paul Roback and Julie Leger.\nIn this approach, we model the dependent variable of body_mass_g with a fixed effect for bill_depth_mm (what we want to draw inferences about) and a random effect for species (which can be thought of as a sample from a larger population of species that we are not interested in drawing specific inferences about). The correlation estimate adjusting for species group membership is the sign of the bill_depth_mm coefficient multiplied by the square root of the variance explained at the different species levels. Here, I use the r2_nakagawa function from the performance package to extract the relevant \\(R^2\\) value.\nAdded 2021-10-13: Mattan Ben-Schachar contributed two alternative solutions on Twitter, one by mean centering and another using the {correlation} package. Another approach inspired by Bryan Shalloway is to compute a weighted correlation, which provides a reasonable estimate as well! (123*0.72 + 68*0.60 + 151*0.58)/342 = 0.634 (though he uses Fisher Z transformation for weighted averages).\n\n\n# estimate mixed model ----\nmixed_model <- lme4::lmer(body_mass_g ~ bill_depth_mm + (1 | species), dat_penguins)\n\n# retrieve sign of coefficient ----\ncoef_sign <- mixed_model %>% \n  broom.mixed::tidy() %>% \n  filter(term == \"bill_depth_mm\") %>% \n  pull(estimate) %>% \n  sign()\n\n# retrieve r2 measure ----\nr2_by_group <- performance::r2_nakagawa(mixed_model, by_group = TRUE)$R2[1]\n\n# compute adjusted correlation ----\nadj_corr <- coef_sign * sqrt(r2_by_group)\n\n# print result ----\nadj_corr\n\n\n[1] 0.6207223\n\nAh, much better! 🤩 A correlation estimate between bill_depth_mm and body_mass_g of 0.62 across all three penguins species is a much more sensible estimate.\nMultiple features: spotify songs example\nData\nThe data comes from the TidyTuesday Spotify songs data set.\n\n\n# import data ----\ndat_raw <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')\n\n\n\nTo reduce the scope and simplify, I am going to examine a reduced set of variables from pop, rock, or rap songs in 2019. Our goal is to understand which of energy, speechiness, acousticness, instrumentalness, liveness, and valence have the strongest association with danceability.\n\n\n# initial data prep ----\ndat <- dat_raw %>% \n  mutate(year = str_sub(track_album_release_date, start = 1, end = 4) %>% as.numeric) %>% \n  filter(playlist_genre %in% c(\"pop\", \"rock\", \"rap\")) %>%\n  filter(year == 2019) %>% \n  dplyr::select(track_id, playlist_genre, danceability, energy, speechiness, acousticness, instrumentalness, liveness, valence) %>% \n  # create an additional variable for genre to be used later ----\n  mutate(music_category = playlist_genre) \n\n\n\n\n\ndat\n\n\n# A tibble: 3,774 x 10\n   track_id             playlist_genre danceability energy speechiness\n   <chr>                <chr>                 <dbl>  <dbl>       <dbl>\n 1 6f807x0ima9a1j3VPbc~ pop                   0.748  0.916      0.0583\n 2 0r7CVbZTWZgbTCYdfa2~ pop                   0.726  0.815      0.0373\n 3 1z1Hg7Vb0AhHDiEmnDE~ pop                   0.675  0.931      0.0742\n 4 75FpbthrwQmzHlBJLuG~ pop                   0.718  0.93       0.102 \n 5 1e8PAfcKUYoKkxPhrHq~ pop                   0.65   0.833      0.0359\n 6 7fvUMiyapMsRRxr07cU~ pop                   0.675  0.919      0.127 \n 7 2OAylPUDDfwRGfe0lYq~ pop                   0.449  0.856      0.0623\n 8 6b1RNvAcJjQH73eZO4B~ pop                   0.542  0.903      0.0434\n 9 7bF6tCO3gFb8INrEDcj~ pop                   0.594  0.935      0.0565\n10 1IXGILkPm0tOCNeq00k~ pop                   0.642  0.818      0.032 \n# ... with 3,764 more rows, and 5 more variables: acousticness <dbl>,\n#   instrumentalness <dbl>, liveness <dbl>, valence <dbl>,\n#   music_category <chr>\n\nHere are the definitions of the variables, straight from the TidyTuesday repo:\nvariable\nclass\ndescription\ntrack_id\ncharacter\nSong unique ID\nplaylist_genre\ncharacter\nPlaylist genre\ndanceability\ndouble\nDanceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\nenergy\ndouble\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\nspeechiness\ndouble\nSpeechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\nacousticness\ndouble\nA confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\ninstrumentalness\ndouble\nPredicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\nliveness\ndouble\nDetects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\nvalence\ndouble\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n\n# create stacked data for analysis ----\ndat_stacked <- dat %>% \n  # add in all level for across all genres ----\n  bind_rows(dat %>% mutate(music_category = \"all\")) %>% \n  dplyr::select(track_id, music_category, playlist_genre, danceability, energy, \n                speechiness, acousticness, instrumentalness, liveness, valence) %>% \n  # convert to long format for plotting and analysis ----\n  pivot_longer(\n    -c(track_id, music_category, playlist_genre, danceability),\n    names_to = \"feature\",\n    values_to = \"value\"\n  ) %>% \n  mutate(\n    music_category = fct_relevel(music_category, \"pop\", \"rock\", \"rap\", \"all\"),\n    # reordered by a sneak peak at results below ----\n    feature = fct_relevel(feature, \"valence\", \"speechiness\", \"energy\", \"acousticness\", \"liveness\", \"instrumentalness\")\n  )\n\n\n\nVisual exploration\nLet’s start with a visual exploration of the data.\n\n\n# colors for playlist genre ----\ncolors_genre <- c(\n  \"pop\" = \"#E76F51\",\n  \"rock\" = \"#264653\",\n  \"rap\" = \"#E9C46A\"\n)\n\n\n\n\n\ndat_stacked %>% \n  ggplot(aes(x = value, y = danceability)) +\n  geom_point(aes(color = playlist_genre), alpha = 0.2, show.legend = FALSE) +\n  facet_grid(feature ~ music_category, scales = \"free\") +\n  scale_color_manual(values = colors_genre) +\n  theme_shannon()\n\n\n\n\nVisual inspection shows the possibility of some non-linear relationships, for example, with energy, and also some potential outliers that could influence the correlation estimate. However, for this analysis we will proceed with data as-is. In addition, note that pop and rock tend to have similar distributions, which differ from rap.\nCorrelation computations\nFirst, we compute correlations for the three genres individually and the unadjusted correlations across all genres together.\n\n\nresults_1 <- dat_stacked %>%\n  group_by(music_category, feature) %>% \n  nest() %>% \n  mutate(\n    corr = map(data, ~ cor.test(x = .x$value, y = .x$danceability) %>% broom::tidy())\n    ) %>% \n  ungroup() %>% \n  unnest(c(corr)) %>% \n  dplyr::select(music_category, feature, corr = estimate)\n\n\n\nNext, we compute the adjusted correlations across all genres together through the linear mixed model. To facilitate this computation, here is a quick function.\n\n\ncompute_adj_corr <- function(data, var_dep, var_ind, var_group){\n  \n  mixed_formula <- glue::glue(\"{var_dep} ~ {var_ind} + (1 | {var_group})\")\n  \n  mixed_model <- lme4::lmer(mixed_formula, data)\n\n  coef_sign <- mixed_model %>% \n    broom.mixed::tidy() %>% \n    filter(term == var_ind) %>% \n    pull(estimate) %>% \n    sign()\n\n r2_by_group <- performance::r2_nakagawa(mixed_model, by_group = TRUE)$R2[1]\n\n adj_corr <- coef_sign * sqrt(r2_by_group)\n\n return(adj_corr)\n}\n\n\n\nNow we compute the adjusted correlations.\n\n\nresults_2 <- dat_stacked %>%\n  filter(music_category == \"all\") %>%\n  group_by(feature) %>%\n  nest() %>%\n  mutate(\n    corr = map_dbl(\n      data,\n      compute_adj_corr,\n      var_dep = \"danceability\",\n      var_ind = \"value\",\n      var_group = \"playlist_genre\"\n    )\n  ) %>%\n  mutate(music_category = \"adjusted\") %>%\n  dplyr::select(music_category, feature, corr)\n\n\n\nFinally, we combine both sets of results into a single data set for presentation.\n\n\nresults_all <- results_1 %>% \n  bind_rows(results_2) %>% \n  pivot_wider(\n    names_from = \"music_category\",\n    values_from = \"corr\"\n    ) %>% \n  mutate_if(is.numeric, round, 2) %>% \n  arrange(desc(adjusted)) %>% \n  rename(unadjusted = all)\n\n\n\nResults\n\n\nresults_all %>% \n  gt::gt() %>% \n  gt::tab_header(\n    title = \"Correlations with danceability\"\n  ) %>% \n  gt::tab_spanner(\n    label = \"Genres\",\n    columns = c(pop, rap, rock)\n  ) %>%\n  gt::tab_spanner(\n    label = \"All\",\n    columns = c(unadjusted, adjusted)\n  ) %>%\n  gt::cols_align(\n    align = \"left\",\n    columns = feature\n  ) %>% \n  gtExtras::gt_hulk_col_numeric(pop:adjusted)\n\n\n\nCorrelations with danceability\n    feature\n      \n        Genres\n      \n      \n        All\n      \n    pop\n      rap\n      rock\n      unadjusted\n      adjusted\n    valence\n0.40\n0.33\n0.35\n0.33\n0.36speechiness\n0.06\n0.13\n-0.11\n0.25\n0.09energy\n0.03\n-0.03\n-0.30\n-0.22\n-0.05acousticness\n-0.05\n-0.15\n0.14\n0.07\n-0.08liveness\n-0.08\n-0.12\n-0.12\n-0.14\n-0.11instrumentalness\n-0.02\n-0.27\n-0.16\n-0.11\n-0.19\n\nHere, we see that valence has the strongest association with danceability, both within each of the genres and across the three genres. Regarding valence, both the unadjusted and adjusted correlation estimates are similar. However, for other attributes, such as speechiness, the association varies by genre (weakly positive for rap, weakly negative for rock). Furthermore, the unadjusted (0.25) and adjusted (0.09) correlations differ by quite a bit, with the unadjusted correlation once again producing a nonsensical result that exceeds the correlations for any individual genre (-0.11 rock, 0.06 pop, and 0.13 rap).\nDiscussion\nCorrelations are used by researchers of all backgrounds and expertise due to their relatively straightforward computation and interpretation. When the distributions of dependent and independent variables are similar across subgroups, the unadjusted simple correlation estimates are likely to be similar to the more complex adjusted correlation estimates; however, when distributions vary by subgroups, simple correlation estimates across subgroups could be incorrect and misleading. A primary defense to guard against this is to plot your data - this can help you to identify grouping effects or when relationships are non-linear. In addition, it should be carefully considered if the relationships across groups are truly of more interest than relationships within groups, as across group results can obscure potentially interesting within group findings.\nAcknowledgements\nThank you to Alex Fisher for brainstorming with me the linear mixed model approach to adjusted correlation estimates. In addition, thank you to Alice Walsh for your enthusiasm and feedback on this post.\n\n\n\n\n\n\n",
    "preview": "posts/2021-10-11-estimating-correlations-adjusted-for-group-membership/img/thumbnail.PNG",
    "last_modified": "2022-08-11T11:39:28-04:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 540
  },
  {
    "path": "posts/2021-09-23-curating-for-wearerladies-on-twitter/",
    "title": "Curating for @WeAreRLadies on Twitter",
    "description": "From creating content to cultivating connections.",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2021-09-23",
    "categories": [],
    "contents": "\n\nContents\nTL; DR\nAbout @WeAreRLadies\nOvercoming imposter syndrome\nTimeline\nSelecting a date\nSigning up\nNotifying my manager\nStyling content\nDrafting content\nContent inspiration\nPolls\nFirst and last tweets\nTweetDeck\nWhat I would have done differently\nFleeting fame\nSupporting your curators\nReflection\nAcknowledgements\n\n\n\n\n\n\n\n\n\n\nFigure 1: Photo by Arno Smit on Unsplash (https://unsplash.com/photos/iI72r3gSwWY)\n\n\n\nTL; DR\nIn February 2021 I tweeted to a daunting >20k followers by curating for @WeAreLadies on Twitter. This was great opportunity to share knowledge, interact with others, and learn something in return, ultimately cultivating new connections and collaborations. From preparation to fruition, I hope this post helps you confidently enroll as a curator!\nAdded 2021-10-05: For a broad overview, check out the poster presentation Be great and curate! for the 2021 Women in Statistics and Data Science conference.\nAbout @WeAreRLadies\nThe @WeAreRLadies rotating Twitter curator exists to “encourage and maintain Twitter engagement within the R-Ladies community”, and to “spotlight female and minority genders” working with R. R-Ladies has a comprehensive guide describing the program, procedures and protocols for the week, and tips for successful curation.\nOvercoming imposter syndrome\nYou may be hesitant to sign up as a curator due to imposter syndrome - I certainly was. I was on Twitter for three years before I gathered the courage. However, you do not need to know everything about R nor Twitter in order to be a successful @WeAreRLadies curator - that is impossible! In fact,\n\nNo matter where you are in your R learning journey 🌄 YOU are an important part of the #rstats community!🤗 pic.twitter.com/ThBhjE3ZA2— We are R-Ladies (@WeAreRLadies) February 16, 2021\n\n\nEvery R user, new or experienced, has a valuable perspective to share. I was particularly impressed when Danielle Brantley excellently curated in September 2020, after being an R user for one year! To help alleviate general R imposter syndrome check out Catlin Hudon’s blog post on imposter syndrome in data science; to increase comfort with Twitter, try Twitter for R Programmers by Oscar Baruffa and Veerle van Son.\nMy personal strategy for combating imposter syndrome is to prepare. For my curating week, my preparation involved reflecting on past curators and creating some content in advance. I hope this post helps you to prepare and motivates you to sign up. 😉\nTimeline\nHere is my personal timeline to leading up to curation.\n\n\n\n\nTime before curation\nAction taken\n3 years\nBecame active on twitter\n3 months\nSigned up to curate\n3 weeks\nNotified manager; discussed work-related content\n2 weeks\nResearched R-Ladies fonts and colors\n1 week\nStarted drafting tweets\n1 day\nFiddled with formats for code gifs\n\nSelecting a date\nYou can view the schedule of upcoming curators to identify available dates; records of previous curators are also maintained here.\nBeing a curator will be time intensive, so be kind to yourself. Choose dates when you will have time to invest and a flexible work schedule. I chose Feb 15-20 because I hoped by then I would be recovered from an intense Q4 work cycle; additionally, Feb 15 (President’s Day) was a company holiday. You may want to select a date far enough in the future that allows you time to create content.\nAnother consideration is to schedule your curation to coincide with dates that align with your interests. For example, are you passionate about Black History Month in February, LGBT Pride Month in June, or Universal Human Rights Month in December? If so, take advantage of the @WeAreRLadies large platform as an opportunity to inform and educate others on issues that are important to you as they relate to the R community. Diversity best practices has a comprehensive list of calendar holidays and observances.\nSigning up\nYou sign up by submitting a form - give yourself at least 30 minutes to sign up as part of the form includes filling out details that complete your curating profile.\nThere was a gap between when I filled out the form and when I was confirmed as a curator, which I suspect was due to timings and holidays. Be kind, be patient - all organizing R-Ladies are volunteers.\nNotifying my manager\nAbout three weeks before my curation, I started planning my curating efforts a bit more seriously. I notified my manager that I was curating, and I discussed potential work-related content with her. One idea was approved and another was reasonably denied. This honest conversation facilitated new awareness about my passions - my manager was not aware of R-Ladies, and she was enthusiastic and supportive.\nStyling content\nAdditionally, I considered how to visually style content beyond text in a tweet. I asked on R-Ladies slack about R-Ladies styles, and I was directed to the xaringan R-Ladies css and the R-Ladies branding guides. You are not required to use R-Ladies style and branding, but it was convenient for me.\nI developed two visual layouts using the Google slide template from R-Ladies branding (see tweets for blogdown vs distill and asking for help online).\n\n\n\nFigure 2: Comparison of blogdown vs distill styled using R-Ladies Google slide template.\n\n\n\nI also created five R-Ladies styled code gifs with xaringan and flipbookr - methods and code are in this blog post. Here is an example code gif:\n\n\n\nFigure 3: Example R-Ladies styled code gif.\n\n\n\nDrafting content\nLeading up to my curation week, I regularly jotted down brief notes of content ideas. The week before curation, I started fleshing out those ideas into actual tweets and wrote them down in a document. Not all of my ideas ended up in a draft, and rarely did the draft get tweeted out exactly as I had written.\nOne challenge with drafting tweets in a document was being mindful of character limits and anticipating where the breaks would be for threads. I started copying content into a send tweet window to preview and then pasting it back into my draft document. There is software that facilitates drafting tweets - for example, Daphna Harel recommended getchirrapp.com to me the week of my curation. I also kept emojipedia open all week to easily copy and paste emojis into drafts.\nNot all content was premeditated - I also tweeted in the moment. For example, the W.E.B. Du Bois’ #TidyTuesday visualizations were incredible that week, or when I realized a new colleague wasn’t yet taking advantage of RStudio projects.\nContent inspiration\nAs I approached my curating week, I recalled previous @WeAreRLadies that were memorable for me, my previous experience as an educator, and some reflection questions to inspire content.\nInspiration source\nExample realization\n1. Rotating curator Mine Çetinkaya-Rundel (@minebocek) tweets awesome gifs\nGifs for R code demos\n2. Rotating curator Megan Stodel (@MeganStodel) tweets a project inspired by curating\nAn R project to introduce myself as a curator\n3. Rotating curator Julia Piaskowski (@SeedsAndBreeds) tweets a great technical thread on ANOVA\nA thread on blogging resources\n4. Prior experience as an educator\nStarting discussion with a question\n5. What am I passionate about lately?\nBlogging\n6. What did I have to overcome to be where I am today?\nLearning how to ask for help online\n7. What have colleagues or students asked me about?\nWhat needs updating and when\n8. What are some R functions or packages that have helped me recently?\nsortable package\n9. What are R-Ladies voices I can amplify while I have this large platform?\nQuote tweeting questions\nPolls\n\n\n\n\n\n\n\n\n\nReminiscing about my days teaching in large lectures halls with students actively participating in polling questions through clickers, I planned three polls for the week. Polls on twitter are open for 24 hours and allow up to four response options. The approach was to launch the poll, collect responses, and then discuss. Here are the three polls that I launched during my curation, with follow up discussion:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Three polls and their follow-up discussion; hover over tiles to view tweets and click on tiles to go to links on Twitter.\n\n\n\nFirst and last tweets\nThe introduction and farewell tweets as a curator are important as this is when you actually tell people your name or personal twitter handle. To generate engagement, I aimed to create content-rich first and last tweets to give users more motivation to like or re-tweet, and I also connected the content with links to my blog so that users could easily learn more about me.\nTweetDeck\nWhen you serve as a curator, you will be tweeting from TweetDeck, and it is hard to separate curator experience from the technology. Tweeting from TweetDeck can be overwhelming compared to the standard Twitter interface.\nMoreover, there were limitations to the platform that added challenges to curating, which included:\nThere was no + enabled to easily create threads (I had to send a tweet and then comment on the tweet, and it was initially hard to ensure the thread appeared in correct order). Yes, I deleted many out of order tweets.\nConsequently, I could not draft and save threads in TweetDeck to send later.\nI could not send polls from the curator account on TweetDeck; polls were sent from my personal account and then re-tweeted from the curator account. Adding this context can help better frame polls.\nDepending on the content already in the send tweet interface, sometimes other options in TweetDeck would disappear, like the emoji, gifs, and upload image buttons. I kept emojipedia open to easily copy and paste emojis into my tweets, and it took trial and error to get everything I wanted in a single tweet.\nWhen uploading local content, you can add descriptions to both gifs and images in the regular Twitter interface to create inclusive content for community members that use assistive reading technology; however, in TweetDeck, descriptions were enabled for images but not gifs.\nWith TweetDeck, you can tweet from both your personal account and your curator account. You can set the options to default to the curator account, but there were still some instances where I still managed to inadvertently tweet from my personal account when I meant to tweet from the curator account. (I did delete the tweet and re-tweet from the correct account.)\nI spent a lot of time my first couple of days as a curator getting used to TweetDeck, reaching out to other curators for tips, and researching alternative solutions and plug-ins that ultimately did not help. Twitter is targeting TweetDeck enhancements later in 2021, so I don’t think it is worth documenting all of my methods and work-arounds. However, if you are serving as a curator and struggling with TweetDeck, please reach out - I am happy to share what ended up working for me. You can also prepare yourself by practicing tweeting from TweetDeck with your personal account prior to curating.\nWhat I would have done differently\nIt was a whirlwind week! Here a few things I would have done differently.\nPractice with TweetDeck in advance. Literally, force yourself to tweet from Tweetdeck at least a week before your curation. TweetDeck is very different than the standard Twitter interface, and it took me a few days to get used to it.\nFigure out how I wanted to style shared code in advance - my first couple of days would have gone smoother with this.\nPreface polls tweeted from my personal account with the context that they are for curator rotation.\nPrepare a tweet in honor of any holidays or significant events coinciding with your curation week. One regret that I do have from my curating week is failing to explicitly acknowledge Black History Month as I was tweeting in February. I wish had prepared at least one tweet or better amplified the voices of black members of the R community while I had the large platform.\nFleeting fame\nWhen curating, your tweets in the moment are highly visible. But what persists afterward is fairly anonymous as your tweets are not linked to your personal profile unless you tag yourself. In a weird way, it actually becomes a safe place to put yourself out there with questions you might not have been comfortable asking from your own personal account. Take advantage of this fleeting fame not just to share your knowledge but also to ask your questions.\nSupporting your curators\nJust because a Twitter account has >20K followers, the likes, re-tweets, and comments don’t come automatically. You still have to earn engagement with your content. Many of the tweets I sent had little engagement, and that is okay. Supporting your curators by engaging with their tweets or sending notes of encouragement is much appreciated. I thank everyone who engaged with me during my curation, with a special shout out to Alison Hill who re-energized me mid-week with comments on the R-Ladies bloggers thread. I cannot emphasize this enough: every like, re-tweet, comment, and direct message helps!\nIn addition, if you have curated in the past, consider sending new curators a personal welcome message and an invitation to ask you any questions. Following my curation week, I offered camaraderie and tips to Ale Segura, and in return, she did the same for Shreya Louis following her.\nReflection\nBetween prepared and ad-hoc content and discussions with followers, I tweeted a lot! (At least for me.) Here is a summary thread of my tweets for the week. My tweets were not perfect, and that is okay. I messed up threads, had typos, and shared deprecated code, among other things. Check out my blooper reel1 for tweets that I bungled.\nServing as a curator was intimidating and time consuming, but I am very glad I did it. Many good things have happened as direct result of that week, including:\ndiscussing comparisons between {blogdown} and {distill} with Alison Hill.\ncollaborating with Silvia Canelón to style code gifs.\nengaging with new people on Twitter that I want to continue to engage with.\nlearning about valuable new-to-me packages, functions, and work flows.\nbeing invited to speak for R-Ladies Miami.\nseeing my “Asking for help online” content re-used in Sharla Gelfand’s “make a reprex… please” presentation.\nco-developing a unit testing workshop with Gordon Shotwell for R-Ladies Philly.\nDuring my curating week I tried to embody the tweets that I value: honest questions, thoughtful discussion, generous sharing, supportive community, and humorous exchanges. To borrow from Vicki Boykis in the rstudio::global(2021) keynote, I created my own public garden that cultivated new connections and collaborations. And now, I am more confident in continuing these practices from my personal Twitter account.\nAcknowledgements\nThank you to Maëlle Salmon and Alison Hill for encouraging me to write this - it might not have happened without you! Thank you also to Maëlle Salmon, Ale Segura, and Isabella Velásquez for your suggestions; I truly appreciate your sharp eyes and thoughtful feedback. 💜\n\nA blooper is an embarrassing mistake, often sports-related, and humorous in retrospect; a blooper reel is a compilation of multiple bloopers.↩︎\n",
    "preview": "posts/2021-09-23-curating-for-wearerladies-on-twitter/img/arno-smit-iI72r3gSwWY-unsplash.jpg",
    "last_modified": "2022-08-11T11:39:28-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-27-a-tidyverse-pivot-approach-to-data-preparation-in-r/",
    "title": "A tidyverse pivot approach to data preparation in R",
    "description": "Going from wide to long with #TidyTuesday beach volleyball",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2021-08-27",
    "categories": [],
    "contents": "\n\nContents\nTL; DR\nBackground\nGetting started\nImport the data\nOriginal data preparation\nAlternative data preparation\nStep 1: initial manipulation of wide data\nStep 2: reshape data to long\nStep 3: sum values of player 1 and player 2 for all metrics\nStep 4: reshape data to wide for modeling\n\nSummary\n\n\n\n\nFigure 1: Artwork by @allison_horst, modified to substitute pivot_wider() and pivot_longer() for spread and gather.\n\n\n\nTL; DR\nI demonstrate a pivot_longer() plus pivot_wider() approach to data preparation as an alternative to explicitly coding computations. This approach might be beneficial for you if you have:\n✅ wide data,\n✅ with a consistent naming structure,\n✅ and many variables to aggregate.\nBackground\nI love reading and watching Julia Silge’s #TidyTuesday tidymodels tutorials! Recently I was following her post about xgboost classification models with the beach volleyball data. The first 15 minutes of the 50 minute video are about getting familiar with the data and preparing it for modeling, and I realized I would have taken a different approach to the data preparation involving pivot_longer() and pivot_wider(). Also, Spencer Zeigler recently tweeted asking about this approach.\n\n\ndo you ever have to pivot_longer() first to get pivot_wider() to do what you want? or is my data just formatted badly? or am I bad at pivoting data? or both? #rstats #tidyr\n\n— Spencer Zeigler (@spenceometry) August 24, 2021\n\nYes Spencer, I do this all the time!\nGetting started\nThis material was developed using:\nSoftware / package\nVersion\nR\n4.0.5\nRStudio\n1.4.1103\ntidyverse\n1.3.1\n\n\nlibrary(tidyverse)  # general use ----\n\n\n\nImport the data\n#TidyTuesday provides more information about the beach volleyball data.\n\n\nvb_matches  <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-19/vb_matches.csv', guess_max = 76000)\n\n\n\n\n\nvb_matches \n\n\n# A tibble: 76,756 x 65\n   circuit tournament     country     year date       gender match_num\n   <chr>   <chr>          <chr>      <dbl> <date>     <chr>      <dbl>\n 1 AVP     Huntington Be~ United St~  2002 2002-05-24 M              1\n 2 AVP     Huntington Be~ United St~  2002 2002-05-24 M              2\n 3 AVP     Huntington Be~ United St~  2002 2002-05-24 M              3\n 4 AVP     Huntington Be~ United St~  2002 2002-05-24 M              4\n 5 AVP     Huntington Be~ United St~  2002 2002-05-24 M              5\n 6 AVP     Huntington Be~ United St~  2002 2002-05-24 M              6\n 7 AVP     Huntington Be~ United St~  2002 2002-05-24 M              7\n 8 AVP     Huntington Be~ United St~  2002 2002-05-24 M              8\n 9 AVP     Huntington Be~ United St~  2002 2002-05-24 M              9\n10 AVP     Huntington Be~ United St~  2002 2002-05-24 M             10\n# ... with 76,746 more rows, and 58 more variables: w_player1 <chr>,\n#   w_p1_birthdate <date>, w_p1_age <dbl>, w_p1_hgt <dbl>,\n#   w_p1_country <chr>, w_player2 <chr>, w_p2_birthdate <date>,\n#   w_p2_age <dbl>, w_p2_hgt <dbl>, w_p2_country <chr>, w_rank <chr>,\n#   l_player1 <chr>, l_p1_birthdate <date>, l_p1_age <dbl>,\n#   l_p1_hgt <dbl>, l_p1_country <chr>, l_player2 <chr>,\n#   l_p2_birthdate <date>, l_p2_age <dbl>, l_p2_hgt <dbl>,\n#   l_p2_country <chr>, l_rank <chr>, score <chr>, duration <time>,\n#   bracket <chr>, round <chr>, w_p1_tot_attacks <dbl>,\n#   w_p1_tot_kills <dbl>, w_p1_tot_errors <dbl>,\n#   w_p1_tot_hitpct <dbl>, w_p1_tot_aces <dbl>,\n#   w_p1_tot_serve_errors <dbl>, w_p1_tot_blocks <dbl>,\n#   w_p1_tot_digs <dbl>, w_p2_tot_attacks <dbl>,\n#   w_p2_tot_kills <dbl>, w_p2_tot_errors <dbl>,\n#   w_p2_tot_hitpct <dbl>, w_p2_tot_aces <dbl>,\n#   w_p2_tot_serve_errors <dbl>, w_p2_tot_blocks <dbl>,\n#   w_p2_tot_digs <dbl>, l_p1_tot_attacks <dbl>,\n#   l_p1_tot_kills <dbl>, l_p1_tot_errors <dbl>,\n#   l_p1_tot_hitpct <dbl>, l_p1_tot_aces <dbl>,\n#   l_p1_tot_serve_errors <dbl>, l_p1_tot_blocks <dbl>,\n#   l_p1_tot_digs <dbl>, l_p2_tot_attacks <dbl>,\n#   l_p2_tot_kills <dbl>, l_p2_tot_errors <dbl>,\n#   l_p2_tot_hitpct <dbl>, l_p2_tot_aces <dbl>,\n#   l_p2_tot_serve_errors <dbl>, l_p2_tot_blocks <dbl>,\n#   l_p2_tot_digs <dbl>\n\nOriginal data preparation\nThis is copied straight from Julia’s blog post. The original data preparation involves writing formulas with transmute() followed by stacking data for winners and losers with bind_rows().\n\n\nvb_parsed <- vb_matches %>%\n  transmute(\n    circuit,\n    gender,\n    year,\n    w_attacks = w_p1_tot_attacks + w_p2_tot_attacks,\n    w_kills = w_p1_tot_kills + w_p2_tot_kills,\n    w_errors = w_p1_tot_errors + w_p2_tot_errors,\n    w_aces = w_p1_tot_aces + w_p2_tot_aces,\n    w_serve_errors = w_p1_tot_serve_errors + w_p2_tot_serve_errors,\n    w_blocks = w_p1_tot_blocks + w_p2_tot_blocks,\n    w_digs = w_p1_tot_digs + w_p2_tot_digs,\n    l_attacks = l_p1_tot_attacks + l_p2_tot_attacks,\n    l_kills = l_p1_tot_kills + l_p2_tot_kills,\n    l_errors = l_p1_tot_errors + l_p2_tot_errors,\n    l_aces = l_p1_tot_aces + l_p2_tot_aces,\n    l_serve_errors = l_p1_tot_serve_errors + l_p2_tot_serve_errors,\n    l_blocks = l_p1_tot_blocks + l_p2_tot_blocks,\n    l_digs = l_p1_tot_digs + l_p2_tot_digs\n  ) %>%\n  na.omit()\n\nwinners <- vb_parsed %>%\n  select(circuit, gender, year,\n         w_attacks:w_digs) %>%\n  rename_with(~ str_remove_all(., \"w_\"), w_attacks:w_digs) %>%\n  mutate(win = \"win\")\n\nlosers <- vb_parsed %>%\n  select(circuit, gender, year,\n         l_attacks:l_digs) %>%\n  rename_with(~ str_remove_all(., \"l_\"), l_attacks:l_digs) %>%\n  mutate(win = \"lose\")\n\nvb_df <- bind_rows(winners, losers) %>%\n  mutate_if(is.character, factor)\n\n\n\n\n\nvb_df\n\n\n# A tibble: 28,664 x 11\n   circuit gender  year attacks kills errors  aces serve_errors blocks\n   <fct>   <fct>  <dbl>   <dbl> <dbl>  <dbl> <dbl>        <dbl>  <dbl>\n 1 AVP     M       2004      45    24      7     0            2      5\n 2 AVP     M       2004      71    31     16     3            8      7\n 3 AVP     M       2004      43    26      5     2            4      7\n 4 AVP     M       2004      42    32      5     2            3      7\n 5 AVP     M       2004      44    31      1     0            5      6\n 6 AVP     M       2004      55    31      6     0            4      8\n 7 AVP     M       2004      39    26      8     5            3      4\n 8 AVP     M       2004      41    21      8     1            1      6\n 9 AVP     M       2004      60    33     12     0            1      9\n10 AVP     M       2004      32    11      5     1            5      8\n# ... with 28,654 more rows, and 2 more variables: digs <dbl>,\n#   win <fct>\n\nAlternative data preparation\nThis approach leverages pivot_wider() and pivot_longer() to avoid writing out explicit computations. This can work well if you need to aggregate many variables with a sum or a mean. This approach does require a unique identifier for each record.\n\n\nvb_df <- vb_matches  %>%\n  mutate(id = row_number()) %>%\n  dplyr::select(\n    id, circuit, gender, year,\n    matches(\"attacks|kills|errors|aces|blocks|digs\")\n    ) %>%\n  drop_na() %>%\n  pivot_longer(\n    cols = -c(id, circuit, gender, year),\n    names_to = c(\"status\", \"player\", \"method\", \"metric\"),\n    names_pattern = \"([wl])_(p[12])_(tot)_(.*)\",\n    values_to = \"value\"\n  ) %>% \n  group_by(id, circuit, gender, year, status, metric) %>%\n  summarize(total = sum(value)) %>%\n  ungroup() %>%\n  pivot_wider(\n    names_from = \"metric\",\n    values_from = total\n    )\n\n\n\nAnd here is what the final data looks like!\n\n\nvb_df\n\n\n# A tibble: 28,664 x 12\n      id circuit gender  year status  aces attacks blocks  digs errors\n   <int> <chr>   <chr>  <dbl> <chr>  <dbl>   <dbl>  <dbl> <dbl>  <dbl>\n 1  1846 AVP     M       2004 l          1      57      2    17     17\n 2  1846 AVP     M       2004 w          0      45      5    11      7\n 3  1847 AVP     M       2004 l          0      61      5    23     13\n 4  1847 AVP     M       2004 w          3      71      7    21     16\n 5  1848 AVP     M       2004 l          1      36      2    11      7\n 6  1848 AVP     M       2004 w          2      43      7    10      5\n 7  1849 AVP     M       2004 l          0      41      4    12      8\n 8  1849 AVP     M       2004 w          2      42      7     7      5\n 9  1850 AVP     M       2004 l          1      45      1     7      6\n10  1850 AVP     M       2004 w          0      44      6    15      1\n# ... with 28,654 more rows, and 2 more variables: kills <dbl>,\n#   serve_errors <dbl>\n\nIn case that was hard to follow in one long code chunk, below I show what the data looks like at each of four steps annotated with comments.\nStep 1: initial manipulation of wide data\n\n\n# step 1: initial manipulation of wide data ----\nstep1 <- vb_matches  %>%\n  # create unique identifier ----\n  mutate(id = row_number()) %>%\n  # retain relevant variables ----\n  dplyr::select(\n    id, circuit, gender, year,\n    matches(\"attacks|kills|errors|aces|blocks|digs\")\n    ) %>%\n  # remove records where any observations are missing ----\n  drop_na() \n\n\n\n\n\nstep1\n\n\n# A tibble: 14,332 x 32\n      id circuit gender  year w_p1_tot_attacks w_p1_tot_kills\n   <int> <chr>   <chr>  <dbl>            <dbl>          <dbl>\n 1  1846 AVP     M       2004               11              6\n 2  1847 AVP     M       2004               32             13\n 3  1848 AVP     M       2004               19             11\n 4  1849 AVP     M       2004               19             13\n 5  1850 AVP     M       2004               24             21\n 6  1851 AVP     M       2004               20              7\n 7  1852 AVP     M       2004               17             11\n 8  1853 AVP     M       2004                7              2\n 9  1854 AVP     M       2004               16             10\n10  1855 AVP     M       2004               13              5\n# ... with 14,322 more rows, and 26 more variables:\n#   w_p1_tot_errors <dbl>, w_p1_tot_aces <dbl>,\n#   w_p1_tot_serve_errors <dbl>, w_p1_tot_blocks <dbl>,\n#   w_p1_tot_digs <dbl>, w_p2_tot_attacks <dbl>,\n#   w_p2_tot_kills <dbl>, w_p2_tot_errors <dbl>, w_p2_tot_aces <dbl>,\n#   w_p2_tot_serve_errors <dbl>, w_p2_tot_blocks <dbl>,\n#   w_p2_tot_digs <dbl>, l_p1_tot_attacks <dbl>,\n#   l_p1_tot_kills <dbl>, l_p1_tot_errors <dbl>, l_p1_tot_aces <dbl>,\n#   l_p1_tot_serve_errors <dbl>, l_p1_tot_blocks <dbl>,\n#   l_p1_tot_digs <dbl>, l_p2_tot_attacks <dbl>,\n#   l_p2_tot_kills <dbl>, l_p2_tot_errors <dbl>, l_p2_tot_aces <dbl>,\n#   l_p2_tot_serve_errors <dbl>, l_p2_tot_blocks <dbl>,\n#   l_p2_tot_digs <dbl>\n\nStep 2: reshape data to long\n\n\n# step 2: reshape data to long ----\nstep2 <- step1 %>% \n  pivot_longer(\n    # specify variables to hold fixed and not pivot ----\n    cols = -c(id, circuit, gender, year),\n    # create four new variables extracted from the variable name ----\n    names_to = c(\"status\", \"player\", \"method\", \"metric\"),\n    # regex pattern to extract values from variable name ---\n    names_pattern = \"([wl])_(p[12])_(tot)_(.*)\",\n    # the value of the metric ---\n    values_to = \"value\"\n  ) \n\n\n\n\n\nstep2\n\n\n# A tibble: 401,296 x 9\n      id circuit gender  year status player method metric       value\n   <int> <chr>   <chr>  <dbl> <chr>  <chr>  <chr>  <chr>        <dbl>\n 1  1846 AVP     M       2004 w      p1     tot    attacks         11\n 2  1846 AVP     M       2004 w      p1     tot    kills            6\n 3  1846 AVP     M       2004 w      p1     tot    errors           1\n 4  1846 AVP     M       2004 w      p1     tot    aces             0\n 5  1846 AVP     M       2004 w      p1     tot    serve_errors     1\n 6  1846 AVP     M       2004 w      p1     tot    blocks           0\n 7  1846 AVP     M       2004 w      p1     tot    digs             7\n 8  1846 AVP     M       2004 w      p2     tot    attacks         34\n 9  1846 AVP     M       2004 w      p2     tot    kills           18\n10  1846 AVP     M       2004 w      p2     tot    errors           6\n# ... with 401,286 more rows\n\nStep 3: sum values of player 1 and player 2 for all metrics\n\n\n# step 3: sum values of player 1 and player 2 for all metrics ----\nstep3 <- step2 %>%\n  group_by(id, circuit, gender, year, status, metric) %>%\n  summarize(total = sum(value)) %>%\n  ungroup()\n\n\n\n\n\nstep3\n\n\n# A tibble: 200,648 x 7\n      id circuit gender  year status metric       total\n   <int> <chr>   <chr>  <dbl> <chr>  <chr>        <dbl>\n 1  1846 AVP     M       2004 l      aces             1\n 2  1846 AVP     M       2004 l      attacks         57\n 3  1846 AVP     M       2004 l      blocks           2\n 4  1846 AVP     M       2004 l      digs            17\n 5  1846 AVP     M       2004 l      errors          17\n 6  1846 AVP     M       2004 l      kills           31\n 7  1846 AVP     M       2004 l      serve_errors     5\n 8  1846 AVP     M       2004 w      aces             0\n 9  1846 AVP     M       2004 w      attacks         45\n10  1846 AVP     M       2004 w      blocks           5\n# ... with 200,638 more rows\n\nStep 4: reshape data to wide for modeling\n\n\n# step 4: reshape data to wide for modeling ----\nstep4 <- step3 %>% \n  pivot_wider(\n    names_from = \"metric\",\n    values_from = total\n    )\n\n\n\n\n# A tibble: 28,664 x 12\n      id circuit gender  year status  aces attacks blocks  digs errors\n   <int> <chr>   <chr>  <dbl> <chr>  <dbl>   <dbl>  <dbl> <dbl>  <dbl>\n 1  1846 AVP     M       2004 l          1      57      2    17     17\n 2  1846 AVP     M       2004 w          0      45      5    11      7\n 3  1847 AVP     M       2004 l          0      61      5    23     13\n 4  1847 AVP     M       2004 w          3      71      7    21     16\n 5  1848 AVP     M       2004 l          1      36      2    11      7\n 6  1848 AVP     M       2004 w          2      43      7    10      5\n 7  1849 AVP     M       2004 l          0      41      4    12      8\n 8  1849 AVP     M       2004 w          2      42      7     7      5\n 9  1850 AVP     M       2004 l          1      45      1     7      6\n10  1850 AVP     M       2004 w          0      44      6    15      1\n# ... with 28,654 more rows, and 2 more variables: kills <dbl>,\n#   serve_errors <dbl>\n\nSummary\nIf you are fortunate enough to have wide data with a consistent naming structure, using a pivot_longer() / pivot_wider() data preparation approach can save you from writing out tedious formulas. Let me know what you think!\n\n\n\n",
    "preview": "posts/2021-08-27-a-tidyverse-pivot-approach-to-data-preparation-in-r/gatherspread_modified.jpg",
    "last_modified": "2022-08-11T11:39:28-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-14-polished-summary-tables-in-r-with-gtsummary/",
    "title": "Polished summary tables in R with gtsummary",
    "description": "Also plays well with labelled data",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2021-07-14",
    "categories": [],
    "contents": "\n\nContents\nTL; DR\nBackground\nThe data\nGetting started\nData preparation\nThe best, quickest table\nA few modifications\nDiscrete numeric values\nMissing data\nOutputs\nConclusion\nAcknowledgements\n\n\n\n\nFigure 1: Happy R adapted from artwork by @allison_horst; the beach and cocktail images are from pngtree.com,\n\n\n\nTL; DR\nThe gtsummary package in R creates amazing publication / presentation / whatever-you-need-it-for ready tables of summary statistics. Try it out!\nBackground\nA colleague learning R just told me that he spent 45 minutes searching for a summary table function and couldn’t quite find anything that met his needs. When I showed him gtsummary in 5 minutes, his reaction was all\n\n\n\nvia GIPHY\n\nThis blog post is to promote gtsummary and make it more searchable for those still seeking the one table to rule them all. The gtsummary documentation is excellent so I won’t cover all of its awesome functionality, but I will add a bit of my specific experience.\nIf you are still searching for your favorite table package, here are two round up resources:\nHow to make beautiful tables in R by R for the Rest of Us (2019).\nMy favourite R package for: summarising data by Dabbling with data (2018)\nThe data\nI’ll demonstrate with the Youth Risk Behavior Surveillance System (YRBSS) data; my previous post Leveraging labelled data in R has more background details. You can download the .sav data from my github repo.\n\n\n# import data ----\ndat_raw <- haven::read_sav(here::here( \"_posts\", \"2020-12-23-leveraging-labelled-data-in-r\", \"data\", \"sadc_2017_national.sav\"))\n\n\n\nGetting started\nThis material was developed using:\nSoftware / package\nVersion\nR\n4.0.5\nRStudio\n1.4.1103\ntidyverse\n1.3.0\nhere\n1.0.1\nhaven\n2.3.1\nlabelled\n2.5.0\ngtsummary\n1.3.5\nwritexl\n1.4.0\n\n\nlibrary(tidyverse)  # general use ----\nlibrary(here)       # file paths  ----\nlibrary(haven)      # import .sav files ----  \nlibrary(labelled)   # tools for labelled data ----\nlibrary(gtsummary)  # produce summary tables ----\nlibrary(writexl)    # export excel tables ----\n\n\n\nData preparation\nFirst, I import the data with haven::read_sav().\n\n\n# import data ----\ndat_raw <- haven::read_sav(here::here( \"_posts\", \"2020-12-23-leveraging-labelled-data-in-r\", \"data\", \"sadc_2017_national.sav\"))\n\n\n\nThen I keep just two years of data for comparison on three question.\n\n\ndat <- dat_raw %>% \n  # two most recent years ----\n  dplyr::filter(year %in% c(2015, 2017)) %>% \n  # a few variables\n  dplyr::select(record, year, stheight, q12, q69) \n\n\n\nHere is a quick preview of the data.\n\n\n# print data for viewing----\ndat\n\n\n# A tibble: 30,389 x 5\n    record  year stheight              q12                       q69\n     <dbl> <dbl>    <dbl>        <dbl+lbl>                 <dbl+lbl>\n 1 1349684  2015       NA NA                3 [Stay the same weight]\n 2 1349685  2015       NA  1 [0 days]       3 [Stay the same weight]\n 3 1349686  2015       NA NA               NA                       \n 4 1349687  2015       NA  1 [0 days]       1 [Lose weight]         \n 5 1349688  2015       NA NA                2 [Gain weight]         \n 6 1349689  2015       NA  1 [0 days]       3 [Stay the same weight]\n 7 1349690  2015       NA NA                2 [Gain weight]         \n 8 1349691  2015       NA  1 [0 days]       1 [Lose weight]         \n 9 1349692  2015       NA  3 [2 or 3 days]  1 [Lose weight]         \n10 1349693  2015       NA  1 [0 days]       3 [Stay the same weight]\n# ... with 30,379 more rows\n\nI recommend reading Leveraging labelled data in R for more details on labelled data, but for now here is a quick preview of the metadata.\n\n\n# create data dictionary ----\ndictionary <- labelled::look_for(dat, details = TRUE)\n\n\n\n\n\n# view select items from dictionary ----\ndictionary %>% \n  dplyr::select(variable, label, value_labels) %>% \n  knitr::kable() \n\n\nvariable\nlabel\nvalue_labels\nrecord\nRecord ID\n\nyear\n4-digit Year of survey\n\nstheight\nHeight in meters\n\nq12\nWeapon carrying\n[1] 0 days; [2] 1 day; [3] 2 or 3 days; [4] 4 or 5 days; [5] 6 or more days\nq69\nWeight loss\n[1] Lose weight; [2] Gain weight; [3] Stay the same weight; [4] Not trying to do anything\n\nThe best, quickest table\nTo get a quick summary table, first retain only the variables you want to see in the summary table. In this case, I removed record to avoid summary statistics of an ID. Next, because the imported data has value labels from the .sav data file, I converted those variables to a factor for summarizing.\n\n\ndat %>% \n  # remove from summary table ----\n  dplyr::select(-record) %>% \n  # convert labelled values to a factor ----\n  mutate_at(vars(matches(\"q\")), haven::as_factor) %>% \n  # create a table with columns for year ----\n  gtsummary::tbl_summary(by = year)\n\n\nCharacteristic\n      2015, N = 15,6241\n      2017, N = 14,7651\n    Height in meters\n      1.68 (1.60, 1.75)\n      1.68 (1.60, 1.75)\n    Unknown\n      1,266\n      1,619\n    Weapon carrying\n      \n      \n    0 days\n      11,897 (82%)\n      10,027 (85%)\n    1 day\n      494 (3.4%)\n      349 (3.0%)\n    2 or 3 days\n      580 (4.0%)\n      398 (3.4%)\n    4 or 5 days\n      202 (1.4%)\n      166 (1.4%)\n    6 or more days\n      1,250 (8.7%)\n      798 (6.8%)\n    Unknown\n      1,201\n      3,027\n    Weight loss\n      \n      \n    Lose weight\n      6,676 (48%)\n      5,462 (47%)\n    Gain weight\n      2,439 (18%)\n      2,247 (19%)\n    Stay the same weight\n      2,354 (17%)\n      1,833 (16%)\n    Not trying to do anything\n      2,375 (17%)\n      1,999 (17%)\n    Unknown\n      1,780\n      3,224\n    \n        \n          1\n          \n           \n          Statistics presented: Median (IQR); n (%)\n          \n      \n    \n\nSo easy to obtain, and so readable!\nAnd wait - did you see that?! The raw data had variable names of q12, stheight, and q69 but the table printed the variable label! (I previously tweeted about the awesome package pairing of haven and gtsummary.) If your data does not come with handy labels, you can create them with the label option in tbl_summary or with the var_label function in the labelled package.\nA few modifications\nHere are a few modifications you might be interested in trying to customize your table, including adding an overall column, custom statistic formatting, and table styling. Note that there is an overall N that corresponds to the number of observations, and each each variable can have its own N that corresponds to the number of non-missing observations for that variable.\n\n\ndat %>% \n  # remove from summary table\n  dplyr::select(-record) %>% \n  # covert labelled values to a factor ----\n  mutate_at(vars(matches(\"q\")), haven::as_factor) %>% \n  tbl_summary(by = year,\n    # custom statistic formats ----\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\",\n                     all_categorical() ~ \"{p}% ({n} / {N})\")\n  ) %>%\n  add_overall() %>% # add an overall column\n  add_n() %>% # add column with total number of non-missing observations\n  add_p() %>% # test for a difference between groups\n  bold_labels() \n\n\nCharacteristic\n      N\n      Overall, N = 30,3891\n      2015, N = 15,6241\n      2017, N = 14,7651\n      p-value2\n    Height in meters\n      27,504\n      1.69 (0.10)\n      1.69 (0.10)\n      1.69 (0.10)\n      0.12\n    Unknown\n      \n      2,885\n      1,266\n      1,619\n      \n    Weapon carrying\n      26,161\n      \n      \n      \n      <0.001\n    0 days\n      \n      84% (21,924 / 26,161)\n      82% (11,897 / 14,423)\n      85% (10,027 / 11,738)\n      \n    1 day\n      \n      3.2% (843 / 26,161)\n      3.4% (494 / 14,423)\n      3.0% (349 / 11,738)\n      \n    2 or 3 days\n      \n      3.7% (978 / 26,161)\n      4.0% (580 / 14,423)\n      3.4% (398 / 11,738)\n      \n    4 or 5 days\n      \n      1.4% (368 / 26,161)\n      1.4% (202 / 14,423)\n      1.4% (166 / 11,738)\n      \n    6 or more days\n      \n      7.8% (2,048 / 26,161)\n      8.7% (1,250 / 14,423)\n      6.8% (798 / 11,738)\n      \n    Unknown\n      \n      4,228\n      1,201\n      3,027\n      \n    Weight loss\n      25,385\n      \n      \n      \n      <0.001\n    Lose weight\n      \n      48% (12,138 / 25,385)\n      48% (6,676 / 13,844)\n      47% (5,462 / 11,541)\n      \n    Gain weight\n      \n      18% (4,686 / 25,385)\n      18% (2,439 / 13,844)\n      19% (2,247 / 11,541)\n      \n    Stay the same weight\n      \n      16% (4,187 / 25,385)\n      17% (2,354 / 13,844)\n      16% (1,833 / 11,541)\n      \n    Not trying to do anything\n      \n      17% (4,374 / 25,385)\n      17% (2,375 / 13,844)\n      17% (1,999 / 11,541)\n      \n    Unknown\n      \n      5,004\n      1,780\n      3,224\n      \n    \n        \n          1\n          \n           \n          Statistics presented: Mean (SD); % (n / N)\n          \n        \n          2\n          \n           \n          Statistical tests performed: Wilcoxon rank-sum test; chi-square test of independence\n          \n      \n    \n\nIn addition, gtsummary makes an educated guess on how to summarize your data and which statistical test to use. Pay attention to the footnote on the statistical tests performed and adjust if needed with the test argument in the add_p function..\nDiscrete numeric values\nOne default I frequently correct is treatment of discrete numeric values. For example, consider a rating scale with possible values of 1, 2, 3, … 7, but in which respondents only select values of 3, 4, 5. Making an educated guess and only seeing three unique values, gtsummary will treat this as a categorical variable and return frequencies of those values; however, you may still want a mean. You can override these potentially undesirable defaults in gtsummary. 🙌\nThis data set does not have a great example of this, so I’ll make one. I am going to pretend that q12 is numeric for demonstration,\n\n\ndat %>% \n  # keep q12 for summary table ----\n  dplyr::select(q12) %>% \n  # convert labelled values to numeric by removing value labels ----\n  mutate_at(vars(q12), haven::zap_labels) %>% \n  # subtract one to make pretend example more realistic ----\n  mutate_at(vars(q12), ~ . - 1) %>% \n  # create a table with columns for year ----\n  gtsummary::tbl_summary()\n\n\nCharacteristic\n      N = 30,3891\n    Weapon carrying\n      \n    0\n      21,924 (84%)\n    1\n      843 (3.2%)\n    2\n      978 (3.7%)\n    3\n      368 (1.4%)\n    4\n      2,048 (7.8%)\n    Unknown\n      4,228\n    \n        \n          1\n          \n           \n          Statistics presented: n (%)\n          \n      \n    \n\nIn this table, a 4 represents carrying a weapon to school 4 days in the past month. Notice the default is to provide a frequency summary as if it is a categorical variable due the few values present. If instead you want a mean to summarize this variable, specify the variable as continuous with the type argument in tbl_summary.\n\n\ndat %>% \n  # keep q12 for summary table ----\n  dplyr::select(q12) %>% \n  # convert labelled values to numeric by removing value labels ----\n  mutate_at(vars(q12), haven::zap_labels) %>% \n  # subtract one to make pretend example more realistic ----\n  mutate_at(vars(q12), ~ . - 1) %>% \n  # create a table with columns for year ----\n  gtsummary::tbl_summary(\n    # treat q12 variable as continuous ----\n    type = list(q12 ~ \"continuous\"),\n    # custom statistic format ----\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\")\n  )\n\n\nCharacteristic\n      N = 30,3891\n    Weapon carrying\n      0.46 (1.16)\n    Unknown\n      4,228\n    \n        \n          1\n          \n           \n          Statistics presented: Mean (SD)\n          \n      \n    \n\nMissing data\nBefore we part, let’s make sure we understand how the package handles missing data and our options.\n\n\ndat %>% \n  dplyr::select(q69) %>% \n  # convert labelled values to a factor ----\n  mutate_at(vars(matches(\"q\")), haven::as_factor) %>% \n  gtsummary::tbl_summary()\n\n\nCharacteristic\n      N = 30,3891\n    Weight loss\n      \n    Lose weight\n      12,138 (48%)\n    Gain weight\n      4,686 (18%)\n    Stay the same weight\n      4,187 (16%)\n    Not trying to do anything\n      4,374 (17%)\n    Unknown\n      5,004\n    \n        \n          1\n          \n           \n          Statistics presented: n (%)\n          \n      \n    \n\nThis summary table tells us that there are 30,389 records in the years 2015 and 2017; 25,385 of them have non-missing values and 5,004 have missing values. The percents shown in the table are the percent of the non-missing base N; that is, 12,138 / 25,385 is 48%.\nYou can suppress printing the count of unknown values if you like, which does not change any of other numbers in your table.\n\n\ndat %>% \n  dplyr::select(q69) %>% \n  # convert labelled values to a factor ----\n  mutate_at(vars(q69), haven::as_factor) %>% \n  gtsummary::tbl_summary(\n    # supress printing count of unknown values ----\n    missing = \"no\"\n  )\n\n\nCharacteristic\n      N = 30,3891\n    Weight loss\n      \n    Lose weight\n      12,138 (48%)\n    Gain weight\n      4,686 (18%)\n    Stay the same weight\n      4,187 (16%)\n    Not trying to do anything\n      4,374 (17%)\n    \n        \n          1\n          \n           \n          Statistics presented: n (%)\n          \n      \n    \n\nIf you want the N at the top of the column to reflect the N for non-missing observations, I would remove those in your data cleaning process.\n\n\ndat %>% \n  dplyr::select(q69) %>% \n  # convert labelled values to a factor ----\n  mutate_at(vars(q69), haven::as_factor) %>% \n  # remove all observations with missing values ----\n  drop_na() %>% \n  gtsummary::tbl_summary()\n\n\nCharacteristic\n      N = 25,3851\n    Weight loss\n      \n    Lose weight\n      12,138 (48%)\n    Gain weight\n      4,686 (18%)\n    Stay the same weight\n      4,187 (16%)\n    Not trying to do anything\n      4,374 (17%)\n    \n        \n          1\n          \n           \n          Statistics presented: n (%)\n          \n      \n    \n\nOn the other hand, if the missing data represents a valid category that you want counted, you could replace the missing values (which would shift the percentages).\n\n\ndat %>% \n  dplyr::select(q69) %>% \n  # convert labelled values to a factor ----\n  mutate_at(vars(q69), haven::as_factor) %>% \n  # replace missing values for factor ----\n  mutate_at(vars(q69), forcats::fct_explicit_na, \"Missing weight action\") %>% \n  gtsummary::tbl_summary()\n\n\nCharacteristic\n      N = 30,3891\n    Weight loss\n      \n    Lose weight\n      12,138 (40%)\n    Gain weight\n      4,686 (15%)\n    Stay the same weight\n      4,187 (14%)\n    Not trying to do anything\n      4,374 (14%)\n    Missing weight action\n      5,004 (16%)\n    \n        \n          1\n          \n           \n          Statistics presented: n (%)\n          \n      \n    \n\nOutputs\ngtsummay has a variety of supported outputs, including html and word. I work with many who prefer to see tables in excel, and this an output not directly supported in gtsummary. (No shade here, I’m waiting patiently for this to happen in the gt package. 😁) However, gtsummary does have a work around available with the gtsummary::as_tibble() function. Save the table as an object, convert to a tibble, and then export.\n\n\n# 1 - save table ----\ngt_table <- dat %>% \n  dplyr::select(q69) %>% \n  # convert labelled values to a factor ----\n  mutate_at(vars(matches(\"q\")), haven::as_factor) %>% \n  gtsummary::tbl_summary() %>% \n  # 2 convert to tibble ----\n  gtsummary::as_tibble()\n\n\n\nAnd now you can export to excel.\n\n\n# 3 - export to excel ----\nwritexl::write_xlsx(gt_table, here::here( \"_posts\", \"2021-07-14-polished-summary-tables-in-r-with-gtsummary\",  \"example_gtsummary.xlsx\"))\n\n\n\nYou can download the exported output example_gtsummary.xlsx.\nConclusion\nThe gtsummary package has readable output that is easily customizable. My quest for the best presentation ready R table is over! 🥳\nAcknowledgements\nDaniel Sjoberg + team rock for their work on gtsummary! Thanks to my colleague Patrick Freeman for reviewing this post and providing feedback, and thanks to new learneRs who inspire me to write. 💜\n\n\n\n",
    "preview": "posts/2021-07-14-polished-summary-tables-in-r-with-gtsummary/r-gtsummary.png",
    "last_modified": "2022-08-11T11:39:28-04:00",
    "input_file": {},
    "preview_width": 2540,
    "preview_height": 1932
  },
  {
    "path": "posts/2021-06-01-custom-interactive-sunbursts-with-ggplot-in-r/",
    "title": "Custom interactive sunbursts with ggplot in R",
    "description": "geom_rect() + geom_coord() + {ggiraph}",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2021-06-01",
    "categories": [],
    "contents": "\n\nContents\nTL; DR\nBackground\nThe data\nGetting started\nData processing\nImport\nSummarize\nAssign colors\nOuter ring\nCombine colors\nInner ring\n\nNon-interactive sunbursts\nVariable length sunburst\nFixed length sunburst\n\nInteractive sunburst\nVariable length sunburst\nFixed length sunburst\n\nFin\nFor fun\n\nTL; DR\nI manually defined coordinates for geom_rect() to create custom sunbursts with ggplot(). Pair this with {ggiraph} package to create an awesome interactive graphic!\nBackground\nSunbursts in R first caught my eye from the sunburstR package with its amazing interactivity through d3. However, I had trouble customizing it and sought a ggplot alternative. You can create a version using geom_col, as shown in this RStudio Community post; I chose to go the more complex geom_rect route for more customization options.1\n\npretty pumped I got this custom \"sunburst\"-esque figure to work!!  #ggplot #rstats pic.twitter.com/L88IQWoVCs— Shannon Pileggi (@PipingHotData) January 10, 2020\n\n\nThe data\nTo illustrate the concept on a publicly available data set, I am using the May 2021 #TidyTuesday salary data. Here, I’ll be looking at the top 5 industries and their top 5 job titles by number of respondents. I don’t think this visualization is necessarily the best for this data set, I am just using this data as a proof of concept.\nThe data attributes that work well for this visualization include:\n4 to 6 categories, with associated weight or frequency of occurrence\nour categories are the top 5 industries\n\n4 to 6 sub-items within each category, with associated weight or frequency of occurrence\nour sub-items are the top 5 job titles within industry\n\noptional: a metric for the sub-items\nour metric is the median salary for the titles\n\nGetting started\nThis material was developed using:\nSoftware / package\nVersion\nR\n4.0.5\nRStudio\n1.4.1103\ntidyverse\n1.3.1\nggiraph\n0.7.8\nglue\n1.4.2\npals\n1.6\n\n\nlibrary(tidyverse) # general use\nlibrary(ggiraph)   # interactive ggplots\nlibrary(glue)      # pasting strings\nlibrary(pals)      # color palette\n\n\n\nData processing\nImport\nFirst, import the data from GitHub.\n\n\n# import data\nsurvey_raw <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-05-18/survey.csv')\n\n\n\nSummarize\nNext, summarize the data in terms of the top 5 industries and top 5 job titles. For the job title, we’ll get (1) the percent of respondents with that title, and (2) the median salary for that title.\n\n\nsurvey_summary <- survey_raw %>%\n  # identify top industries\n  group_by(industry) %>%\n  mutate(# number of respondents per industry\n    n_industry = n()) %>%\n  ungroup() %>%\n  arrange(desc(n_industry), industry) %>%\n  mutate(\n    # rank of industry in terms of number of survey respondents\n    # know a better way to do this bit? pls let me know!\n    n_industry_rank = as.numeric(factor(-n_industry)),\n    # very light cleaning of job title\n    job_title = tolower(job_title) %>%\n      str_remove_all(\"[:digit:]+\") %>%\n      ifelse(. == \"rn\", \"registered nurse\", .) %>%\n      str_trim()\n  ) %>%\n  # reduce to top 5 industries present\n  filter(n_industry_rank %in% 1:5) %>%\n  # identify top 5 job titles in each industry\n  group_by(n_industry_rank, n_industry, industry, job_title) %>%\n  summarize(# number of titles within industry\n    n_title = n(),\n    # median salary per title\n    median_salary = median(annual_salary)) %>%\n  ungroup() %>%\n  arrange(n_industry_rank, industry, desc(n_title)) %>%\n  group_by(industry) %>%\n  # keep top 5 job titles within industry\n  slice(1:5) %>%\n  # recompute number of respondents per industry based on titles selected\n  mutate(n_industry = sum(n_title)) %>%\n  ungroup() %>%\n  mutate(\n    # relative scale of salary for plotting, on 0 - 100 scale\n    median_salary_scaled = median_salary / max(median_salary) * 100,\n    # base n respondents\n    n_total = sum(n_title),\n    # percent of respondents per title\n    pct_title = n_title / n_total * 100,\n    pct_industry = n_industry / n_total * 100,\n    # unique job title, as some repeat across industry\n    title_unique = glue::glue(\"{str_sub(industry, 1, 3)}: {job_title}\")\n  ) %>%\n  arrange(desc(pct_industry, pct_title))\n\n\n\nHere is a preview of our data, which is the basis needed for the sunburst’s custom rectangles.\n\n\n# view input data\nsurvey_summary %>%\n  dplyr::select(\n    industry,\n    n_industry,\n    pct_industry,\n    job_title,\n    n_title,\n    pct_title,\n    median_salary,\n    median_salary_scaled\n  ) %>%\n  mutate_at(vars(pct_industry, pct_title, median_salary_scaled),\n            scales::number,\n            accuracy = 0.1) %>%\n  DT::datatable()\n\n\n\n\nAssign colors\nPick a color palette for industry, and name the color vector.\n\n\ncolors_industry <- pals::cubicyf(n = 5) %>% \n  set_names(unique(survey_summary[[\"industry\"]]))\n\n\n\n\n\ncolors_industry\n\n\n                   Computing or Tech \n                           \"#830CAB\" \n        Education (Higher Education) \n                           \"#6574FA\" \n                          Nonprofits \n                           \"#3BBCAC\" \n                         Health care \n                           \"#62E44A\" \nGovernment and Public Administration \n                           \"#CCEC5A\" \n\nOuter ring\nNext, I manually define rectangles for the outer ring of the sunburst. I figured out the bits to create custom rectangles from various blog posts that I did not save. It could have been this post on variable width column charts. I got tips on how to angle text for labels from the R Graph Gallery’s circular barplots; sample code is shown below but not implemented in the figure.\nThe idea of the rectangles’ color assignment is that darker shades represent more frequent or heavier weight items within a category. To create the colors, I concocted a solution to implement a degree of transparency off of the original industry colors based on the total number of sub-categories. Jake Riley shared with me an alternative solution for sub-category color assignment that he contributed on Stack Overflow.\n\n\nring_outer <- survey_summary %>%\n  # join colors to data frame ----\n  left_join(enframe(colors_industry, name = \"industry\", value = \"color_industry\"),\n          by = \"industry\") %>%\n  # arrange descending industry but ascending title\n  # to compute title colors\n  arrange(desc(n_industry), industry, n_title, job_title) %>%\n  group_by(industry) %>%\n  mutate(# enumerate title in industry\n    id_title = row_number(),\n    # number of titles per industry\n    # this can vary - it does not have to be same number per category\n    num_title = max(id_title)) %>%\n  ungroup() %>%\n  mutate(\n    # degree of transparency based on number of attributes ----\n    color_trans = id_title / num_title ,\n    color_title = map2_chr(color_industry, color_trans, ~ adjustcolor(.x, .y))\n  ) %>%\n  # this arrange specifies the ordering of the figure\n  # clockwise ---\n  # had to include job title here as health care had ties in title frequency\n  arrange(-pct_industry, industry,-pct_title, desc(job_title)) %>%\n  # counter clockwise ---\n  # arrange(pct_industry, industry, pct_title, job_title) %>%\n  mutate(industry = fct_inorder(industry),\n         title_unique = fct_inorder(title_unique)) %>%\n  group_by(industry) %>%\n  mutate(cum_title_pct = cumsum(pct_title)) %>%\n  ungroup() %>%\n  mutate(cum_industry_pct = cumsum(cum_title_pct),\n         cum_all = cumsum(pct_title)) %>%\n  mutate(\n    # compute coordinates of the rectangles ----\n    # the 0.3 is an adjustment on the 0 to 100 scale to add a small\n    # amount of white space between rectangles\n    rect_x_max = cumsum(pct_title) - 0.3,\n    # xmax ----\n    rect_x_min = rect_x_max - pct_title + 0.3,\n    # xmin ----\n    rect_x_mid = (rect_x_min + rect_x_max) / 2,\n    # xmid ----\n    # angles in case adding text to plot, not shown in figures below ----\n    angle = 90 - 360 * rect_x_mid / 100,\n    hjust = ifelse(angle < -90, 1, 0),\n    angle = ifelse(angle < -90, angle + 180, angle),\n    # label for ggiraph\n    label_title = glue::glue(\n      '{scales::percent(pct_industry/100, accuracy = 1)} ({n_industry}/{n_total}) {industry}\\n {scales::percent(pct_title/100, accuracy = 1)} ({n_title}/{n_total}) {job_title} \\n {scales::dollar(median_salary)} median salary'\n    )\n  )\n\n\n\n\n\n# view sunburst data\nring_outer %>% \n   mutate_at(vars(pct_industry,pct_title, median_salary_scaled, rect_x_max, rect_x_min), scales::number, accuracy = 0.1) %>% \n  dplyr::select(industry, n_industry, pct_industry, job_title, n_title, pct_title, id_title, num_title, color_trans, color_title, rect_x_max, rect_x_min, median_salary, median_salary_scaled) %>% \nDT::datatable()\n\n\n\n\nCombine colors\nCombine all industry and title colors in a single vector.\n\n\ncolors_title <- ring_outer[[\"color_title\"]] %>% \n  set_names(ring_outer[[\"title_unique\"]])\n\nall_colors <- c(colors_industry, colors_title)\n\n\n\nInner ring\nLastly in data prep, we create a smaller data set of rectangle coordinates just for the industries. The requires a dummy extra row to create white space between the first and last categories.\n\n\n# create an inner ring for industry ----\n# there is probably a better way to do this too, please let me know\n# if you have suggestions to streamline!\nring_inner <- ring_outer %>%\n  dplyr::select(industry,\n                pct_industry,\n                n_industry,\n                n_total,\n                rect_x_max,\n                rect_x_min) %>%\n  # first, get stopping point for each rectangle from least common item ----\n  arrange(industry, desc(rect_x_max)) %>%\n  group_by(industry) %>%\n  # keep one entry per industry ----\n  slice(1) %>%\n  ungroup() %>%\n  dplyr::select(industry, pct_industry, n_industry, n_total, stop = rect_x_max) %>%\n  # second, get starting point for each rectangle from most common item ----\n  left_join(\n  ring_outer %>%\n    dplyr::select(\n      industry,\n      pct_industry,\n      n_industry,\n      n_total,\n      rect_x_max,\n      rect_x_min\n    ) %>%\n    arrange(industry, rect_x_max) %>%\n    group_by(industry) %>%\n    slice(1) %>%\n    ungroup() %>%\n    dplyr::select(industry, pct_industry, n_industry, n_total, start = rect_x_min),\n  by = c(\"industry\", \"pct_industry\", \"n_industry\", \"n_total\")\n  ) %>%\n  # insert blank row for extra white space where circle starts/stops ---\n  bind_rows(tibble(\n    attr_category = NA,\n    start = 99.7,\n    stop = 100.1\n  )) %>%\n  mutate(\n    # compute midpoint for labeling if needed ----\n    mid = (stop - start) / 2 + start,\n    # a label for industry pct ----\n    label_industry = ifelse(\n      is.na(industry),\n      NA,\n      glue::glue(\n        '{scales::percent(pct_industry/100, accuracy = 1)} ({n_industry}/{n_total}) {industry} '\n      )\n    )\n  )\n\n\n\n\n\n# view industry rectangles\nring_inner %>%\n  dplyr::select(industry, pct_industry, start, stop) %>%\n  mutate_at(vars(pct_industry, start, stop), scales::number, accuracy = 0.1) %>%\n  DT::datatable()\n\n\n\n\nNon-interactive sunbursts\nVariable length sunburst\nFinally, we can plot! This first figure has variable length bars, which are represented by the scaled median salary. The outer ring goes from a height of 0 up to a height of the scaled median salary, which is 100 here. Adding extra negative space to the ylim creates the donut in the middle.\n\n\n# outer ring for job title\nring_outer %>%\n  ggplot() +\n  geom_rect(\n    aes(\n      ymin = 1,\n      # variable length bars\n      ymax = median_salary_scaled,\n      xmin = rect_x_min,\n      xmax = rect_x_max,\n      fill = title_unique\n    ),\n    show.legend = FALSE\n  ) +\n  coord_polar() +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    panel.grid = element_blank(),\n    legend.position = \"none\",\n  ) +\n  # fiddle lower limit here to control donut size\n  ylim(-40, 100) +\n  scale_fill_manual(values = all_colors) +\n  # inner industry ring ----\n  geom_rect(data = ring_inner,\n          aes(\n            # the inner ring spans a height from 0 to -20\n            ymin = -20,\n            ymax = 0,\n            xmin = start,\n            xmax = stop,\n            fill = industry\n          ))\n\n\n\n\nFigure 1: Non-interactive sunburst with variable length burst.\n\n\n\nWell, it is an example! Not quite as aesthetically appealing as my original example shown in the tweet, but it will do to put code to the page.\nHere, we see that the Computing or Tech industry had the most respondents, and within that industry the most common job title was software engineer. The median salary of software engineer was $125,000, which was less than the next most common job title of senior software engineer, with a median salary of $151,500.\nAlthough I think this figure can be eye-catching, it is challenging to make it informative with labeling and still maintain appeal. For this reason, I have omitted labels.\nFixed length sunburst\nFor a simpler alternative, drop the job title metric of median salary and only show the frequency of industry and title in survey respondents by fixing the rectangle height. This figure can effectively communicate parts of a whole.\n\n\n# outer ring for job title\nring_outer %>%\n  ggplot() +\n  geom_rect(\n    aes(\n      ymin = 1,\n      # fix rectangle height here\n      ymax = 20, \n      xmin = rect_x_min,\n      xmax = rect_x_max,\n      fill = title_unique\n    ),\n    show.legend = FALSE\n  ) +\n  coord_polar() +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    panel.grid = element_blank(),\n    legend.position = \"none\",\n  ) +\n  # fiddle lower limit here to control donut size\n  # reduce max lim as well\n  ylim(-40, 30) +\n  scale_fill_manual(values = all_colors) +\n  # inner industry ring ----\n  geom_rect(data = ring_inner,\n          aes(\n            # the inner ring spans a height from 0 to -20\n            ymin = -20,\n            ymax = 0,\n            xmin = start,\n            xmax = stop,\n            fill = industry\n          ))\n\n\n\n\nFigure 2: Non-interactive sunburst with fixed length burst.\n\n\n\nInteractive sunburst\nWith some very small tweaks to incorporate the {ggiraph} package, we can transform this to an interactive sunburst!\nVariable length sunburst\nHover over figure to view summary statistics!\n\n\n# outer ring for job title\ngg_interactive_variable <- ring_outer %>%\n  ggplot() +\n  # change 1: use geom_rect_interactive function from ggiraph\n  ggiraph::geom_rect_interactive(\n    aes(\n      ymin = 1,\n      # variable length bars\n      ymax = median_salary_scaled,\n      xmin = rect_x_min,\n      xmax = rect_x_max,\n      fill = title_unique,\n      # change 2: add id for hover over tooltip ---\n      tooltip = label_title\n    ),\n    show.legend = FALSE\n  ) +\n  coord_polar() +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    panel.grid = element_blank(),\n    legend.position = \"none\",\n  ) +\n  # fiddle lower limit here to control donut size\n  # reduce max lim as well\n  ylim(-40, 100) +\n  scale_fill_manual(values = all_colors) +\n  # change 3: add interactivity to inner ring  ----\n  ggiraph::geom_rect_interactive(\n    data = ring_inner,\n          aes(\n            # the inner ring spans a height from 0 to -20\n            ymin = -20,\n            ymax = 0,\n            xmin = start,\n            xmax = stop,\n            fill = industry,\n            # change 4: add tooltip to inner ring\n            tooltip = label_industry\n          ))\n\n\n\n\n\n# change 5: create interactive version using ggirafe\ngirafe(ggobj = gg_interactive_variable,\n       # this option fills tooltip background with same color from figure\n       options = list(opts_tooltip(use_fill = TRUE)))\n\n\n\n\nFixed length sunburst\nHover over figure to view summary statistics!\n\n\n# outer ring for job title\ngg_interactive_fixed <- ring_outer %>%\n  ggplot() +\n  # change 1: use geom_rect_interactive function from ggiraph\n  ggiraph::geom_rect_interactive(\n    aes(\n      ymin = 1,\n      # fix rectangle height here\n      ymax = 20, \n      xmin = rect_x_min,\n      xmax = rect_x_max,\n      fill = title_unique,\n      # change 2: add id for hover over tooltip ---\n      tooltip = label_title\n    ),\n    show.legend = FALSE\n  ) +\n  coord_polar() +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    panel.grid = element_blank(),\n    legend.position = \"none\",\n  ) +\n  # fiddle lower limit here to control donut size\n  ylim(-40, 30) +\n  scale_fill_manual(values = all_colors) +\n  # change 3: add interactivity to inner ring  ----\n  ggiraph::geom_rect_interactive(\n    data = ring_inner,\n          aes(\n            # the inner ring spans a height from 0 to -20\n            ymin = -20,\n            ymax = 0,\n            xmin = start,\n            xmax = stop,\n            fill = industry,\n            # change 4: add tooltip to inner ring\n            tooltip = label_industry\n          ))\n\n\n\n\n\n# change 5: create interactive version using ggirafe\ngirafe(ggobj = gg_interactive_fixed,\n       # this option fills tooltip background with same color from figure\n       options = list(opts_tooltip(use_fill = TRUE)))\n\n\n\n\nFin\nLike this figure? Use it in your own work? Or have suggestions for improvement in either code or the graphic? Please, let me know!\nFor fun\nI did experiment with some variations on this figure that did not make the cut!2\n\nMonday's work 🤔🥰 #rstats #ggplot pic.twitter.com/GbZWLhLeDA— Shannon Pileggi (@PipingHotData) February 4, 2020\n\n\n\nAlt text for tweet: Thin inner ring with solid shades shows relative frequency of broader categories. Outer ring juts out - shades and width indicate relative frequency of items within category; bar length indicates a metric on item.↩︎\nAlt text: Left hand side: 4 year old coloring letter “O”, in rainbow colors and outside the line; right hand side: figure created from ggplot with shape of letter “O” in rainbow colors with rectangles both inside and outside the “O”.↩︎\n",
    "preview": "posts/2021-06-01-custom-interactive-sunbursts-with-ggplot-in-r/sunburst_preview.PNG",
    "last_modified": "2022-08-11T11:39:28-04:00",
    "input_file": {},
    "preview_width": 528,
    "preview_height": 475
  },
  {
    "path": "posts/2021-04-01-deploy-previews-with-netlifly/",
    "title": "Deploy previews with Netlifly",
    "description": "Collaborative {distill} website workflows",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2021-04-01",
    "categories": [],
    "contents": "\n\nContents\nTL; DR\nBackground\nNetlifly settings\nBack in website development\nAcknowledgements\n\nTL; DR\nIf you are deploying your {distill} website through Netlifly, you can enable deploy previews to create temporary urls to preview a branch.\nBackground\nMy usual website workflow so far has been to draft posts in my master branch with draft: true in the yaml, and then when I am ready to publish switch to draft: false and commit to my main/master branch. Recently, I’ve been inspired to try alternative workflows:\n\n\nI’m also a huge convert to the church of making PRs to yourself. Start with an R Markdown project like a simple slide deck or a bookdown book, then get into the habit of every change is a branch + PR. Low cost, but teaches you a lot! pic.twitter.com/Ee3tuoS4Ef\n\n— Alison Presmanes Hill (@apreshill) February 20, 2021\n\nI also collaborated with someone on a blog post for the first time, and I learned that you can send links to preview branches! 🎉 For both solo and collaborative posts, this is my new workflow.\nI did a quick search for existing guides, and I found that Garrick Aden‑Buie has a comprehensive 2019 blog post titled A Blogdown New Post Workflow with Github and Netlify. Some of the steps are specific to {blogdown}, so here is my {distill} take on it.\nNetlifly settings\nFirst, make sure you have deploy previews enabled on Netlifly. Login to Netlifly, go to your site, and then:\n➡️ Site settings\n➡️ Build & deploy\n➡️ Deploy contexts\n➡️ Deploy previews\n➡️ Select Any pull request against your production branch / branch deploy branches Netlify will generate a deploy preview with a unique URL for each built pull request.\n\n\n\nFigure 1: Screenshot of Netlifly settings with Deploy previews set to “Any pull request…”.\n\n\n\nBack in website development\nCreate a branch for your website repository. I am naming my branch netlifly-deploy-branch, and in the R console submit:\n\n\nusethis::pr_init(\"netlifly-deploy-branch\")\n\n\n\nCreate a post. In R console submit:\n\n\ndistill::create_post(\"Deploy previews with Netlifly\")\n\n\n\nDraft post; set draft: false in the yaml.\nKnit post. Depending on what you are doing in your branch, you may also need to 🔨 Build Website.\nCommit everything. In terminal submit:\n\ngit add .\ngit commit -m \"draft netlifly post\"\n\nPush to GitHub. In R console submit:\n\n\nusethis::pr_push()\n\n\n\nIn GitHub, click “Create pull request”.\nSome automatic checks will run. Click on “Details” where it says “Deploy preview ready!”\n\n\n\nFigure 2: Screenshot of checks run on GitHub pull request. Click on “Details” where it says “Deploy preview ready!” to open the preview url.\n\n\n\nThis opens up a window in my browser with the url https://deploy-preview-3--flamboyant-mccarthy-854a9b.netlify.app/. Send this link to a friend for review!\nEdit draft, repeat steps 4-8.\nWhen ready to publish, back in GitHub click on “Merge pull request”.\nAdded April 3, 2020:\nMaëlle Salmon recommends “Squash and merge” from the drop down menu to create a cleaner commit history.\nShe also recommends deleting branches after merging and provided this link to Refined GitHub, a “browser extension that simplifies the GitHub interface and adds useful features.” 😄\nI’m new to this workflow and this post was written quickly - edits and suggestions are welcome!\nAcknowledgements\nThanks, Alison Hill!🤗\nAlso, thanks for the additional recommendations, Maëlle Salmon!\n\n\n\n",
    "preview": "posts/2021-04-01-deploy-previews-with-netlifly/img/netlifly-checks.png",
    "last_modified": "2022-08-11T11:39:28-04:00",
    "input_file": {},
    "preview_width": 1358,
    "preview_height": 704
  },
  {
    "path": "posts/2021-03-08-r-ladies-styled-code-gifs-with-xaringan-and-flipbookr/",
    "title": "R-Ladies styled code gifs with xaringan and flipbookr",
    "description": "The code behind my @WeAreRLadies curating week",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2021-03-08",
    "categories": [],
    "contents": "\n\nContents\nTL; DR\nGetting started\nCreating slides\nStyling slides\nRecording the gif\nCode gifs shared\nOther options\nAccessibility\nAcknowledgements\nOuttakes\n\nTL; DR\nI curated the @WeAreRLadies twitter account from Feb\n15 - 20, 2021. During my curation, I shared brief R code gifs created\nwith xaringan, flipbookr, a bit of custom CSS, and the\nScreenToGif recorder. Full code is\navailable in the rladies-flipbookr repo; here is an example gif:\n\n\n\nFigure 1: Walrus operator gif. You can read more about this\ntopic at https://www.tidyverse.org/blog/2020/02/glue-strings-and-tidy-eval/.\n\n\n\nGetting started\nThis material was developed using:\nSoftware / package\nVersion\nR\n4.0.3\nRStudio\n1.4.1103\ntidyverse\n1.3.0\nxaringan\n0.19\nflipbookr\n0.1.0\nCreating slides\nThe flipbookr template makes it easy to quickly create content! I\nused the classic flipbook, with defaults to walk through the code\npipeline (slides 3-4 in the template).\nTo create a flipbook slide that advances code line by line, create a\nnamed R code chunk with the include = FALSE option, and\nthen present the code chunk with flipbookr::chunk_reveal.\nThe order of defining the chunk versus revealing the chunk does not\nmatter in your RMarkdown document. Longer content was broken into\nmultiple code chunks.\n`r chunk_reveal(\"walrus\", title = \"## Walrus operator\")`\n```{r walrus, include = FALSE}\nlibrary(tidyverse)\nlibrary(rlang)\nlibrary(glue)\n\nsuffix <- \"A\" \n\ndat <- tibble(x = 1:5) %>%\n  mutate(\n    \"x_{suffix}\" := x\n  ) \n\ndat %>% \n  dplyr::select(\n    glue::glue(\"x_{suffix}\")\n    ) \n```\nStyling slides\nI first styled the content used Alison Hill’s R-Ladies xaringan theme by declaring R-Ladies CSS\noptions in the yaml parameters:\ncss: [\"default\", \"rladies\", \"rladies-fonts\"]\nresulting in\n\n\n\nFigure 2: Flipbookr walrus operator content styled with\nR-Ladies xaringan theme.\n\n\n\nHowever, the highlighting shade was a tad too dark for me, and I was\nconcerned that the font size would be too small to be legible in a gif\non twitter. I updated the CSS with bits lifted from xaringan templates and some modifications on font\nsize and color kindly developed by Silvia Canelón.\n\n\n\nFigure 3: Flipbookr walrus operator content styled with\nR-Ladies xaringan theme adjusted with lighter highlight shade and larger\nfont size.\n\n\n\nThe changes are subtle, but I hope they improve readability! The\nincreased font size did create a challenge for code formatting and for\nthe amount of code that could be shown on a single slide.\nSilvia showed me how to modify the CSS through (1) inclusion of a CSS\ncode chunk, and (2) calling a separate CSS sheet. For option (2), change the yaml\nto point to your custom CSS:\ncss: [\"default\", \"default-fonts\", \"css/rladies-adjusted.css\"]\nYou can see the full code for the default xaringan theme and the two\nways of adjusting the CSS for all the gifs I created in the shannonpileggi/rladies-flipbookr github\nrepository.\nIf you have configured a GitHub personal access token, you can fork\nand clone this repo with:\n\n\nusethis::create_from_github(\"shannonpileggi/rladies-flipbookr\")\n\n\n\nRecording the gif\nI recorded the gifs by manually advancing the slides with the screen\nrecorder ScreenToGif (for Windows OS). Bryan\nShalloway has some code to automatically create gifs from xaringan slides, but\nI didn’t go that route this time - maybe next!\nCode gifs shared\nHere are the R code gifs that I shared on twitter during my\n@WeAreRLadies curating week:\nwalrus operator\nmutate_at + vars + matches\nmutate + across + matches\nhaven + gtsummary\nnamed color vectors + scale_color_manual\nOther options\nDuring my curating week, I learned about the carbonate\npackage! This also seems like a fantastic option for aesthetically\nappealing code sharing.\nAccessibility\nVisual code sharing through gifs and images can be a great way to\nshare content! However, please remember to share the content with\neveryone. Gifs and images on their own are not accessible to members of\nour community that use assistive technology to read digital content. In\norder to make gifs and images accessible to screen readers, you can\ninclude descriptions or share code in otherwise accessible\nformats like GitHub\ngists. If I miss making any of my content accessible, please let me\nknow!\n\nAdded April 3, 2020: This article provides a framework for how to write alt\ntext for data visualization. In addition, you can install chrome or firefox browser plugins to prevent yourself from\ntweeting an image without alt text.\nAcknowledgements\nI have to give Silvia Canelón (@spcanelon) a huge\nshout out here. My curating week started on a Monday, and on Monday\nnight I was still trying to figure out how I wanted to style my code. We\nchatted on R-Ladies slack, and she quickly helped me make CSS\nadjustments to the default xaringan R-Ladies theme. After the curating\nweek, Silvia helped me fine tune the bits to share that are now in the\nrladies-flipbookr repo. Additionally, she advises me\non inclusivity and accessibility best practices, which I am still\nstriving to meet. Thank you, Silvia! 💜 🙏\nLastly, thanks to the friendly RStudio\nCommunity forum for helping me figure out both the in line verbatim I used in this post and the dplyr::select() portion of the walrus operator\ngif.\nOuttakes\nI experimented with, and actually recorded, some alternative formats\nbefore I landed on flipbookr. Here are two gifs that didn’t\nmake the cut! 🙈\n\n\n\nFigure 4: Walrus operator gif recorded as a scrolling\nRMarkdown document.\n\n\n\n\n\n\nFigure 5: Walrus operator gif recorded as code submitted to\nconsole.\n\n\n\n\n\n\n",
    "preview": "posts/2021-03-08-r-ladies-styled-code-gifs-with-xaringan-and-flipbookr/walrus-adjusted.png",
    "last_modified": "2022-08-11T11:39:28-04:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 861
  },
  {
    "path": "posts/2021-02-15-gganimating-a-geographic-introduction/",
    "title": "GGanimating a geographic introduction",
    "description": "A gif for social media",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2021-02-15",
    "categories": [],
    "contents": "\n\nContents\nTL;DR\nGetting started\nFonts\nThe data\nBase map\nAnimated map\nDiscussion\n\nTL;DR\nTo kick off my week curating the @WeAreRLadies twitter account I created a gif to introduce myself using gganimate and ggtext.\nGetting started\nThis material was developed using:\nSoftware / package\nVersion\nR\n4.0.3\nRStudio\n1.4.1103\ntidyverse\n1.3.0\ngganimate\n1.0.7\nextrafont\n0.17\nggtext\n0.1.1\nglue\n1.4.1\n\n\nlibrary(tidyverse) # general use ----\nlibrary(gganimate) # to create animated gif ----\nlibrary(extrafont) # access and enable fonts ----\nlibrary(ggtext)    # style text in ggplot ----\nlibrary(glue)      # paste strings ----  \n\n\n\nFonts\nFonts were challenging for me to get started with in R, and I got some great tips on twitter. I ended up manually installing new fonts on my Windows 10 OS and then using the extrafont package to access the fonts in R.\n\n\n# import fonts; only needed one time or after new font installation ----\n# fonts may install to multiple directories; figure out where yours are installed ----\n# and adjust path accordingly if needed ----\n# this can take a while ----\nextrafont::font_import()\n\n# do this each R session in which you want to use fonts ----\nextrafont::loadfonts(device = \"win\")\n\n# examine font family names available for use (output not shown) ----\nextrafont::fonts()\n\n\n\nThe data\nThe data needed to create a map of the US is already contained within ggplot2.\n\n\nus_states <- ggplot2::map_data(\"state\")\n\n\n\nTo show locations where I have lived, I manually compiled a data frame with city, state, latitude, longitude, and a description.\n\n\n# use <br> to render line breaks with ggtext ----\nresidence <- tribble(\n  ~city,           ~state,  ~lat,   ~long, ~years, ~description,\n  \"Raleigh\",         \"NC\", 35.82,  -78.66,  17,    \"Childhood\",\n  \"Greenville\",      \"NC\", 35.60,  -77.37,   4,    \"Undergrad at ECU\",\n  \"Atlanta\",         \"GA\", 33.76,  -84.42,  10,    \"Grad school at Emory<br>Statistician at CDC<br>Lecturer at Emory\",\n  \"San Luis Obispo\", \"CA\", 35.28, -120.66,   3,    \"Asst. Professor at Cal Poly SLO\",\n  \"Williamsburg\",    \"VA\", 37.27,  -76.71, 0.5,    \"Time with family\",\n  \"Doylestown\",      \"PA\", 40.31,  -75.13,   2,    \"Statistician at Adelphi Research\"\n) \n\n\n\nThen I needed to create a transition state for gganimate (city_order) as well as indicate connections between residences for the arrows.\n\n\nresidence_connections_prelim <- residence %>% \n  mutate(\n    # need this to create transition state ----\n    city_order = row_number() + 1,\n    # where I moved to next, for curved arrows ----\n    lat_next = lead(lat),\n    long_next = lead(long),\n    # label to show in plot, styled using ggtext ---\n    label = glue::glue(\"**{city}, {state}** ({years} yrs)<br>*{description}*\"),\n    # label of next location ----\n    label_next = lead(label)\n  ) \n\n\n\nLastly, I modified this data a bit so that the first residence shows the label at the residence with no arrow and all remaining residences show an arrow with the label positioned at the next residence.\n\n\nresidence_connections <- residence_connections_prelim %>%\n  # get first row of residence ----\n  slice(1) %>% \n  # manually modify for plotting ----\n  mutate(\n    city_order = 1,\n    label_next = label,\n    lat_next = lat,\n    long_next = long,\n    ) %>% \n  # combine with all other residences ----\n  bind_rows(residence_connections_prelim) %>% \n  # last (7th) row irrelevant ----\n  slice(1:6) %>% \n  # keep what we neeed ----\n  dplyr::select(city_order, lat, long, lat_next, long_next, label_next)\n\n\n\n\n\nresidence_connections\n\n\n# A tibble: 6 x 6\n  city_order   lat   long lat_next long_next label_next               \n       <dbl> <dbl>  <dbl>    <dbl>     <dbl> <glue>                   \n1          1  35.8  -78.7     35.8     -78.7 **Raleigh, NC** (17 yrs)~\n2          2  35.8  -78.7     35.6     -77.4 **Greenville, NC** (4 yr~\n3          3  35.6  -77.4     33.8     -84.4 **Atlanta, GA** (10 yrs)~\n4          4  33.8  -84.4     35.3    -121.  **San Luis Obispo, CA** ~\n5          5  35.3 -121.      37.3     -76.7 **Williamsburg, VA** (0.~\n6          6  37.3  -76.7     40.3     -75.1 **Doylestown, PA** (2 yr~\n\nBase map\nFor the map itself, I created a base map showing pins of the locations I have lived. Hex codes for R-Ladies colors were obtained from this blog post by Alison Hill. I briefly experimented with coord_map for projection, but then the subsequent curved arrows presented incorrectly.\n\n\nbase_map <- ggplot() +\n  # plot states ----\n  geom_polygon(\n    data = us_states,\n    aes(\n      x     = long, \n      y     = lat, \n      group = group\n      ),\n    fill  = \"#F2F2F2\",\n    color = \"white\"\n  ) +\n  # lines for pins ----\n  geom_segment(\n    data = residence,\n    aes(\n      x    = long,\n      xend = long,\n      y    = lat,\n      yend = lat + 0.5\n      ),\n    color = \"#181818\",\n    size = 0.3\n    ) +\n    # pin heads, a bit above actual location, color with R ladies lighter purple ----\n  geom_point(\n    data = residence,\n    aes(\n      x = long, \n      y = lat + 0.5\n      ),\n    size = 0.5,\n    color = \"#88398A\"\n  ) +\n  theme_void()\n\n\n\nThe sizing of the pins in the base_map appears different than in the gif below as increasing the resolution of the output gif alters plotting ratios.\n\n\nbase_map\n\n\n\n\nAnimated map\n\n\nanim <- base_map +\n  # show arrows connecting residences ----\n  geom_curve(\n    # do not include 1st residence in arrows as no arrow is intended ----\n    # and inclusion messes up transition ---\n    data = residence_connections %>% slice(-1),\n    # add slight adjustment to arrow positioning ----\n    aes(\n      y     = lat - 0.1,\n      x     = long,\n      yend  = lat_next - 0.2,\n      xend  = long_next,\n      # group is used to create the transition ----\n      group = seq_along(city_order)\n    ),\n    color = \"#181818\",\n    curvature = -0.5,\n    arrow = arrow(length = unit(0.02, \"npc\")),\n    size  = 0.2\n  ) +\n  # add in labels for pins, with inward positioning ----\n  # show labels either top left or top right of pin ----\n  geom_richtext(\n    data = residence_connections,\n    aes(\n      x     = ifelse(long_next < -100, long_next + 1, long_next - 1),\n      y     = lat_next + 5,\n      label = label_next,\n      vjust = \"top\",\n      hjust = ifelse(long_next < -100, 0, 1),\n      # group is used to create the transition ----\n      group = seq_along(city_order)\n    ),\n    size = 2,\n    label.colour = \"white\",\n    # R ladies purple ----\n    color = \"#562457\",\n    # R ladies font used in xaringan theme ----\n    family = \"Lato\"\n  ) +\n  # title determined by group value in transition ----\n  ggtitle(\"Home {closest_state} of 6\") +\n  # create animation ----\n  transition_states(\n    city_order,\n    transition_length = 2,\n    state_length = 5\n    ) +\n  # style title ----\n  theme(\n    plot.title = element_text(\n      color = \"#562457\",\n      family = \"Permanent Marker\",\n      size = 12\n      )\n    )\n\n\n\n\n\n# render and save transition ----\n# the default nframes 100 frames, 150 makes the gif a bit longer for readability ----\n# changing dimensions for output w/ height & width ----\n# increasing resolution with res ----\nanimate(anim, nframes = 150, height = 2, width = 3, units = \"in\", res = 150)\nanim_save(\"homes_animation.gif\")\n\n\n\n\nDiscussion\nI hope this gif helped you learn a little bit about me! I’m a southern girl who was never quite cool enough for California still learning how to navigate personalities in the northeast. 😂\nI do have two additional short-term residences not shown here. Although I was born in Raleigh, NC, I lived in Accra, Ghana until I was 1 year old (my father served in the US military and worked at the American Embassy in Accra). I also lived in Seville, Spain for a semester abroad during college.\nIn this gif I did not include transitions between countries (because, you know, I’m a parent working in a pandemic and I have to draw the line on my free time somewhere). In addition, aligning sizing with resolution of the output device took some experimentation.\nI would love to get to know you through your gif - if you happen to replicate this for yourself please share with me on Twitter, LinkedIn, or email! I also invite you to improve this gif with cooler transitions, international locations, awesome styles, or anything else. 😀 💜\n\n\n\n",
    "preview": "posts/2021-02-15-gganimating-a-geographic-introduction/homes_preview.png",
    "last_modified": "2022-08-11T11:39:28-04:00",
    "input_file": {},
    "preview_width": 919,
    "preview_height": 461
  },
  {
    "path": "posts/2021-01-11-from-gmailr-to-the-google-books-api/",
    "title": "From gmailr to the Google Books API",
    "description": "A parenting project to catalog my childrens' 2020 library history",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2021-01-11",
    "categories": [],
    "contents": "\nTable of Contents\nTL; DR\nBackground\nGetting started\n1 {gmailr} retrieve library notifications from gmail\n2 {stringr} extract title and author from email\n3 {httr} query google books api\n4 {purrrr} extract content from search results\n5 {DT} present books in an interactive table\nTechnical reflection\nPersonal note\nAcknowledgements\n\n\n\nTL; DR\nIn the last six months of 2020, I read 214 children’s books from the library with my daughters. The gmailr and httr packages, and the Google Books API, allowed me to create a catalog of the books we read with preview images. You can skip straight to the books, or read on for code that follows these main steps with their primary packages.\n\nBackground\nI live in Doylestown, PA (Bucks County), with my husband and two daughters (2 and 5 years old). On March 14, 2020, the Bucks County library system closed due to COVID-19; on June 22, 2020, the library opened with curbside pick-up service. Every one to two weeks, I began to place children’s books on hold (up to 15 at a time), and when a hold arrives, I receive an email notice like this:\n\nEmail notifications are sent around midnight, collating any books that arrived that day. As the email notifications trickled in and patterns emerged, I conceptualized a project to programmatically create a catalog of the library books we had placed on hold by extracting the books from my gmail and then augmenting their information from an API.\nGetting started\nThis material was developed using:\nSoftware / package\nVersion\nR\n4.0.3\nRStudio\n1.3.1073\ntidyverse\n1.3.0\ngmailr\n1.0.0\nhttr\n1.4.2\nlubridate\n1.7.8\ngt\n0.2.2\nglue\n1.4.1\nDT\n0.15\n\n\nlibrary(tidyverse) # general use ----\nlibrary(gmailr)    # retrieve emails  ----\nlibrary(httr)      # access api ----\nlibrary(lubridate) # repair dates ----\nlibrary(gt)        # for web_image helper function ----\nlibrary(glue)      # paste strings ----  \nlibrary(DT)        # interactive table ----  \n\n1 {gmailr} retrieve library notifications from gmail\nI followed the gmailr set up instructions to authenticate myself and enable R’s gmailr package to interact with the Gmail API for my personal Gmail account, which was fairly straightforward. This required some point and click in other interfaces, followed by two lines of R code.\n\n\ngm_auth_configure(path = \"path/credentials.json\")\ngm_auth()\n\nOnce authenticated, I retrieved the email notifications from my gmail account. It was not immediately clear to me how to do this, but this stack overflow thread helped.\n\n\n# 1 - retrieve messages corresponding to this search ----\nmessages <- gmailr::gm_messages(\"Bucks County Library System - Available Hold Pickup Notice\")\n\n# 2 - extract ids from messages corresponding to search ----\nmessage_ids <- gmailr::gm_id(messages)\n\n# 3 - extract contents from messages with these ids -----\nmessage_full <- purrr::map(message_ids, gmailr::gm_message)\n\n# 4 - extract message body from contents -----\nmessage_body <- purrr::map(message_full, gmailr::gm_body)\n\nThis resulted in 67 email notifications from June 22, 2020 to December 30, 3020, containing from 1 to 11 books per email. Here is an example character string returned to me, corresponding to the email notification shown above.\n\n\nmessage_body[[66]]\n\n[1] \"Wednesday, June 24, 2020\\r\\n\\r\\nBCFL Doylestown Branch\\r\\n150 South Pine Street\\r\\nDoylestown, PA\\r\\n18901-4932\\r\\n215-348-9081\\r\\n\\r\\n          Shannon M Pileggi\\r\\n          \\r\\n          Doylestown, PA\\r\\n          18901\\r\\n          \\r\\n\\r\\nDear Shannon Pileggi,\\r\\n                     \\\"AVAILABLE HOLD PICKUP NOTICE\\\"\\r\\n\\r\\nGood news! The material you requested is available for pickup from the library \\r\\nfor 2 weeks from the date of this email.\\r\\n\\r\\nPlease call your library to learn about curbside pickup or visit buckslib.org/cu\\r\\nrbside to see instructions for curbside pickup at your library.\\r\\n\\r\\nWe are accepting returns at our bookdrops.\\r\\n\\r\\nIf you no longer need this material, please cancel your hold by calling your \\r\\nlibrary or updating the information in the Review My Account section of My \\r\\nAccount: buckslib.org/MyAccount \\r\\n\\r\\nThank you.\\r\\n\\r\\n*********\\r\\nPlease do not respond to this email; it has been sent automatically from an \\r\\nunmonitored mailbox. For questions, contact the library listed above.\\r\\n\\r\\n  1   My trip to the science museum / by Mercer Mayer.\\r\\n      Mayer, Mercer, 1943-\\r\\n      call number:E FICTION MAYER                             copy:1    \\r\\n        Pickup by:7/8/2020  \\r\\n      hold pickup library:BCFL Doylestown Branch                                \\r\\n      \\r\\n \\r\\n\\r\\n  2   American girl. Bitty Baby has a tea party / by Kirby Larson & Sue\\r\\n      Cornelison.\\r\\n      Larson, Kirby\\r\\n      call number:E FICTION AMERICAN                          copy:3    \\r\\n        Pickup by:7/8/2020  \\r\\n      hold pickup library:BCFL Doylestown Branch                                \\r\\n      \\r\\n \\r\\n\\r\\n  3   The day the crayons quit / by Drew Daywalt ; pictures by Oliver Jeffers.\\r\\n      Daywalt, Drew.\\r\\n      call number:E FICTION DAYWALT                           copy:1    \\r\\n        Pickup by:7/8/2020  \\r\\n      hold pickup library:BCFL Doylestown Branch                                \\r\\n      \\r\\n \\r\\n\\r\\n\"\n\n\n2 {stringr} extract title and author from email\nI then did my best to convert the date of the email notification, the book title, and book author to a tibble. This was a three step iterative process.\nFirst, I created a list with the dates of the 67 emails.\n\n\n# 1 - extract date of email notification ----\nmessage_date <- purrr::map(\n  message_body, \n    # start string at first character, end where 2020 ends ----\n    ~ stringr::str_sub(.x , start = 1, end = str_locate(.x, \"2020\") %>% .[2]) %>% \n    # convert text string to date ----\n    lubridate::mdy(.)\n    )\n\n\n\n# view date corresponding to email notice ----\nmessage_date[[66]]\n\n[1] \"2020-06-24\"\n\nSecond, I did some initial cleaning to remove the introductory text, and then split the strings by numbers.\n\n\nmessage_books <- purrr::map(\n  message_body,  \n    # remove formatting text, affect other stringr code ----\n    ~ str_remove_all(.x, \"[\\\\r|\\\\n]\") %>% \n    # remove all introductory text ----\n    str_remove(\".* listed above.  \") %>%\n    # split string when any number is identified ----\n    str_split(\"[[:digit:]]   \")\n)\n\n\n\n# view entries corresponding to email notice ----\nmessage_books[[66]]\n\n[[1]]\n [1] \"\"                                                                                                                                                                        \n [2] \"My trip to the science museum / by Mercer Mayer.      Mayer, Mercer, 1943-      call number:E FICTION MAYER                             copy:\"                           \n [3] \"         Pickup by:7/8/202\"                                                                                                                                              \n [4] \"     hold pickup library:BCFL Doylestown Branch                                         \"                                                                                \n [5] \"American girl. Bitty Baby has a tea party / by Kirby Larson & Sue      Cornelison.      Larson, Kirby      call number:E FICTION AMERICAN                          copy:\"\n [6] \"         Pickup by:7/8/202\"                                                                                                                                              \n [7] \"     hold pickup library:BCFL Doylestown Branch                                         \"                                                                                \n [8] \"The day the crayons quit / by Drew Daywalt ; pictures by Oliver Jeffers.      Daywalt, Drew.      call number:E FICTION DAYWALT                           copy:\"         \n [9] \"         Pickup by:7/8/202\"                                                                                                                                              \n[10] \"     hold pickup library:BCFL Doylestown Branch                                       \"                                                                                  \n\nThird, I converted this to a tibble, removed all non-book rows, and iterated with stringr functions to get the title and author as best as I could. This was challenging as not all entries had consistent formatting, and some adjustments I could make here improved the Google Books API search later. Some of the stringr functions could probably be collapsed, but the code mimics the steps I took and also allowed me to easily backtrack on failed attempts.\n\n\n# split author and books into separate variables ----\nbooks <- message_books %>% \n  # coerce list to tibble ----\n  tibble() %>% \n  # rename first column ----\n  rename(\"text\" = 1) %>% \n  # unnest list text field ----\n  unnest(cols = \"text\") %>%\n  # merge in notice date ----\n  mutate(notice_date = message_date %>% unlist() %>% as_date()) %>% \n  # unnest one more time ----\n  unnest(cols = \"text\") %>% \n  # remove strings starting with spaces, these are not books ----\n  filter(!str_sub(text, 1, 1) %in% c(\" \", \"\")) %>% \n  # a lot of iteration here to extract author/title ----\n  # sort by notice date before assigning id ----\n  arrange(notice_date) %>% \n  mutate(\n    # create id ----\n    id = row_number(),\n    # clean title until looks ok ----\n    title = str_remove(text, \" /.*\") %>% \n      str_remove(\"Wells.*\") %>%\n      str_remove(\" \\\\[.*\") %>% \n      str_remove(\": from.*\") %>% \n      str_remove(\"\\\\*\") %>%\n      str_remove(\"Nino\") %>% # affected api search ----\n      str_remove(\"American [Gg]irl.\") %>% # affected api search ----\n      str_trim(),\n    # clean author until looks ok ----\n    author = str_remove(text, \".* /\") %>% \n      str_remove(\"Sendak, Maurice\") %>% \n      str_remove(\" ; .*\") %>% \n      str_remove(\"\\\\[\") %>% \n      str_remove(\"\\\\]\") %>% \n      str_remove(\".* by \") %>%\n      str_remove(\".*! \") %>% \n      str_remove(\"and .*\") %>%\n      # remove everything after a period, but affected authors with first or middle initial ----\n      str_remove(\"\\\\..*\") %>% \n      str_remove(\"\\\\*\") %>% \n      str_remove_all(\"& Sue .*| an |book\") %>% \n      str_squish()\n  ) %>% \n  # remove non pre-k books (I mostly read on kindle) ----\n  dplyr::filter(!(title %in% c(\"Pax\", \"Luster\", \"Last night in Montreal : a novel\"))) %>% \n  # count number of books per emails ----\n  group_by(notice_date) %>% \n  add_count() %>% \n  ungroup()\n\n\n\n# view entries corresponding to email notice ----\nbooks %>% \n  filter(notice_date == \"2020-06-24\") %>% \n  dplyr::select(text, title, author) %>% \n  knitr::kable()\ntext\ntitle\nauthor\nMy trip to the science museum / by Mercer Mayer. Mayer, Mercer, 1943- call number:E FICTION MAYER copy:\nMy trip to the science museum\nMercer Mayer\nAmerican girl. Bitty Baby has a tea party / by Kirby Larson & Sue Cornelison. Larson, Kirby call number:E FICTION AMERICAN copy:\nBitty Baby has a tea party\nKirby Larson\nThe day the crayons quit / by Drew Daywalt ; pictures by Oliver Jeffers. Daywalt, Drew. call number:E FICTION DAYWALT copy:\nThe day the crayons quit\nDrew Daywalt\n\n3 {httr} query google books api\nI then got started with the Google Books API. Some key steps in this process included:\nExploring Google Books and doing some manual searches to get a feel for how it behaves and what it returns.\nIdentifying the format to get started with the API: https://www.googleapis.com/books/v1/{collectionName}/resourceID?parameters.\nUnderstanding the search terms permissible in the query (like inauthor and intitle): https://www.googleapis.com/books/v1/volumes?q=search+terms.\nObtaining an API key (google console developer -> enable google books api -> create credentials -> calling API from other non UI -> get API key). However, this ended up being unnecessary for the search I executed.\nThen I used the httr package to actually perform the query. First, I defined the url.\n\n\n# url base format ----\nurl <- \"https://www.googleapis.com/books/v1/volumes\"\n\nNext I created a wrapper function around httr::GET to allow for a search by title and author, or title alone.\n\n\nget_book <- function(this_title, this_author = NA){\n  httr::GET(\n    url = url,\n    query = list(\n      key = token, \n      q = ifelse(\n        is.na(this_author),\n        glue::glue('intitle:{this_title}'),\n        glue::glue('intitle:{this_title}+inauthor:{this_author}')\n        )))\n}\n\nI then performed an initial search with title and author. I learned that the API would error out with too many queries in quick succession, and I included Sys.sleep to slow down the search.\n\n\n# first search with title and author ----\n# create progress bar for mapping ----\np <- dplyr::progress_estimated(nrow(books))\nbooks_1 <- books %>%\n  mutate(book_info = purrr::map2(title, author,\n                                 function(x, y) {\n                                   # print progress bar ----\n                                   p$tick()$print()\n                                   # add in delay for api ----\n                                   Sys.sleep(1)\n                                   get_book(x, y)\n                                 }))\n\nFrom there, I checked the status code, extracted book content, and checked to see if any items were empty (which meant that no result was returned). In addition, more than one search result could have been returned; only the first was extracted.\n\n\nbooks_1_content <- books_1 %>%\n  mutate(\n    # extract status code: 200 = success ----\n    status_code = purrr::map_int(book_info, \"status_code\"),\n    # extract relevant content from author title search ----\n    book_items = purrr::map(book_info, ~ httr::content(.x)[[\"items\"]][[1]][[\"volumeInfo\"]]),\n    # check to see if items are empty ----\n    empty_items = purrr::map_lgl(book_items, rlang::is_empty)\n  )\n\nThe initial title and author search were all technically successful, though 10 searches results in empty items.\n\n\nbooks_1_content %>% count(status_code, empty_items)\n\n# A tibble: 2 x 3\n  status_code empty_items     n\n        <int> <lgl>       <int>\n1         200 FALSE         204\n2         200 TRUE           10\n\nFor these 10 items, I manually checked some with various searches on Google Books, and I noticed that some actually did have an entry, but did not have an author in that entry, so I repeated the search but with title only.\n\n\n# 10 books did not have a result with author title search ----\np <- dplyr::progress_estimated(nrow(books))\n# perform title search only ----\nbooks_2 <- books_1_content %>%\n  # keep only empty items for another search ----\n  filter(empty_items == TRUE) %>%\n  # keep original first six columns ----\n  select(1:6) %>%\n  mutate(book_info = purrr::map(title, \n                                function(x) {\n                                  p$tick()$print()\n                                  Sys.sleep(1)\n                                  get_book(x)\n                                }))\n\n\n\nbooks_2_content <- books_2 %>% \n  mutate(\n    # extract status code: 200 = success ----\n    status_code = purrr::map_int(book_info, \"status_code\"),\n    # extract relevant content from author only search ----  \n    book_items = purrr::map(book_info, ~ httr::content(.x)[[\"items\"]][[1]][[\"volumeInfo\"]]\n  ),\n  # check to see if items are empty ----  \n  empty_items = purrr::map_lgl(book_items, rlang::is_empty)\n)\n\nThese 10 items were all technically successful and contained result contents.\n\n\nbooks_2_content %>% count(status_code, empty_items)\n\n# A tibble: 1 x 3\n  status_code empty_items     n\n        <int> <lgl>       <int>\n1         200 FALSE          10\n\nIn these two searches, I created a nested tibble with two nested lists:\nbook_info is a very long list which contains the full result from the original query.\n\n\n# showing entry for a book in initial email notification ----\nbooks_1_content[[\"book_info\"]][[2]]\n\nResponse [https://www.googleapis.com/books/v1/volumes?key=AIzaSyCUqoG382nRpWFTT-3BKX-m_cB7UyfVTMw&q=intitle%3AMy%20trip%20to%20the%20science%20museum%2Binauthor%3AMercer%20Mayer]\n  Date: 2021-01-03 16:42\n  Status: 200\n  Content-Type: application/json; charset=UTF-8\n  Size: 6.14 kB\n{\n  \"kind\": \"books#volumes\",\n  \"totalItems\": 2,\n  \"items\": [\n    {\n      \"kind\": \"books#volume\",\n      \"id\": \"htdVvgAACAAJ\",\n      \"etag\": \"zxSYXCOb+ho\",\n      \"selfLink\": \"https://www.googleapis.com/books/v1/volumes/htdV...\n      \"volumeInfo\": {\n...\n\nbook_items contains into the specific bits of information I considered for extraction to augment book information.\n\n\n# showing entry for a book in initial email notification ----\nstr(books_1_content[[\"book_items\"]][[2]])\n\nList of 19\n $ title              : chr \"My Trip to the Science Museum\"\n $ authors            :List of 1\n  ..$ : chr \"Mercer Mayer\"\n $ publisher          : chr \"Little Critter\"\n $ publishedDate      : chr \"2017-03-07\"\n $ description        : chr \"For use in schools and libraries only. It's Science Day and Little Critter and his classmates are heading to th\"| __truncated__\n $ industryIdentifiers:List of 2\n  ..$ :List of 2\n  .. ..$ type      : chr \"ISBN_10\"\n  .. ..$ identifier: chr \"060639625X\"\n  ..$ :List of 2\n  .. ..$ type      : chr \"ISBN_13\"\n  .. ..$ identifier: chr \"9780606396257\"\n $ readingModes       :List of 2\n  ..$ text : logi FALSE\n  ..$ image: logi FALSE\n $ pageCount          : int 24\n $ printType          : chr \"BOOK\"\n $ categories         :List of 1\n  ..$ : chr \"Juvenile Fiction\"\n $ maturityRating     : chr \"NOT_MATURE\"\n $ allowAnonLogging   : logi FALSE\n $ contentVersion     : chr \"preview-1.0.0\"\n $ panelizationSummary:List of 2\n  ..$ containsEpubBubbles : logi FALSE\n  ..$ containsImageBubbles: logi FALSE\n $ imageLinks         :List of 2\n  ..$ smallThumbnail: chr \"http://books.google.com/books/content?id=htdVvgAACAAJ&printsec=frontcover&img=1&zoom=5&source=gbs_api\"\n  ..$ thumbnail     : chr \"http://books.google.com/books/content?id=htdVvgAACAAJ&printsec=frontcover&img=1&zoom=1&source=gbs_api\"\n $ language           : chr \"en\"\n $ previewLink        : chr \"http://books.google.com/books?id=htdVvgAACAAJ&dq=intitle:My+trip+to+the+science+museum%2Binauthor:Mercer+Mayer&\"| __truncated__\n $ infoLink           : chr \"http://books.google.com/books?id=htdVvgAACAAJ&dq=intitle:My+trip+to+the+science+museum%2Binauthor:Mercer+Mayer&\"| __truncated__\n $ canonicalVolumeLink: chr \"https://books.google.com/books/about/My_Trip_to_the_Science_Museum.html?hl=&id=htdVvgAACAAJ\"\n\n4 {purrrr} extract content from search results\nTo assemble the final data, I combined the results of the two searches and proceeded to extract information from book_info. A tricky part here was that not all API results returned the same information, and if the named item did not exist the entire mapping would fail. My solution was to check if the named element existed for extraction, and otherwise return NA. I also checked to see if there was a fuzzy match between the author I originally extracted from the emails versus the author that the Google Books API returned.\n\n\nbooks_all <- books_1_content %>%\n  # keep non-empty items from title author search ----\n  filter(empty_items == FALSE) %>%\n  # append results from title only search ---\n  bind_rows(books_2_content) %>%\n  # remove original api results ----\n  dplyr::select(-book_info) %>%\n  mutate(\n    # extract fields from api results ----\n    book_title = purrr::map_chr(book_items, ~ ifelse(\n      rlang::has_name(.x, \"title\"), .x[[\"title\"]], NA\n    )),\n    book_author = purrr::map_chr(book_items, ~ ifelse(\n      rlang::has_name(.x, \"authors\"), str_c(unlist(.x[[\"authors\"]]), collapse = \", \"),\n      NA\n    )),\n    book_publish_date = purrr::map_chr(book_items, ~ ifelse(\n      rlang::has_name(.x, \"publishedDate\"), .x[[\"publishedDate\"]], NA\n    )),\n    book_volume_link = purrr::map_chr(book_items, ~ ifelse(\n      rlang::has_name(.x, \"canonicalVolumeLink\"), .x[[\"canonicalVolumeLink\"]], NA\n    )),\n    book_image_link = purrr::map_chr(book_items, ~ ifelse(\n      rlang::has_name(.x, \"imageLinks\"), .x[[\"imageLinks\"]][[\"smallThumbnail\"]], NA\n    )),\n    # clean book publish date to year only ----\n    book_publish_year = str_sub(book_publish_date, start = 1, end = 4) %>% as.numeric(),\n    # check for fuzzy matching of author from email vs api result ----\n    author_fuzzy = purrr::map2(author, book_author, agrep),\n    # if result is 1, there is a fuzzy match; otherwise, not ----\n    author_match = purrr::map_lgl(author_fuzzy, ~ length(.x) > 0)\n  )\n\nIf the authors matched, I assumed a matching result was returned. For the 13 authors that did not match, I manually inspected entries for further cleaning.\n\n\nbooks_all %>% \n  filter(author_match == FALSE) %>% \n  # title, author from email; book_title, book_author from api ---\n  select(id, title, author, book_title, book_author) %>% \n  knitr::kable()\nid\ntitle\nauthor\nbook_title\nbook_author\n58\nSnow sisters!\nKerri Kokias, Teagan White\nSnow Sisters!\nKerri Kokias\n75\nSquare\nMac Barnett & Jon Klassen\nSquare\nMac Barnett\n109\nSpend it! : a moneybunny book\nCinders McLeod\nSpend It!\nNA\n179\nBack to school with the Berenstain Bears\nStan & Jan Berenstain\nBack to School with the Berenstain Bears\nStan Berenstain, Jan Berenstain\n15\nSplat the Cat and the quick chicks\nLaura Driscoll\nSplat the Cat and the Quick Chicks\nRob Scotton\n28\nRaindrop, plop!\nWendy Cheyette Lewison\nPlop the Raindrop\nKevin Alan Richards\n54\nDrawn together\nMinh Le\nDrawn Together\nLeah Pearlman\n89\nAll the world\nMarla Frazee\nAll the World Loves a Quarrel\nDaniel Wright Kittredge\n139\nChee-Kee : a panda in Bearland\nSujean Rim\nChee-Kee: A Panda in Bearland\nNA\n140\nBirdie’s happiest Halloween\nSujean Rim\nBirdie’s Happiest Halloween\nNA\n159\nCostume fun!\nWells, Rosemary\nCreative Homemade Halloween Costume Ideas - Fun, Unusual and Inexpensive\nM Osterhoudt\n203\nCorduroy’s Christmas surprise\nLisa McCue\nCorduroy’s Christmas Surprise\nDon Freeman\n211\nRudolph the Red-Nosed Reindeer\nAlan Benjamin\nRudolph the Red-Nosed Reindeer\nRobert L. May\n\nOf these 13 books, I determined that there were:\n3 instances where Google Books had missing author but the search result was a correct match\n5 instances where the authors did not fuzzy match, but the search result was a correct match\n5 instances with an incorrect match\nI finished cleaning the books data with this in mind, and also formatted fields to show the thumbnail images and convert titles to links.\n\n\nbooks_clean <- books_all %>%\n  #filter(author_match == TRUE) %>% \n  mutate(\n    # replace author as extracted from email ----\n    book_author = ifelse(is.na(book_author), author, book_author),\n    # replace author as extracted from email ----\n    book_author = case_when(\n       # correct result not available from api search ----\n       title %in% c(\"Raindrop, plop!\", \"Drawn together\", \"All the world\", \"Costume fun!\", \"Rudolph the Red-Nosed Reindeer\") ~ author,\n       TRUE ~ book_author\n     ),\n    # replace title as extracted from email ----\n     book_title = case_when(\n       # correct result not available on google books ----\n       title %in% c(\"Raindrop, plop!\", \"Drawn together\", \"All the world\", \"Costume fun!\", \"Rudolph the Red-Nosed Reindeer\") ~ title,\n       TRUE ~ book_title\n     ),\n    # make volume link missing ----\n     book_volume_link = case_when(\n       # correct result not available on google books ----\n        title %in% c(\"Raindrop, plop!\", \"Drawn together\", \"All the world\", \"Costume fun!\", \"Rudolph the Red-Nosed Reindeer\") ~ NA_character_,\n       TRUE ~ book_volume_link\n     ),\n     # make image link missing ----\n     book_image_link = case_when(\n       # correct result not available on google books ----\n      title %in% c(\"Raindrop, plop!\", \"Drawn together\", \"All the world\", \"Costume fun!\", \"Rudolph the Red-Nosed Reindeer\") ~ NA_character_,\n       TRUE ~ book_image_link\n     ),\n     # make year missing ----\n     book_publish_year = case_when(\n       title %in% c(\"Raindrop, plop!\", \"Drawn together\", \"All the world\", \"Costume fun!\", \"Rudolph the Red-Nosed Reindeer\") ~ NA_real_,\n       TRUE ~ book_publish_year\n     ),\n     # create html text strings for display in table ----\n     image_link_html = gt::web_image(url = book_image_link, height = 80),\n     title_link_html = ifelse(\n       is.na(book_volume_link),\n       # if link unavailable, print title with no link ----\n       book_title,\n       str_c(\"<a href='\", book_volume_link, \"' target='_blank'>\", book_title, \"<\/a>\")\n     )\n  ) %>% \n  arrange(id) %>% \n  # rename for table ---\n  dplyr::select(\n    ID = id,\n    Preview = image_link_html, \n    Title = title_link_html, \n    `Author(s)` = book_author, \n    `Published` = book_publish_year, \n    `Notification` = notice_date\n  ) \n\n5 {DT} present books in an interactive table\nFinally! Over the last 6 months, we read 214 amazing books. If you aren’t familiar with this type of table (I’m looking at you, Mom), note that this table is interactive. You can search for any word like “Carle” or “bear” in the search bar, change the number of entries shown, and sort by published date (for example) by using the light gray triangles in the header of the table.\n\n\nbooks_clean %>%\n  DT::datatable(\n    escape = FALSE,\n    rownames = FALSE,\n    options = list(\n      columnDefs = list(\n        list(className = 'dt-center', targets = c(1, 4, 5)),\n        list(className = 'dt-left', targets = c(2:3))\n      ),\n      pageLength = 15,\n      lengthMenu = seq(15, 225, by = 15)\n    )\n  )\n\n\nTechnical reflection\nThis project was certainly a learning experience! I used the gmailr and httr packages for the first time, in addition to accessing my first API. Here are some of the challenges I faced:\nMy emails from the library did not always have consistent punctuation in reporting available books.\nBook titles are not consistently presented; e.g., Up up up down in my email from the library vs Up, Up, Up, Down! in the Google Books result.\nAuthor names can be formatted differently across books (books with multiple authors or authors who identify with a middle initial, and period, in their name).\nAuthorship is not always consistently defined. For example, I Can Read! Splat the Cat and the Quick Chicks was attributed to Laura Driscoll according to the library (who actually wrote the text), but Rob Scotton according to Google Books (who wrote the original Splat the Cat books that this was based on).\nThe API results varied in information presented; for example, not all results contained authors or preview images.\nPerforming the API search in two phases (author and title vs title only) gave eight additional correct results and five additional incorrect results. I’m not sure if that is worth it considering the manual effort to identify the incorrect results.\nThe books shown in my search are not necessarily the exact books that I read. For example, I did not read the 2021 edition of “What the Ladybird Heard” as shown in my final table, which would not have been available when I read it in October 2020.\nI encountered more issues with NULL, integer(0), and empty items than ever before (which is not the same as NA 🤣).\nAlternative solutions to these challenges are welcome! You can download books.rda (which contains message_body, books_1, and books_2) from my github repo.\nPersonal note\nLike many aspects of life, COVID-19 changed our relationship with the library. Previously, our family library experience consisted of leisurely browsing for a couple of books followed by a quick exit when the girls inevitably started playing hide and seek in the aisles. Instead, I now actively research children’s books to check out. Without this change, I may have not have encountered many books that we now treasure. Completion of this cataloging project with preview images allows us to better recall the books we read together, and decide together which books we want to read again in the future. I hope that nurturing a love for books and reading can help to alleviate the lapse in formal pre-k education my children experienced this year.\nAcknowledgements\nAnthony Pileggi (my husband) talked me through my first API and discussed possible solutions to some coding challenges. Anthony is a data scientist currently specializing in e-commerce for outdoor power equipment at Paul B. Moyer and Sons, and sometimes he makes fun R packages like this.\n\n\n",
    "preview": "posts/2021-01-11-from-gmailr-to-the-google-books-api/preview_books.png",
    "last_modified": "2022-08-11T11:39:27-04:00",
    "input_file": {},
    "preview_width": 705,
    "preview_height": 459
  },
  {
    "path": "posts/2020-12-23-leveraging-labelled-data-in-r/",
    "title": "Leveraging labelled data in R",
    "description": "Embracing SPSS, SAS, and Stata data sets with the haven, labelled, and sjlabelled packages",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2020-12-23",
    "categories": [],
    "contents": "\n\nContents\nLast updated\nTL; DR\nIntroduction\nYRBSS labelled data\nGetting started\nImporting labelled data\nCreating a data dictionary\nIdentifying labelled features\nViewing labelled features\nCommon operations\nExample\nOther packages and haven_labelled objects\nWorkflow for labelled data manipulation\nSummary\nAcknowledgments\n\n\n\n\nFigure 1: Artwork adapted from @allison_horst.\n\n\n\nLast updated\nThis post was last updated on 2021-09-14 to reflect package updates.\nTL; DR\nThe haven, labelled, and sjlabelled packages can be used to effectively work with SPSS, SAS, and Stata data sets in R through implementation of the haven_labelled class, which stores variable and value labels. Here are my most used functions for getting started with labelled data:\nPurpose\nFunction\n1. Import SPSS labelled data\nhaven::read_sav()\n2. Create data dictionary\nlabelled::generate_dictionary()\n3. Identify if variable is haven_labelled\nhaven::is.labelled()\n4. Convert haven_labelled variables to numeric\nbase::as.numeric()\n5. Convert haven_labelled variables to factors\nhaven::as_factor()\n6. Convert variable label to variable name\nsjlabelled::label_to_colnames()\nIntroduction\nLabelled data traditionally, though not exclusively, arises in survey data. SAS, SPSS, and Stata have established infrastructures for labelled data, which consists of metadata in the form of variable and value labels. This post is for R users who already have a SPSS (.sav), SAS (.sas7bdat), or Stata (.dta) data file and want to incorporate the labelled data features into their R workflow. With R’s haven, labelled, and sjlabelled packages, you can leverage the inherent data labelling structure in these data sets to work interactively with variable and value labels, making it easier to navigate data while also allowing the user to convert metadata to data. This post discusses general characteristics of labelled data and practical tips for data analysis with labelled data.\n\nI work exclusively with SPSS (.sav) data files, and so I have not evaluated this process on SAS (.sas7bdat) or Stata (.dta) data files. 🤞 everything works the same!\nYRBSS labelled data\nThe Youth Risk Behavior Surveillance System (YRBSS) is a publicly available data set from the Centers for Disease Control and Prevention (CDC) that “monitors health-related behaviors that contribute to the leading causes of death and disability among youth and adults.” On Aug 9, 2020, I downloaded YRBSS materials from the CDC website. This site has both the the 2017 national data (sadc_2017_national.dat) and the SPSS syntax to convert the .dat file to an SPSS labelled data file (2017_sadc_spss_input_program.sps). I do have an SPSS license, and I used the SPSS syntax to convert the .dat file to an SPSS labelled data file (sadc_2017_national.sav). As the .sav data file is not available on the CDC site, you can download the .sav data from my github repo.\nGetting started\nThis material was developed using:\nSoftware / package\nVersion\nR\n4.0.5\nRStudio\n1.4.1103\ntidyverse\n1.3.1\nhere\n0.1\nhaven\n2.3.1\nlabelled\n2.8.0\nsjlabelled\n1.1.7\n\n\nlibrary(tidyverse)  # general use ----\nlibrary(here)       # file paths  ----\nlibrary(haven)      # import .sav files ----  \nlibrary(labelled)   # tools for labelled data ----\nlibrary(sjlabelled) # more tools for labelled data ----\n\n\n\nImporting labelled data\nI use the haven package to import SPSS (.sav) data files.\n\n\n# import data ----\ndat_raw <- haven::read_sav(here::here( \"_posts\", \"2020-12-23-leveraging-labelled-data-in-r\", \"data\", \"sadc_2017_national.sav\"))\n\n\n\nVariables in a data set have a class, which consists of assignments like numeric, character, and factor, among others. When labelled features are present, the haven package assigns a class of haven_labelled. This is important to know as many packages you work with may not have methods for haven_labelled objects.\nWhen I first started working with SPSS data files, I also explored the foreign package, which preceeds haven. Using foreign takes a bit longer than haven, can result in truncation of long character variables, and produces a different labelled data structure compared to haven. I have a strong preference for the haven package.\nCreating a data dictionary\nA data dictionary contains metadata about your data. The labelled::generate_dictionary function can be used to create a data dictionary, extracted straight from your data. The usefulness of the data dictionary depends on the quality of your metadata.\n\n\n# create data dictionary ----\ndictionary <- labelled::generate_dictionary(dat_raw)\n\n\n\nThe result is a data frame in my R environment with the number of observations equal to number of variables in the original data set. I can interactively explore the dictionary in R to quickly find variables or documentation of interest. For example, I can find all variables related to “weapons” with a search.\n\n\n\nFigure 2: Gif demonstrating search feature in data viewer utilized to find variables with the word weapon in the data dictionary.\n\n\n\nIdentifying labelled features\nStandard data consists of variables (e.g., country) and values (e.g. US, UK, CA). When working with labelled data, variables and values each have two features. Variables consist of a name and a label; values consist of a code and a label. For example, here are the features of the q8 variable.\nFeature\nAssignment\nVariable name\nq8\nVariable label\nSeat belt use\nValue codes\n1, 2, 3, 4, 5\nValue labels\nNever, Rarely, Sometimes, Most of the time, Always\nYou can see this information in the data dictionary - here is a snippet of the dictionary for three variables. The value_labels field combines the value codes and value labels.\n\n\ndictionary %>% \n  dplyr::filter(variable %in% c(\"q8\", \"q11\", \"q12\")) %>% \n  dplyr::select(variable, label, value_labels) \n\n\n variable label              \n q8       Seat belt use      \n q11      Texting and driving\n q12      Weapon carrying    \n value_labels                                                                                                                           \n [1] Never; [2] Rarely; [3] Sometimes; [4] Most of the time; [5] Always                                                                 \n [1] Did not drive; [2] 0 days; [3] 1 or 2 days; [4] 3 to 5 days; [5] 6 to 9 days; [6] 10 to 19 days; [7] 20 to 29 days; [8] All 30 days\n [1] 0 days; [2] 1 day; [3] 2 or 3 days; [4] 4 or 5 days; [5] 6 or more days                                                            \n\nTo dive a bit deeper, you can see the class of the q8 variable:\n\n\ndat_raw %>% \n  dplyr::pull(q8) %>% \n  class(.)\n\n\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"double\"        \n\nand how the metadata of q8 is stored.\n\n\ndat_raw %>% \n  dplyr::select(q8) %>% \n  str(.)\n\n\ntibble [203,663 x 1] (S3: tbl_df/tbl/data.frame)\n $ q8: dbl+lbl [1:203663]  2, NA,  4,  4,  1,  1,  3,  5,  4,  4,  ...\n   ..@ label        : chr \"Seat belt use\"\n   ..@ format.spss  : chr \"F1.0\"\n   ..@ display_width: int 4\n   ..@ labels       : Named num [1:5] 1 2 3 4 5\n   .. ..- attr(*, \"names\")= chr [1:5] \"Never\" \"Rarely\" \"Sometimes\" \"Most of the time\" ...\n\nYou don’t need to get into the weeds of this to work effectively with labelled data, but knowing this can help troubleshoot errors.\nViewing labelled features\nBeyond the dictionary, labelled features can also be seen when working with your data interactively. The console simultaneously prints value codes and labels side by side, with the code first followed by the label in brackets.\n\n\ndat_raw %>% \n  dplyr::select(q8, q11, q12) \n\n\n# A tibble: 203,663 x 3\n                      q8       q11                 q12\n               <dbl+lbl> <dbl+lbl>           <dbl+lbl>\n 1  2 [Rarely]                  NA  4 [4 or 5 days]   \n 2 NA                           NA NA                 \n 3  4 [Most of the time]        NA  3 [2 or 3 days]   \n 4  4 [Most of the time]        NA  5 [6 or more days]\n 5  1 [Never]                   NA  5 [6 or more days]\n 6  1 [Never]                   NA  1 [0 days]        \n 7  3 [Sometimes]               NA  2 [1 day]         \n 8  5 [Always]                  NA  1 [0 days]        \n 9  4 [Most of the time]        NA  5 [6 or more days]\n10  4 [Most of the time]        NA NA                 \n# ... with 203,653 more rows\n\nSometimes the alignment throws me a bit when I am reading this as the value codes and labels are left aligned, which places the value codes associated with q12 closer to q11.\nWhen viewing the data frame in RStudio, the data frame displays the variable label under the variable name; however, only value codes (and not value labels) are displayed.\n\n\n\nFigure 3: Screenshot showing how haven labelled data appear in the viewer pane, with variable labels under the variable name, and value codes (not value labels) displayed.\n\n\n\nCommon operations\nI primarily use three packages for working with labelled data: haven, labelled, and sjlabelled. These three packages do have some overlap in functionality, in addition to naming schemes that differ but achieve the same objective (e.g., haven::as_factor vs sjlabelled::as_label), or naming schemes that are the same but achieve different objectives (e.g., haven::as_factor vs sjlabelled::as_factor). 😬 To compound confusion, the concept of a label can refer to either variable or value labels. Frequently, plural function names refer to value labels, as in haven::zap_labels or labelled::remove_val_labels.\nHere are operations I commonly perform on labelled data:\nEvaluate if variable is of class haven_labelled.\nWhy? Troubleshooting, exploring, mutating.\nFunction(s): haven::is.labelled()\n\nConvert haven_labelled variable to numeric value codes.\nWhy? To treat the variable as continuous for analysis. For example, if a 1-7 rating scale imports as labelled and you want to compute a mean.\nFunction(s): base::as.numeric() (strips variable of all metadata), haven::zap_labels() and labelled::remove_val_labels (removes value labels, retains other metadata)\n\nConvert haven_labelled() variable to factor with value labels.\nWhy? To treat the variable as categorical for analysis.\nFunction(s): haven::as_factor(), labelled::to_factor(), sjlabelled::as_label(). As far as I can tell, these three functions have the same result. By default, the factor levels are ordered by value codes.\n\nConvert variable label to variable name.\nWhy? For more informative or readable variable names.\nFunction(s): sjlabelled::label_to_colnames()\n\nExample\nFor this example, I reduce the data set to 2017 records only and select three variables related to carrying weapons and safety, all of which are measured on the same scale.\n\n\n# retain info on weapons and safety for 2017 ----\ndat_2017 <- dat_raw %>% \n  dplyr::filter(year == 2017) %>% \n  dplyr::select(record, q12, q13, q15) \n\n\n\n\n\n# preview data ----\ndat_2017\n\n\n# A tibble: 14,765 x 4\n    record                q12        q13                q15\n     <dbl>          <dbl+lbl>  <dbl+lbl>          <dbl+lbl>\n 1 1509749 5 [6 or more days] 1 [0 days] 1 [0 days]        \n 2 1509750 1 [0 days]         1 [0 days] 1 [0 days]        \n 3 1509751 1 [0 days]         1 [0 days] 3 [2 or 3 days]   \n 4 1509752 1 [0 days]         1 [0 days] 1 [0 days]        \n 5 1509753 1 [0 days]         1 [0 days] 1 [0 days]        \n 6 1509754 1 [0 days]         1 [0 days] 2 [1 day]         \n 7 1509755 5 [6 or more days] 1 [0 days] 5 [6 or more days]\n 8 1509756 1 [0 days]         1 [0 days] 1 [0 days]        \n 9 1509757 1 [0 days]         1 [0 days] 1 [0 days]        \n10 1509758 1 [0 days]         1 [0 days] 1 [0 days]        \n# ... with 14,755 more rows\n\nThis code produces a bar plot showing the frequencies of the three variables from data as imported, displaying variable names and value codes.\n\n\n# bar plot 1 ----\ndat_2017 %>% \n   pivot_longer(\n    cols = -1,\n    names_to = \"variable\",\n    values_to = \"days\"\n  ) %>% \n  count(variable, days) %>% \n  # include factor(days) to correctly show value codes in ggplot ----\n  ggplot(aes(x = n, y = factor(days))) +\n  facet_wrap(. ~ variable) +\n  geom_col() \n\n\n\n\nFigure 4: Bar plot 1 displays variable names and value codes\n\n\n\nNow I add two lines of code to implement two changes - convert the variables to factors and convert the variable labels to variable names. This plot displays variable labels and value labels, producing a more informative figure.\n\n\n# bar plot 2 ----\ndat_2017 %>% \n  # --------------------------------------------------------------\n  # change 1: convert haven_labelled variables to factors ----\n  mutate_if(haven::is.labelled, haven::as_factor) %>% \n  # change 2: convert variable labels to variable names ----\n  sjlabelled::label_to_colnames() %>% \n  # --------------------------------------------------------------\n  pivot_longer(\n    cols = -1,\n    names_to = \"variable\",\n    values_to = \"days\"\n  ) %>% \n  count(variable, days) %>% \n  # unnecessary to include factor(days) here as was already converted in change 1 ----\n  ggplot(aes(x = n, y = days)) +\n  facet_wrap(. ~ variable) +\n  geom_col() \n\n\n\n\nFigure 5: Bar plot 2 displays variable labels and value labels\n\n\n\nOther packages and haven_labelled objects\nIt is probably safe to assume that most packages you work with don’t know how to handle the haven_labelled class - if the package does produce a result, it is likely making an educated guess which may not be in line with your needs.\nFor example, in using ggplot for Figure 4 above, I included the line y = factor(days); if instead I had y = days in Figure 4, ggplot yields the following message:\n\nDon’t know how to automatically pick scale for object of type haven_labelled/vctrs_vctr/double. Defaulting to continuous.\n\nTreating the days variable as continuous resulted in an uninformative plot (not shown), which was corrected by converting the variable to factor.\nWhat about other packages I use? In skimr 2.1.3 haven_labelled inputs result in value codes treated as numeric values. In gtsummary ≥1.4.0 the value labels of haven_labelled variables are ignored and the underlying values are shown; however, a helpful message is printed with instructions to convert or remove the value labels. In general, you will probably find a mix of messages, warnings, errors, omissions, or guessing when using haven_labelled variables with other packages. These issues can be resolved by converting the haven_labelled variables to numeric or factor, depending on the context.\n\nNote: Although gtsummary does not currently support value labels, it does support variable labels! See this tweet for a quick demo, and the Polished summary tables in R with gtsummary blog post for more information.\nWorkflow for labelled data manipulation\nWhen converting haven_labelled objects to factor or numeric, be intentional about where the conversion happens in your workflow. The Introduction to labelled vignette by Joseph Larmarange outlines two different approaches:\nFirst convert haven_labelled variables; second perform data manipulation utilizing variable labels (if factor).\nFirst perform data manipulation utilizing variable codes; second convert haven_labelled variables.\nFor me this question usually distills down to: for data manipulation, are the value codes or the value labels easier to work with? Sometimes the brevity of the value code helps (i.e., q12 == 1), whereas other times the context of the value label makes the code more readable (i.e., q12 == \"0 days\"). Note that the placement of the conversion can have downstream effects on your code.\nSummary\nThe haven, labelled, sjlabelled packages create new structures and work flows for labelled data that allow you to harness the power of R while still honoring the valuable metadata framework that exists in SPSS, SAS, and Stata data sets. The functions discussed in this post cover most of my daily needs with labelled data; if you want to do more, next steps might include handling specific types of coded missing data or creating labelled data within R.\nAcknowledgments\nThanks to Daniel Sjoberg for the gentle nudge to update this post. 🤗\n\n\n\n",
    "preview": "posts/2020-12-23-leveraging-labelled-data-in-r/labelled_preview.png",
    "last_modified": "2022-08-11T11:39:27-04:00",
    "input_file": {},
    "preview_width": 671,
    "preview_height": 617
  },
  {
    "path": "posts/2020-10-25-your-first-r-package-in-1-hour/",
    "title": "Your first R package in 1 hour",
    "description": "Tools that make R package development easy",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2020-10-25",
    "categories": [],
    "contents": "\nTable of Contents\nTL; DR\nAbstract\nDisclaimer\nGetting started\nTool kit\nFirst package\nFirst check\nFirst function\nCreate\nExecute\nDocument\nCheck\nDependencies\n\nInstalling your package\nSummary\nAcknowledgements\nTL; DR\nThis blog post accompanies the R-Ladies Philly workshop on November 12, 2020. We demonstrate how leveraging the devtools and usethis packages in package development can automate folder structure and file creation, speeding up the development process.\n\nThis post was updated on Nov 21, 2020 with the YouTube workshop recording. Some Q&A from the workshop are here.\nAbstract\nThis workshop is for people looking to learn how to make their own R packages and learn how to use usethis and devtools for package development. The workshop will cover handy one time functions (i.e., usethis::create_package) as well as functions used continuously throughout package development (i.e., devtools::document). At the end of the hour you should have a working, well-documented package with a single function, as well as a better understanding of the files and file structure required for R packages.\nThis workshop is suitable for beginner to intermediate R users. Attendees should be familiar with functions, but will not be writing their own function in this workshop. Familiarity with pipe or tidyverse is helpful.\nDisclaimer\n\n\nArtwork adapted from @allison_horst.\nI have learned enough about package development to share with you what has worked well for me; however, I am still learning! Comments and suggestions are welcome.\nGetting started\nThis material was developed using:\nSoftware / package\nVersion\nR\n4.0.3\nRStudio\n1.3.1073\ndevtools\n2.3.2\nusethis\n1.6.3\ntidyverse\n1.3.0\nrlang\n0.4.8\nbroom\n0.7.1\nPlease install and update all software / packages prior to following along, as otherwise errors may arise.\nTool kit\nSingle usage functions only need to be used one time in the development process; multiple usage functions are executed as needed. This table only contains functions used in this workshop; there are many other handy functions in devtools and usethis for package development.\nUsage\nFunction\nPurpose\nSingle\nusethis::create_package(\"path/package\")\ninitialize package\n\nusethis::use_mit_license(\"Your name\")\nadd license\n\nusethis::use_pipe()\nadd pipe function as a dependency\nMultiple\ndevtools::check()\nbuild package locally and check\n\ndevtools::load_all()\nload functions in 📂 R/ into memory\n\nusethis::use_r(\"function\")\ncreate R script for function\n\nusethis::use_package(\"package\")\nadd package dependency\n\ndevtools::document()\nbuild and add documentation\nOther resources:\nPackage development cheat sheet\nR packages book by Hadley Wickham and Jenny Bryan\nHow to develop good R packages by Maëlle Salmon\nR Package Primer by Karl Broman\nFirst package\nOpen an R session and submit the following, modified to your desired location. Here, I am creating a package named ralph on my desktop. (This name is inspired by R-Ladies Philly.)\n\n\nusethis::create_package(\"C:/Users/Shannon.Pileggi/Desktop/ralph\")\n\n\nNow you have a package! The usethis::create_package function:\nCreates a new R project named ralph at your specified location.\nOpens the ralph project in a new RStudio session.\nCreates the minimal essential files and structure required for R packages.\n\nSee the package development cheat sheet for a picture overview of the full file structure.\nOptional:\nIf you are already authenticated, the usethis package has some handy functions to quickly connect your local package to a GitHub repository,\n\n\nusethis::use_git()     # step 1 ----\nusethis::use_github()  # step 2 ----\n\n\nGitHub is renaming the default branch from master to main. Currently, usethis::use_github() creates a repo with the default branch named master.\nFor more references on using git / GitHub, see Jenny Bryan’s Happy Git and GitHub for the useR or Ch 18 in R Packages by Hadley Wickham and Jenny Bryan.\nFirst check\nNow that we have a package, let’s check it. Submitting devtools::check() updates package documentation, builds the package, and submits over 50 checks for metadata, structure, R code, documentation, and more!\n\n\ndevtools::check()\n\nThis can take a while to run, depending on how big your package is. It is helpful to run frequently, especially if you are planning on submitting to CRAN. But even for internal packages, it is still good practice.\nOur first check results in a single warning - that our package needs a license in the DESCRIPTION file:\n\nTo fix this, add the license of your choice. A standard recommendation is the MIT license due to broad permissions.\n\n\nusethis::use_mit_license(\"Shannon Pileggi\")\n\nThis updates the description file, as well as creates two new license files in your project that you never have to touch.\n\n\nThe DESCRIPTION file automatically populates with some fields from your .Rprofile, which can be modified with usethis::edit_r_profile().\nYou can go back and complete the remaining description fields later. Re-submit\n\n\ndevtools::check()\n\nand our package is error, warning, and note free.\n\nFirst function\nCreate\nThe compute_corr function is a wrapper for cor.test that produces tidy output for Pearson’s correlation estimate (along with a p-value) to quantify the linear relationship between two quantitative variables.\n\n\ncompute_corr <- function(data, var1, var2){\n  \n    # compute correlation ----\n    cor.test(\n      x = data %>% dplyr::pull({{var1}}),\n      y = data %>% dplyr::pull({{var2}})\n    ) %>% \n    # tidy up results ----\n    broom::tidy() %>% \n    # retain and rename relevant bits ----\n    dplyr::select(\n      correlation = estimate, \n      pval = p.value\n    )\n  \n}\n\n\nThis function uses tidy evaluation by embracing ({{) unquoted variable names.\nLet’s add the compute_corr function to ralph.\n\n\nusethis::use_r(\"compute_corr\")\n\nThis creates a blank R script named compute_corr.R located in the R/ folder. It is convention that your function name and R script name are the same. Copy and paste the function to the blank R script, and then save.\n\nExecute\nTake your function for a test drive with devtools::load_all (“arguably the most important part of the devtools workflow”).\n\n\ndevtools::load_all()\n\nThis places your function in local memory so that you may tinker and confirm its execution. Let’s give it a try.\n\n\ncompute_corr(data = faithful, var1 = eruptions, var2 = waiting)\n\n# A tibble: 1 x 2\n  correlation      pval\n        <dbl>     <dbl>\n1       0.901 8.13e-100\n\n\nDepending on which packages are loaded in your R session, you may have a small hiccup here, which we discuss in the workshop.\nDocument\nNext document the compute_corr function using the Roxygen skeleton. First, place your cursor in the function definition, and then you can add the skeleton two ways:\nCode -> Insert Roxygen Skeleton, or\nCntrl + Alt + Shift + R\nYou should see this:\n\nwhere we now have an outline to fill in. Note that the three arguments in our function (data, var1, var2) were automatically detected. Update the documentation as follows:\n\n\n#' Computes a tidy correlation\n#'\n#' @param data input data set\n#' @param var1 name of variable 1 \n#' @param var2 name of variable 2 \n#'\n#' @return A tibble with the Pearson correlation and the p-value\n#' @export\n#'\n#' @examples\n#' compute_corr(data = faithful, var1 = eruptions, var2 = waiting)\n\nSave your function and submit:\n\n\ndevtools::document()\n\nThis did a lot of work for you!\n\nThe man folder (short for manual) now contains a file called compute_corr.Rd. This is the documentation for your function; you never edit this manually.\nThe NAMESPACE now specifies that the package will export the compute_corr function.\nPreview the documentation with:\n\n\n?compute_corr\n\n\nIf you want to modify your documentation, make the changes in the Roxygen skeleton in compute_corr.R, save, and then resubmit devtools::document().\nCheck\nNow submit\n\n\ndevtools::check()\n\nand you’ll see that we have some concerns.\n\nThis is because compute_corr depends on functions from other packages, such as:\nThe pipe function (%>%) from the magrittr package.\nThe tidy function from the broom package.\nThe select function from the dplyr package.\nThe pull function from the dplyr package.\n\nAre any other functions missing from this list?\nDependencies\nWe can fix these errors, warnings, and notes by specifying the dependencies in the compute_corr function.\n1. Package dependencies\nTo specify a package dependency, the name of the package needs to be listed in the DESCRIPTION file. This can be automatically done for you by submitting\n\n\nusethis::use_package(\"broom\")\nusethis::use_package(\"dplyr\")\n\n\n2. Functions within packages\nThere are three ways you can specify required functions within packages.\nUse the :: notation within your function, as in broom::tidy (recommended, used in this post).\nIn the Roxygen section of your function, use @importFrom pkg fun1 fun2 - if you prefer this over using ::.\nIn the Roxygen section of your function, @import pkg - imports all functions from a package; use sparingly/wisely as this makes your package bulkier.\nSpecial case\nThe pipe function (%>%) from the magrittr package is a special case. The easiest way to include the pipe is\n\n\nusethis::use_pipe()  # step 1 ----\ndevtools::document() # step 2 ----\n\nHere, step 1 creates utils-pipe.R in your R folder and adds magrittr to imports in the DESCRIPTION file; step 2 adds the pipe to your NAMESPACE file.\n\nNow let’s run the check again:\n\n\ndevtools::check()\n\nCan you diagnose the note(s) this time? What steps would you take to correct it? I’m saving this for discussion in the workshop.1\n\nInstalling your package\nOpen a new R session. Install your package from your local directory or GitHub, load your package, and execute your functions.\n\n\n# install package from local directory ----\ndevtools::install(\"C:/Users/Shannon.Pileggi/Desktop/ralph\")\n\n# or install package from GitHub ----\n# devtools::install_github(\"shannonpileggi/ralph\")\n\n\nDue to GitHub renaming the default branch from master to main, you may need to fiddle with the ref argument in devtools::install_github.\n\n\n# load package ----\nlibrary(ralph)\n\n\n\n# execute function ----\ncompute_corr(data = faithful, var1 = eruptions, var2 = waiting)\n\n# A tibble: 1 x 2\n  correlation      pval\n        <dbl>     <dbl>\n1       0.901 8.13e-100\n\nSummary\nI hope you feel empowered to start developing your own packages now! We went through many of these steps one time only; however, in the development process, some of these steps are iterative. Here is a recap of our steps, although your work flow may differ.\nStep\nHow\n1. Load development packages.\nlibrary(usethis), library(devtools)\n2. Create new package.\nusethis::create_package(\"path/package\")\n3. Optional: Connect to GitHub repo.\nusethis::use_git(), usethis::use_github()\n4. Check build.\ndevtools::check()\n5. Add license.\nusethis::use_mit_license(\"Your name\")\n6. Check build.\ndevtools::check()\n7. Create new function.\nusethis::use_r(\"function\")\n8. Test drive function.\ndevtools::load_all()\n9. Insert Roxygen skeleton.\nMenu -> Code -> Insert Roxygen Skeleton\n10. Document package.\ndevtools::document()\n11. Preview documentation.\n?function\n12. Check build.\ndevtools::check()\n13. Specify package dependencies.\nusethis::use_package(\"package\")\n14. Specify functions within packages.\npackage::function\n15. Document package.\ndevtools::document()\n16. Check build.\ndevtools::check()\n17. Install package.\ndevtools::install(\"path/package\"), or\n\ndevtools::install_github(\"user/repo\")\n\nAlthough not explicitly written in these steps, I also recommend restarting R frequently in the development process (Session -> Restart R or Ctrl + Shift + F10) in order to test your package in a clean environment.\nAcknowledgements\nThank you R-Ladies Philly for hosting this workshop! Chun Su kindly created the thumbnail image for this workshop, as well as provided feedback on this post and the workshop content (thank you for so generously sharing your time!). On R-Ladies slack, Mayra Valdés exchanged some ideas with me regarding different ways to write the compute_corr function and Jenny Bryan helped me better understand where {{ lives and how it works. I learned much of this content from attending the rstudio::conf2020 Building Tidy Tools workshop co-instructed by Charlotte Wickham and Hadley Wickham. Shout out to Hillary Parker for showing us all how to build packages from scratch in 2014.\nIn the compute_corr function we failed to specify that the cor.test function is from the stats package (which is loaded by default on start up). Update the compute_corr function with stats::cor.test, and then devtools::check() again. The notes regarding no visible binding… remain; solve with the .data pronoun. (1) Add rlang to your DESCRIPTION file with usethis::use_package(\"rlang\"). (2) Include #' @importFrom rlang .data in the Roxygen section of your function documentation. (3) In the compute_corr function, replace estimate with .data$estimate and p.value with .data$p.value. Here is the link to the error, warning, and note free ralph package on github.↩︎\n",
    "preview": "posts/2020-10-25-your-first-r-package-in-1-hour/workshop_thumnnail4.jpg",
    "last_modified": "2022-08-11T11:39:25-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-09-22-exporting-editable-ggplot-graphics-to-powerpoint-with-officer-and-purrr/",
    "title": "Exporting editable ggplot graphics to PowerPoint with officer and purrr",
    "description": "What, why, how, when, and who",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2020-09-22",
    "categories": [],
    "contents": "\nTable of Contents\nTL; DR\nWhat is an editable PowerPoint graphic?\nWhy should I create it?\nHow do I create it?\nWhen should I do this more efficiently?\nWho should do the editing?\nLimitations\nSummary\nAppendix\nAcknowledgments\nTL; DR\nThe officer and rvg packages can be used to create PowerPoint slides with editable ggplot graphics. Skip to creating a single PowerPoint slide or to efficiently exporting multiple PowerPoint graphics with purrr.\nWhat is an editable PowerPoint graphic?\nAn editable PowerPoint graphic that is created within PowerPoint consists of two sets of editable components:\nVarious features of the graphic are editable, including items like size, color, and font (see Gif 1).\nThe data behind the graphic are editable (see Gif 2). This means that you can open the table linked to the chart and manually edit it in order to alter the data displayed in the graphic.\nAn editable PowerPoint graphic constructed in R through the officer + rvg functions described here produce vector graphics (i.e., shapes). This permits editing various features of the graphic (e.g., color, size), but not the data behind it (no linked table is created).\nWhy should I create it?\nIn my line of work, the primary deliverable is a PowerPoint slide deck. When creating an R graphic for a slide deck, I could export the graphic as an image (like a .png) to be inserted into PowerPoint, or I can export the graphic directly to an editable PowerPoint slide. Both of these options have pros and cons.\nFeature\nPowerPoint editable graphic\nImage (e.g., .png)\nEditability\n👍\n👎\nResizing\n👎\n👍\nData table\n👎\n👎\nThe editable PowerPoint graphic allows for direct editing in PowerPoint, but re-sizes poorly when done manually within PowerPoint. A .png image does not allow for direct editing within PowerPoint, but does nicely retain image ratios when re-sizing. Lastly, neither method produces a linked data table behind the graphic for editing.\nHow do I create it?\n\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(glue)\nlibrary(officer)\nlibrary(rvg)\nlibrary(viridis)\n\nFirst, let’s create a quick graphic for demonstration purposes using ggplot2::diamonds. We subset the data on specific values of color and clarity and produce a scatter plot showing the relationship between price and carat.\n\n\np <- diamonds %>% \n  filter(color == \"D\" & clarity == \"I1\") %>% \n  ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point() +\n    theme_minimal() +\n    ggtitle(\"Color: D; Clarity: I1\")\n\n\n\np\n\n\nIn order to export this graphic to an editable PowerPoint slide, first use the rvg package to convert the object to class dml (required to make graphic editable).\n\n\np_dml <- rvg::dml(ggobj = p)\n\nThen export the dml object to a PowerPoint slide with officer.\n\n\n# initialize PowerPoint slide ----\nofficer::read_pptx() %>%\n  # add slide ----\n  officer::add_slide() %>%\n  # specify object and location of object ----\n  officer::ph_with(p_dml, ph_location()) %>%\n  # export slide -----\n  base::print(\n    target = here::here(\n      \"_posts\",\n      \"2020-09-22-exporting-editable-ggplot-graphics-to-powerpoint-with-officer-and-purrr\",\n      \"slides\",\n      \"demo_one.pptx\"\n    )\n  )\n\nHere is a screen shot of the resulting PowerPoint slide, or you can download demo_one.pptx.\n\nWhen should I do this more efficiently?\nThere are 56 combinations of color and clarity in the diamonds data set; naturally, your colleague wants all 56 plots (at least for the appendix of the report 😂). So we definitely want an efficient way to do this!\nAutomate many plots\nThis work follows up on blog posts by Laurens Geffert, Len Kiefer, and Bruno Rodrigues, which were fantastic resources to help me get started. The officer package enacted changes in version 0.3.11(?) which necessitate updates to these methods (see Acknowledgements). In addition, Amber Thomas outlined a purrr work flow that resonates with me.\nI start with a table outlining the 56 combinations of color and clarity.\n\n\n# tibble of all possible combinations ----\ndiamonds_grid <- diamonds %>% \n  count(color, clarity) %>% \n  # for mapping, we need input values to be character ----\n  mutate_all(as.character)\n\n\n\n# view values of grid ----\ndiamonds_grid\n\n# A tibble: 56 x 3\n   color clarity n    \n   <chr> <chr>   <chr>\n 1 D     I1      42   \n 2 D     SI2     1370 \n 3 D     SI1     2083 \n 4 D     VS2     1697 \n 5 D     VS1     705  \n 6 D     VVS2    553  \n 7 D     VVS1    252  \n 8 D     IF      73   \n 9 E     I1      102  \n10 E     SI2     1713 \n# ... with 46 more rows\n\nThen I create a function that produces the plot for any given combination of color and clarity. As ggplot produces plots for the available data, there are some additional updates to this function to maintain consistency across all plots. We use a named color vector to create consistency in plotting colors across all plots, in addition to enforcing consistency in the x and y plotting ranges.\n\n\n# named vector for the colors assigned to cut ----\n# values are the colors assigned ----\ncolor_cut <- viridis::viridis(5) %>% \n  # assign levels of cut as names to colors ----\n  rlang::set_names(levels(diamonds[[\"cut\"]]))\n\n\n\n# view named color vector ----\ncolor_cut\n\n       Fair        Good   Very Good     Premium       Ideal \n\"#440154FF\" \"#3B528BFF\" \"#21908CFF\" \"#5DC863FF\" \"#FDE725FF\" \n\n\n\n# function to produce scatter plot of carat and price for given values of color and clarity ----\nplot_diamonds <- function(this_color, this_clarity){\n  diamonds %>% \n    filter(color == this_color & clarity == this_clarity) %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n      geom_point() +\n      theme_minimal() +\n      # maintain consistent plot ranges ----\n      xlim(range(diamonds[[\"carat\"]])) +\n      ylim(range(diamonds[[\"price\"]])) +\n      # maintain consistent colors for cut ----\n      # show all values of cut in legend, regardless if appear in this plot ----\n      scale_color_manual(values = color_cut, drop = F) +\n      # title indicates which combination is plotted ----\n      ggtitle(glue::glue(\"Color: {this_color}; Clarity: {this_clarity}\")) \n\n}\n\nNext I utilize the plot_diamonds function with purrr to create a list with 56 ggplot objects representing all combinations of color and clarity.\n\n\ndiamonds_gg <- purrr::map2(\n  # first argument to plot_diamonds function ----\n  diamonds_grid[[\"color\"]],\n  # second argument to plot_diamonds function ----\n  diamonds_grid[[\"clarity\"]],\n  # function to map ----\n  plot_diamonds\n)\n\nExport many plots\nTo export these, I use two helper functions. The first function, create_dml, converts the ggplot objects to dml objects.\n\n\ncreate_dml <- function(plot){\n  rvg::dml(ggobj = plot)\n}\n\nApply this function to the list of ggplot objects to create a list of dml objects with the same dimension.\n\n\ndiamonds_dml <- purrr::map(diamonds_gg, create_dml)\n\nThe second function automates exporting all slides to PowerPoint, with some additional options to specify the position and size (inches) of the graphic. The default size (9in x 4.95in) produces a graphic that fills a standard sized slide.\n\n\n# function to export plot to PowerPoint ----\ncreate_pptx <- function(plot, path, left = 0.5, top = 1, width = 9, height = 4.95){\n  \n    # if file does not yet exist, create new PowerPoint ----\n    if (!file.exists(path)) {\n        out <- officer::read_pptx()\n    }\n    # if file exist, append slides to exisiting file ----\n    else {\n        out <- officer::read_pptx(path)\n    }\n  \n    out %>% \n      officer::add_slide() %>% \n      officer::ph_with(plot, location = officer::ph_location(\n        width = width, height = height, left = left, top = top)) %>% \n      base::print(target = path)\n}\n\nNote that this function opens and closes PowerPoint for each slide created, so more slides will take longer to export. This particular set of graphics took ~6 minutes to export due to the number of slides and the number of points on some slides 😬 (which is longer than usual for my typical applications).\n\n\npurrr::map(\n  # dml plots to export ----\n  diamonds_dml, \n  # exporting function ----\n  create_pptx, \n  # additional fixed arguments in create_pptx ----\n  path = here::here(\n    \"_posts\", \n    \"2020-09-22-exporting-editable-ggplot-graphics-to-powerpoint-with-officer-and-purrr\",\n    \"slides\", \n    \"demo_many.pptx\"\n    )\n  )\n\nHere is a screen shot of the resulting PowerPoint slide, or you can download demo_many.pptx.\n\nWho should do the editing?\nNow that you have editable PowerPoint slides, you have two parties capable of editing the graphics: (1) the R developer, and (2) the collaborator.\nThe R developer should do further slide editing when:\nedits are universal (i.e., reduce size of all plots to 6in x 3in).\nedits are data driven (i.e., represent all points where carat exceeds a value of 3 as a star)\nThe collaborator should do further slide editing when:\nedits are bespoke (i.e. Bob wants to see slide 25 in bold for the marketing team)\nedits are beyond the budget (i.e., super custom axis adornment)\nLimitations\nWhile the graphics exported to PowerPoint from R are editable, they do have limitations compared to graphics created within PowerPoint.\nAs previously mentioned, there is no linked data table behind the graphics, which can be unnerving for a colleague who wants to quality check the figure (I often export labeled and unlabeled versions of figures for this).\nThe points are not naturally grouped features as they would be for graphics created within PowerPoint. This means that if your colleagues wants to change the shade of yellow for the ideal cut diamonds they would have to click on each and every single yellow point in all slides (see Gif 3 for preview of what the slide looks like when you click around).\nSummary\nEditable PowerPoint ggplot graphics created through officer + rvg + purrr can be a great way to provide your colleague with fantastic graphics while still allowing them to refine the graphics with their own finishing touches. Do expect a lot of iteration on the graphic to get it as close as possible to their needs before you hit that final send on the PowerPoint deck.\nAppendix\nGif 1\nDemonstration of editable features in graphic created within PowerPoint. Notice that the points are automatically grouped together. Go back to What is an editable PowerPoint graphic?.\n\nGif 2\nDemonstration of editable data table behind graphic created within PowerPoint. Go back to What is an editable PowerPoint graphic?.\n\nGif 3\nDemonstration of editing title and single point color in graphic exported to PowerPoint by rvg + officer (notice points are not grouped). Go back to Limitations.\n\nAcknowledgments\nThe create_pptx function was modified from Bruno Rodrigues. My colleague Tom Nowlan figured out the function updates for officer::ph_with (formerly officer::ph_with_vg) to export figures to a specific size and location. Thumbnail artwork was adapted from @allison_horst.\n\n\n",
    "preview": "posts/2020-09-22-exporting-editable-ggplot-graphics-to-powerpoint-with-officer-and-purrr/data_cowboy_officer.png",
    "last_modified": "2022-08-11T11:39:24-04:00",
    "input_file": {},
    "preview_width": 678,
    "preview_height": 382
  },
  {
    "path": "posts/2020-09-07-introducing-the-rstudio-ide-and-r-markdown/",
    "title": "Introducing RStudio and R Markdown",
    "description": "Gettin' giffy wit it.",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2020-09-07",
    "categories": [],
    "contents": "\nTable of Contents\nTL; DR\nBackground\nCustomizing the interface\nAdjusting panels\nRe-arrange panels\nRecover lost panels\nFont size/resolution\nPersonalization\nMargin column\n\nR Markdown\nOpening a new R Markdown\nKnitting an R Markdown\nPreview an R Markdown\nPersonalizing R Markdown\nInserting/splitting code chunks\nChunk anatomy\nChunk names\nChunk execution\nChunk options\n\nR Markdown Troubleshooting\nYAML error\nChunk error\nChunk error for duplicate chunk name\nLayout error\nR code error\n\nThe end\nAcknowledgments\nTL; DR\nRStudio has some wonderful features! Here are some tips and tricks for new learneRs to get started with regards to customizing your interface, getting started with R Markdown, and a bit of troubleshooting.\nBackground\nGif’s were captured in May 2019, likely with RStudio version 1.2.1335-1. Keyboard shortcuts are for Windows users, Mac users may differ. For Windows users, I also recommend changing your default settings to show file extensions.\nAnd if you are not familiar with it, please allow me introduce you to Will Smith’s 1998 hit “Gettin’ Jiggy Wit It.” 🎵Na na na na na na na nana 🎵\nCustomizing the interface\nAdjusting panels\n\nRe-arrange panels\nTools -> Global Options -> Panel Layout\n\nRecover lost panels\n\nFont size/resolution\nView -> Zoom in / Zoom out / Actual size\n\nPersonalization\nTools -> Global Options -> Appearance\n\nMargin column\nTools -> Global Options -> Code -> Display -> Show margin -> Margin column 80\nConsider placing a margin column at 80 characters as a reminder for code formatting. Keeping code within 80 characters can make it easier to read when switching displays or sharing code.\n\nR Markdown\nOpening a new R Markdown\nFile -> New file -> R Markdown    or    -> R Markdown\n\nKnitting an R Markdown\n\nPreview an R Markdown\n\nPersonalizing R Markdown\n\nInserting/splitting code chunks\n    or    Ctrl + Alt + I\n\nChunk anatomy\n\nChunk names\nChunk names allow you to quickly navigate code, automatically name figures, and troubleshoot errors. Chunk names must be unique! If no name is provided, a default numbered chunk name will be assigned.\n\nChunk execution\n    or    Ctrl + Enter\n\nChunk options\n\nR Markdown Troubleshooting\nYAML error\n\nChunk error\n\nChunk error for duplicate chunk name\n\nLayout error\nLet your markdown breathe! If something doesn’t look right, try adding white space.\n\nR code error\n\nThe end\n\n\nAcknowledgments\nThumbnail artwork adapted from @allison_horst.\nThe phrase IDE was removed from the title and body of this post on Sep. 8, 2020 per @TrashBirdEcol’s suggestion.\n\n\n",
    "preview": "posts/2020-09-07-introducing-the-rstudio-ide-and-r-markdown/introducing_resize2.png",
    "last_modified": "2022-08-11T11:39:24-04:00",
    "input_file": {},
    "preview_width": 473,
    "preview_height": 371
  },
  {
    "path": "posts/2020-08-30-a-job-interview-presentation-inspired-by-the-r-community/",
    "title": "A job interview presentation inspired by the R community",
    "description": "How #tidytuesday and twitter helped me secure a job offer",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2020-08-30",
    "categories": [],
    "contents": "\nTable of Contents\nTL; DR\nBack up\nFormulation\nFortuitous tweets\nWhat I did\nResults\nAcknowledgements\n\nTL; DR\nI utilized resources from #tidytuesday, twitter, and blog posts to create a job interview presentation that provided insights on my prospective employer while showcasing my analytics capabilities.\nBack up\nWhen I was told that my interview at Adelphi Research would involve a presentation, I was pretty happy. A presentation allows the candidate to highlight their strengths, and, after being a college professor for six years, I was confident in my public speaking. The specific presentation instructions were:\n\nThis can be any 20 minute presentation you’d like to share with the team; our recommendation is that it focuses on Market Research, is something innovative, and something that you’re particularly proud of!\n\nIt was also briefly mentioned in the phone screening that the analytics group with whom I was interviewing was just beginning to develop shiny applications.\nFormulation\nThen I faced the dilemma of what to actually present. I wanted to shine as a candidate, but my previous academic experience was tangential to this industry. I took their instructions to heart and decided to create something of which I was truly proud. I developed a project with three goals in mind:\nLearn about my prospective employer.\nCultivate my nascent tidyverse skills.\nInvest in new analytic methods and coding techniques that could serve future me.\nFrom there, I had to figure out what I would actually present.\nFortuitous tweets\nThis was winter 2018, which also happened to be the inaugural year of #tidytuesday. Prompted by David Robinson’s screencast analyzing medium articles with tidytext, I was inspired to apply the same principles of text analysis to my prospective employer’s twitter account.\n\n\nIn this week's #tidytuesday screencast, I use tidytext to analyze what titles get claps on Medium posts. Practical guides on tensorflow/keras are the hottest, words like “marketing”, “trends” and “industry” don't get you far https://t.co/oNhZm40mpW #rstats pic.twitter.com/cxYO2MIIqz\n\n— David Robinson (@drob) December 4, 2018\n\nAs I started to create my presentation, I was also concerned with the actual format / deliverable for the presentation. I knew I wanted to create something in R, but I was not quite sure which direction to go. I tweeted an #rstats plea for advice and was inspired by Emily Riederer’s suggestion to use a flexdashboard for the presentation.\n\n\nIf you’re having trouble with xaringan, storyboards in flexdashboqrd can also make pretty nice presentations depending on the type of content / message you are trying to convey https://t.co/IC3moe01v0\n\n— Emily Riederer (@EmilyRiederer) December 12, 2018\n\nWhat I did\nAfter many hours at the computer, several re-watches of David Robinson’s screencast, and a lot of research on R functions and analytic methods, I created a presentation about my prospective employer based on trends in their twitter account between 2009 (account start date) and 2018 (interview date). Here are the packages that I used, alongside their purpose with links to code on github:\nrtweet: retrieve tweets from @AdelphiResearch.\ndplyr, forcats, lubridate, stringr, and purrr: create new variables with regards to tweet descriptors and hashtag themes; summarize trends in top hashtags over time\nggplot2: exploratory visualization of tweet frequency, trends in likes and re-tweets per tweet, and trends in hashtag themes over time\ntidytext: transform tweet words to analyzable tokens; prepare words for modeling\nwidyr, igraph, ggraph: network analysis of common hashtag themes and tweet text\nglmnet: lasso model to assess word associations with likes and retweets\nshiny: develop shiny apps to search tweets and explore the network analyses\nflexdashboard: assemble it all in a presentation\nYes, it was a lot! Especially because the tidyverse, pulling tweets, text analysis, network analysis, lasso models, flexdashboards, and shiny apps were all very new to me.\nResults\nI hosted my presentation and the two shiny applications on shinyapps.io, and I made my code publicly available on github.\nLink \nPreview \nInterview presentation\n\nShiny app for tweet look up\n\nShiny app for network analysis\n\nCode on github\n\nIn the end, I got the job! Along the way, I gleaned insights about the values and industry of my prospective employer, empowering me throughout the interview process. Additionally, my interviewers enjoyed the external data-driven view of their company’s tweets (despite, or perhaps because of, the fact that none of them were actually on twitter).\nI embraced the interview as an opportunity to create my own side project to foster new skills that would serve me for other potential interviews, personal projects, or work projects. Yes, I probably could have secured a job offer with less effort, but I would have been less confident and I would have missed out on so many data and programming approaches that are now ingrained in my thought processes. And enabled by the #rstats twitter presence, the incredible packages that I used, and the numerous blog posts that I referenced, I had fun. Cheers, R! 🥂\nAcknowledgements\nThumbnail artwork by @allison_horst. Thanks to Megan McClintock (my sister) for feedback and suggestions.\n\n\n",
    "preview": "posts/2020-08-30-a-job-interview-presentation-inspired-by-the-r-community/welcome_to_rstats_twitter.png",
    "last_modified": "2022-08-11T11:39:23-04:00",
    "input_file": {},
    "preview_width": 2009,
    "preview_height": 1942
  },
  {
    "path": "posts/2018-12-11-stringr-4-ways/",
    "title": "Stringr 4 ways",
    "description": "Four approaches to feature engineering with regular expressions in R",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": {}
      }
    ],
    "date": "2018-12-11",
    "categories": [],
    "contents": "\nUpdate May 22, 2019: Thanks to @hglanz for noting that I could have used pull instead of the . as a placeholder.\n\n\nlibrary(tidyverse) # general use\nlibrary(titanic)   # to get titanic data set\n\nOverview\nThe Name variable in the titanic data set has all unique values. To get started, let’s visually inspect a few values.\n\n\ntitanic_train %>% \n  select(Name) %>% \n  head(10)\n\n                                                  Name\n1                              Braund, Mr. Owen Harris\n2  Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n3                               Heikkinen, Miss. Laina\n4         Futrelle, Mrs. Jacques Heath (Lily May Peel)\n5                             Allen, Mr. William Henry\n6                                     Moran, Mr. James\n7                              McCarthy, Mr. Timothy J\n8                       Palsson, Master. Gosta Leonard\n9    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\n10                 Nasser, Mrs. Nicholas (Adele Achem)\n\nIn this brief print out, each passenger’s title is consistently located between , and .. Luckily, this holds true for all observations! With this consistency, we can somewhat easily extract a title from Name. Depending on your field, this operation may be referred to something along the lines of data preparation or feature engineering.\nTL; DR\nApproach\nFunction(s)\nRegular expression(s)\n1\nstr_locate + str_sub\n\",\" + \"\\\\.\"\n2\nstr_match\n\"(.*)(, )(.*)(\\\\.)(.*)\"\n3\nstr_extract + str_sub\n\"([A-z]+)\\\\.\"\n4\nstr_replace_all\n\"(.*, )|(\\\\..*)\"\nRead on for explanations!\nOverview of regular expressions\nRegular expressions can be used to parse character strings, which you can think of as a key to unlock string patterns. The trick is identify the right regular expression + function combination. Let’s demo four ways to tackle the challenge utilizing functions from the stringr package; each method specifies a different string pattern to match.\n\n\nlibrary(stringr)\n\nExtracting title from name\nFirst approach\nICYMI, the double bracket in titanic_train[[\"Name\"]] is used to extract a named variable vector from a data frame, which has some benefits over the more commonly used dollar sign (i.e., titanic_train$Name). Now onward.\nThe str_locate function produces the starting and ending position of a specified pattern. If we consider the comma to be a pattern, we can figure out where it is located in each name. Here, the starting and ending value is the same because the comma is only one character.\n\n\ntitanic_train[[\"Name\"]] %>% \n  str_locate(\",\") %>%\n  head()\n\n     start end\n[1,]     7   7\n[2,]     8   8\n[3,]    10  10\n[4,]     9   9\n[5,]     6   6\n[6,]     6   6\n\nKnowing this, we can identify the positions of the comma and the period and then extract the text in between. Some notes here:\nBecause str_locate function returns a matrix, we use . as a placeholder in .[,1] to access the first column of values.\nBecause . is a special character in regular expressions, we use the double backslash in \"\\\\.\" to escape it.\n\n\ncomma_pos <- titanic_train[[\"Name\"]] %>% \n  str_locate(\",\") %>% \n  .[,1]\n\nperiod_pos <- titanic_train[[\"Name\"]] %>% \n  str_locate(\"\\\\.\") %>% \n  .[,1]\n\nNow we can use str_sub to extract substrings from the character vector based on their physical position. To exclude the punctuation and white space, we can add two to the comma position and subtract one from the period position to get the title only.\n\n\ntitanic_train[[\"Name\"]] %>% \n  str_sub(comma_pos + 2, period_pos - 1) %>% \n  head()\n\n[1] \"Mr\"   \"Mrs\"  \"Miss\" \"Mrs\"  \"Mr\"   \"Mr\"  \n\nSuper!\nSecond approach\nThe str_match function creates a character matrix for each group matched in the specified pattern. With the correct regular expression, str_match returns the complete match in addition to each matched group. Here’s a quick example:\n\n\n# ----------5 groups-->>>----1---2---3----4---5----                      \nstr_match(\"XXX, YYY. ZZZ\", \"(.*)(, )(.*)(\\\\.)(.*)\")\n\n     [,1]            [,2]  [,3] [,4]  [,5] [,6]  \n[1,] \"XXX, YYY. ZZZ\" \"XXX\" \", \" \"YYY\" \".\"  \" ZZZ\"\n\nLet’s break down this regular expression pattern.\nThe parentheses () indicate a grouping\nThe 2nd grouping (, ) only has one modification from the pattern used in the first example - here we include a space after the comma.\nThe 4th grouping (\\\\.) remains unchanged from the first example.\nin the 1st, 3rd, and 5th grouping (.*)\nThe period . means any character\nThe asterisk * means matches at least 0 times\n\nTo execute this, we’ll grab the 4th column to catch our title.\n\n\ntitanic_train[[\"Name\"]] %>% \n  str_match(\"(.*)(, )(.*)(\\\\.)(.*)\") %>%\n  .[,4] %>% \n  head()\n\n[1] \"Mr\"   \"Mrs\"  \"Miss\" \"Mrs\"  \"Mr\"   \"Mr\"  \n\nAll right, we got it again!\nThird approach\nLastly, let’s use the str_extract function to extract matching patterns. This seems like what we wanted to do all along!\n\n\ntitanic_train[[\"Name\"]] %>% \n  str_extract(\"([A-z]+)\\\\.\") %>%\n  head()\n\n[1] \"Mr.\"   \"Mrs.\"  \"Miss.\" \"Mrs.\"  \"Mr.\"   \"Mr.\"  \n\nLet’s break down this regular expression:\nHere, the bracket []specifies a list of permitted characters.\nInside the bracket, we specify strings that consist of both upper and lower case letters with A-z.\nThe + outside the bracket signifies “match at least one time”, i.e., grab all the letters.\nFinally, ending in \\\\. indicates that our pattern should end in period.\nPutting it all together, this pattern translates to “grab all upper case and lower case letters immediately preceeding a period.”\nThis pattern is a bit more sophisticated to compose than the previous ones, but it gets right to the point! This last effort does end in a period, whereas the others do not. If we wanted to remove the period for consistency, we could use str_sub with the end argument to specify the position of the last character.\n\n\ntitanic_train[[\"Name\"]] %>% \n  str_extract(\"([A-z]+)\\\\.\") %>%\n  str_sub(end = -2) %>%\n  head()\n\n[1] \"Mr\"   \"Mrs\"  \"Miss\" \"Mrs\"  \"Mr\"   \"Mr\"  \n\nFourth approach\nAs a last approach, we can use str_replace_all to replace all matched patterns with null character values. Here, we specify the pattern and then the replacement string.\n\n\ntitanic_train[[\"Name\"]] %>% \n  str_replace_all(\"(.*, )|(\\\\..*)\", \"\") %>%\n  head()\n\n[1] \"Mr\"   \"Mrs\"  \"Miss\" \"Mrs\"  \"Mr\"   \"Mr\"  \n\nIn this regular expression,\nThere are two groupings separated by a vertical pipe | which means “or”.\nThe two groupings should look familar:\nThe first grouping looks for all characters preceeding a comma space.\nThe second grouping looks for all characters following a period.\n\nPutting it together, if grouping 1 or grouping 2 is satisifed then those character values are replaced with null character values.\nRe-classifying entries\nNow that we figured out how to extract the title, I’ll utilize the last method and assign title as a variable to the titanic_train data set using the mutate function.\n\n\ntitanic_train <- titanic_train %>%\n  mutate(title = str_replace_all(titanic_train[[\"Name\"]], \"(.*, )|(\\\\..*)\", \"\"))\n\nNow let’s use count to get a frequency table of the titles, with the sort = TRUE option to arrange the results in descending order.\n\n\ntitanic_train %>%\n  count(title, sort = TRUE)\n\n          title   n\n1            Mr 517\n2          Miss 182\n3           Mrs 125\n4        Master  40\n5            Dr   7\n6           Rev   6\n7           Col   2\n8         Major   2\n9          Mlle   2\n10         Capt   1\n11          Don   1\n12     Jonkheer   1\n13         Lady   1\n14          Mme   1\n15           Ms   1\n16          Sir   1\n17 the Countess   1\n\nWe can see that there are several infrequent titles occuring only one or two times, and so we should re-classify them. If you want to squeeze the most juice out of your data, try to figure out the historical context and meaning of those titles to create a better classification for them. For now, let’s take the easy way out by just re-classifying them to an other group.\nFortunately, the forcats package has an awesome function that let’s us do this quickly: fct_lump. We’re using mutate again to re-classified title. The fct_lump function combines the least frequent values together in an other group, and the n = 6 option specifies to keep the 6 most common values (so the 7th value is other).\n\n\ntitanic_train %>%\n  mutate(title = fct_lump(title, n = 6)) %>%\n  count(title, sort = TRUE)\n\n   title   n\n1     Mr 517\n2   Miss 182\n3    Mrs 125\n4 Master  40\n5  Other  14\n6     Dr   7\n7    Rev   6\n\nIf you wanted to explicitly re-code the infrequent titles to something more meaningful than other, look into fct_recode.\nSuper, now title is ready to use for analysis!\n\n\n",
    "preview": "posts/2018-12-11-stringr-4-ways/table2.png",
    "last_modified": "2022-08-11T11:39:23-04:00",
    "input_file": {},
    "preview_width": 1010,
    "preview_height": 319
  },
  {
    "path": "posts/2018-11-05-welcome-to-piping-hot-data/",
    "title": "Welcome to Piping Hot Data",
    "description": "What's in a name?",
    "author": [
      {
        "name": "Shannon Pileggi",
        "url": "www.pipinghotdata.com"
      }
    ],
    "date": "2018-11-05",
    "categories": [],
    "contents": "\nSo do I have piping hot data or am I piping hot data? Let’s break it down.\nPiping Hot Data\nWow, my data is piping hot! This connotes exciting, newly released data! I can’t promise that I’ll fulfill this expectation. Let’s just say that I’ll talk about data occasionally, and on special occasions it might even be piping hot.\nPiping Hot Data\nHere, I am piping my elusive hot data. This is what I was really going for - an ode to the pipe in R:\n\n%>%\n\nThe pipe operator simplifies long operations by linking multiple functions simultaneously. Although the coding construct of the pipe has been floating around since the 1970’s, the floodgates didn’t open for R until 2014. I don’t know the exact date, but I do remember the first time I saw those three characters and the sea of emotions that rained down. Confusion. Curiousity. Excitement.\nIn just 4 short years, the pipe and its friends in the tidyverse have revolutionized how we code in R, to the point that you may feel illiterate at conferences if you don’t have some baseline understanding - at first. Because the beauty of the pipe is that it streamlines readability of R code, such that even if you have never done it, you can still get the gist of what is going on. So much so that believers are proselytizing “Teach the tidyverse to beginners”!\nLet’s lay the pipelines with a quick example using the classic iris data set. To get started, load the tidyverse library and get an overview of the data.\n\n\nlibrary(tidyverse)\nglimpse(iris)\n\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.~\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.~\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.~\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.~\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa,~\n\nOur simple objective is to compute the mean Sepal.Length for each Species in the data set and then arrange the results in descending order. There are many ways to accomplish this without the tidyverse, but for the sake of side-by-side comparisons I’ll demo this using tidyverse functions first without and then with piping.\n\n\narrange(\n  summarise(\n    group_by(iris, Species), \n    mean = mean(Sepal.Length)\n  ), \n  desc(mean)\n)\n\n\n# A tibble: 3 x 2\n  Species     mean\n  <fct>      <dbl>\n1 virginica   6.59\n2 versicolor  5.94\n3 setosa      5.01\n\nWithout using pipes, we have to read our code inside to out to understand the operations executed. Our iris data set is buried in the middle of the code, and then the operations performed of group_by, summarise, and arrange spring outward from there (reading up from iris). Now let’s try the same manipulations utilizing piping.\n\n\niris %>% \n  group_by(Species) %>% \n  summarise(mean = mean(Sepal.Length)) %>%\n  arrange(desc(mean))\n\n\n# A tibble: 3 x 2\n  Species     mean\n  <fct>      <dbl>\n1 virginica   6.59\n2 versicolor  5.94\n3 setosa      5.01\n\nVoilá! It’s clear from the left side of the pipe that all manipulations are done on the iris data set, and it’s clear from the right side of the pipe that the series of operations performed are group_by, summarise, and arrange. Wow, I like the way data flow through those pipes!\nWhile the name of this blog gives an nod to the powerful pipe, the pipe isn’t going to permeate every solution to programming challenges. So here is what to expect from Piping Hot Data:\nDemo data science tools and methods.\nDiscover new data and R packages.\nDeliberate data and technical topics.\nI hope you enjoy!\nAcknowledgements\nThumbnail artwork by @allison_horst.\n\n\n\n",
    "preview": "posts/2018-11-05-welcome-to-piping-hot-data/tidyverse_celestial.png",
    "last_modified": "2022-08-11T11:39:23-04:00",
    "input_file": {},
    "preview_width": 2048,
    "preview_height": 2048
  }
]
